SpoonOS Cookbook â€“ Combined Markdown Export
Generated by cookbook/scripts/generate-llm.js

---

FILE: docs/api-reference/agents/base-agent.md

# Base Agent API Reference

The `BaseAgent` class is the foundation for all agents in SpoonOS, providing core functionality for LLM interaction, tool management, and conversation handling.

## Class Definition

```python
from spoon_ai.agents.base import BaseAgent
from spoon_ai.tools import ToolManager
from spoon_ai.llm import LLMManager

class BaseAgent:
    def __init__(
        self,
        name: str,
        system_prompt: str = None,
        llm_manager: LLMManager = None,
        tool_manager: ToolManager = None,
        **kwargs
    )
```

## Parameters

### Required Parameters

- **name** (`str`): Unique identifier for the agent

### Optional Parameters

- **system_prompt** (`str`, optional): System prompt that defines agent behavior
- **llm_manager** (`LLMManager`, optional): LLM manager instance for model interactions
- **tool_manager** (`ToolManager`, optional): Tool manager for available tools
- **kwargs**: Additional configuration options

## Methods

### Core Methods

#### `async run(message: str, **kwargs) -> str`

Execute the agent with a user message.

**Parameters:**
- `message` (str): User input message
- `**kwargs`: Additional execution parameters

**Returns:**
- `str`: Agent response

**Example:**
```python
agent = BaseAgent(name="assistant")
response = await agent.run("Hello, how are you?")
print(response)
```

#### `async chat(messages: List[Dict], **kwargs) -> Dict`

Process a conversation with multiple messages.

**Parameters:**
- `messages` (List[Dict]): List of conversation messages
- `**kwargs`: Additional chat parameters

**Returns:**
- `Dict`: Chat response with metadata

**Example:**
```python
messages = [
    {"role": "user", "content": "What's the weather like?"}
]
response = await agent.chat(messages)
```

### Configuration Methods

#### `set_system_prompt(prompt: str)`

Update the agent's system prompt.

**Parameters:**
- `prompt` (str): New system prompt

**Example:**
```python
agent.set_system_prompt("You are a helpful coding assistant.")
```

#### `add_tool(tool: BaseTool)`

Add a tool to the agent's tool manager.

**Parameters:**
- `tool` (BaseTool): Tool instance to add

**Example:**
```python
from spoon_ai.tools import CustomTool

custom_tool = CustomTool()
agent.add_tool(custom_tool)
```

#### `remove_tool(tool_name: str)`

Remove a tool from the agent's tool manager.

**Parameters:**
- `tool_name` (str): Name of the tool to remove

**Example:**
```python
agent.remove_tool("custom_tool")
```

### Information Methods

#### `get_available_tools() -> List[str]`

Get list of available tool names.

**Returns:**
- `List[str]`: List of tool names

**Example:**
```python
tools = agent.get_available_tools()
print(f"Available tools: {tools}")
```

#### `get_config() -> Dict`

Get current agent configuration.

**Returns:**
- `Dict`: Agent configuration dictionary

**Example:**
```python
config = agent.get_config()
print(f"Agent config: {config}")
```

## Properties

### `name: str`
Agent's unique identifier (read-only)

### `system_prompt: str`
Current system prompt

### `llm_manager: LLMManager`
LLM manager instance

### `tool_manager: ToolManager`
Tool manager instance

### `config: Dict`
Agent configuration dictionary

## Events

### `on_message_received(message: str)`
Triggered when agent receives a message

### `on_response_generated(response: str)`
Triggered when agent generates a response

### `on_tool_executed(tool_name: str, result: Any)`
Triggered when a tool is executed

### `on_error(error: Exception)`
Triggered when an error occurs

## Configuration Schema

```json
{
  "name": "string",
  "system_prompt": "string",
  "config": {
    "max_steps": "integer",
    "temperature": "float",
    "max_tokens": "integer",
    "timeout": "integer"
  },
  "tools": [
    {
      "name": "string",
      "type": "builtin|custom|mcp",
      "enabled": "boolean",
      "config": {}
    }
  ]
}
```

## Error Handling

### Common Exceptions

#### `AgentError`
Base exception for agent-related errors

#### `ConfigurationError`
Raised when agent configuration is invalid

#### `ToolError`
Raised when tool execution fails

#### `LLMError`
Raised when LLM interaction fails

### Error Handling Example

```python
from spoon_ai.agents.base import BaseAgent
from spoon_ai.agents.errors import AgentError, ToolError

try:
    agent = BaseAgent(name="test_agent")
    response = await agent.run("Hello")
except ConfigurationError as e:
    print(f"Configuration error: {e}")
except ToolError as e:
    print(f"Tool execution error: {e}")
except AgentError as e:
    print(f"Agent error: {e}")
```

## Best Practices

### Initialization
- Always provide a unique name for each agent
- Set appropriate system prompts for your use case
- Configure tools before first use
- Validate configuration before deployment

### Performance
- Reuse agent instances when possible
- Configure appropriate timeouts
- Monitor tool execution times
- Use caching for expensive operations

### Security
- Validate all user inputs
- Sanitize system prompts
- Limit tool permissions
- Monitor agent behavior

### Debugging
- Enable debug logging for troubleshooting
- Use event handlers to monitor agent behavior
- Test with simple inputs first
- Validate tool configurations

## Examples

### Basic Agent Setup

```python
from spoon_ai.agents.base import BaseAgent
from spoon_ai.llm import LLMManager
from spoon_ai.tools import ToolManager

# Create LLM manager
llm_manager = LLMManager(
    provider="openai",
    model="gpt-4"
)

# Create tool manager
tool_manager = ToolManager()

# Create agent
agent = BaseAgent(
    name="my_assistant",
    system_prompt="You are a helpful assistant.",
    llm_manager=llm_manager,
    tool_manager=tool_manager
)

# Use agent
response = await agent.run("What can you help me with?")
print(response)
```

### Agent with Custom Configuration

```python
from spoon_ai.agents.base import BaseAgent

agent = BaseAgent(
    name="custom_agent",
    system_prompt="You are a specialized assistant.",
    config={
        "max_steps": 10,
        "temperature": 0.7,
        "max_tokens": 2000,
        "timeout": 30
    }
)

# Add event handlers
def on_message(message):
    print(f"Received: {message}")

def on_response(response):
    print(f"Generated: {response}")

agent.on_message_received = on_message
agent.on_response_generated = on_response

# Use agent
response = await agent.run("Hello, world!")
```

### Multi-turn Conversation

```python
from spoon_ai.agents.base import BaseAgent

agent = BaseAgent(name="conversational_agent")

# Start conversation
messages = [
    {"role": "user", "content": "Hello, I need help with Python."}
]

response1 = await agent.chat(messages)
messages.append({"role": "assistant", "content": response1["content"]})

# Continue conversation
messages.append({"role": "user", "content": "Can you show me a simple example?"})
response2 = await agent.chat(messages)

print(f"Final response: {response2['content']}")
```

## See Also

- [Agent Types](../../core-concepts/agents.md)
- [Tool Manager API](../tools/base-tool.md)
- [LLM Providers](../../core-concepts/llm-providers.md)
- [Agent Development Guide](../../how-to-guides/build-first-agent.md)"}

---

FILE: docs/api-reference/cli/commands.md

# CLI Commands API Reference

Comprehensive reference for all SpoonOS CLI commands, options, and usage patterns.

## Command Categories

### Core Commands

#### `help`
**Aliases:** `h`, `?`
**Description:** Display help information
**Usage:** `help [command]`

**Examples:**
```bash
> help
> help load-agent
> h
> ?
```

#### `exit`
**Aliases:** `quit`, `q`
**Description:** Exit the CLI
**Usage:** `exit`

**Examples:**
```bash
> exit
> quit
> q
```

#### `system-info`
**Aliases:** `sysinfo`, `status`, `info`
**Description:** Display comprehensive system information, environment status, and health checks
**Usage:** `system-info`

**Output includes:**
- System details (platform, Python version, architecture)
- Environment variables status
- Configuration validation
- Agent status
- Health checks with scoring

**Examples:**
```bash
> system-info
> sysinfo
> status
> info
```

### Agent Management

#### `load-agent`
**Aliases:** `load`
**Description:** Load an agent with the specified name
**Usage:** `load-agent <agent_name>`

**Parameters:**
- `agent_name` (required): Name or alias of the agent to load

**Examples:**
```bash
> load-agent react
> load-agent spoon_react_mcp
> load spoon_react
```

#### `list-agents`
**Aliases:** `agents`
**Description:** List all available agents
**Usage:** `list-agents`

**Output format:**
```
Available agents:
- react (SpoonReactAI) - Standard blockchain analysis agent
- spoon_react_mcp (SpoonReactMCP) - MCP-enabled blockchain agent
```

**Examples:**
```bash
> list-agents
> agents
```

#### `action`
**Aliases:** `a`
**Description:** Perform a specific action using the current agent
**Usage:** `action <action_type> [parameters]`

**Action Types:**
- `chat [message]`: Start chat session or send message
- `react`: Start step-by-step reasoning session
- `new`: Start new chat (clear history)
- `list`: List available chat histories
- `load <chat_id>`: Load specific chat history

**Examples:**
```bash
> action chat "Hello, how are you?"
> action react
> action new
> action list
> action load chat_001
> a chat
```

### Configuration Management

#### `config`
**Aliases:** `cfg`, `settings`
**Description:** Configure settings such as API keys
**Usage:** `config [key] [value]`

**Parameters:**
- `key` (optional): Configuration key to view or modify
- `value` (optional): New value for the configuration key

**Special usage:**
- `config api_key <provider> <key>`: Set provider API key

**Examples:**
```bash
> config
> config api_key openai sk-your-openai-key
> config DEFAULT_AGENT react
> cfg
```

#### `reload-config`
**Aliases:** `reload`
**Description:** Reload the current agent's configuration
**Usage:** `reload-config`

**Examples:**
```bash
> reload-config
> reload
```

#### `migrate-config`
**Aliases:** `migrate`
**Description:** Migrate legacy configuration to new unified format
**Usage:** `migrate-config`

**Examples:**
```bash
> migrate-config
> migrate
```

#### `check-config`
**Aliases:** `check-migration`
**Description:** Check if configuration needs migration
**Usage:** `check-config`

**Examples:**
```bash
> check-config
> check-migration
```

#### `validate-config`
**Aliases:** `validate`
**Description:** Validate current configuration and check for issues
**Usage:** `validate-config`

**Examples:**
```bash
> validate-config
> validate
```

### Chat Management

#### `new-chat`
**Aliases:** `new`
**Description:** Start a new chat (clear history)
**Usage:** `new-chat`

**Examples:**
```bash
> new-chat
> new
```

#### `list-chats`
**Aliases:** `chats`
**Description:** List available chat history records
**Usage:** `list-chats`

**Examples:**
```bash
> list-chats
> chats
```

#### `load-chat`
**Description:** Load a specific chat history record
**Usage:** `load-chat <chat_id>`

**Parameters:**
- `chat_id` (required): ID of the chat history to load

**Examples:**
```bash
> load-chat chat_20250101_143022
```

### Cryptocurrency Operations

#### `transfer`
**Aliases:** `send`
**Description:** Transfer tokens to a specified address
**Usage:** `transfer <address> <amount> <token>`

**Parameters:**
- `address` (required): Destination wallet address
- `amount` (required): Amount to transfer
- `token` (required): Token symbol or address

**Examples:**
```bash
> transfer 0x123... 0.1 SPO
> send 0xabc... 100 USDC
```

#### `swap`
**Description:** Exchange tokens using an aggregator
**Usage:** `swap <source_token> <target_token> <amount>`

**Parameters:**
- `source_token` (required): Token to swap from
- `target_token` (required): Token to swap to
- `amount` (required): Amount to swap

**Examples:**
```bash
> swap ETH USDC 1.0
> swap BTC SPO 0.01
```

#### `token-info`
**Aliases:** `token`
**Description:** Get token information by address
**Usage:** `token-info <address>`

**Parameters:**
- `address` (required): Token contract address

**Examples:**
```bash
> token-info 0x123...
> token 0xabc...
```

#### `token-by-symbol`
**Aliases:** `symbol`
**Description:** Get token information by symbol
**Usage:** `token-by-symbol <symbol>`

**Parameters:**
- `symbol` (required): Token symbol (e.g., BTC, ETH, SPO)

**Examples:**
```bash
> token-by-symbol SPO
> symbol BTC
```

### Document Management

#### `load-docs`
**Aliases:** `docs`
**Description:** Load documents from the specified directory to the current agent
**Usage:** `load-docs <directory_path>`

**Parameters:**
- `directory_path` (required): Path to directory containing documents

**Examples:**
```bash
> load-docs ./documents
> docs /path/to/docs
```

#### `delete-docs`
**Description:** Delete documents from the current agent or specified agent
**Usage:** `delete-docs [agent_name]`

**Parameters:**
- `agent_name` (optional): Name of agent to delete documents from

**Examples:**
```bash
> delete-docs
> delete-docs react
```

### Toolkit Management

#### `list-toolkit-categories`
**Aliases:** `toolkit-categories`, `categories`
**Description:** List all available toolkit categories
**Usage:** `list-toolkit-categories`

**Examples:**
```bash
> list-toolkit-categories
> toolkit-categories
> categories
```

#### `list-toolkit-tools`
**Aliases:** `toolkit-tools`
**Description:** List tools in a specific category
**Usage:** `list-toolkit-tools <category>`

**Parameters:**
- `category` (required): Toolkit category name

**Examples:**
```bash
> list-toolkit-tools crypto
> toolkit-tools blockchain
```

#### `load-toolkit-tools`
**Aliases:** `load-tools`
**Description:** Load toolkit tools from specified categories
**Usage:** `load-toolkit-tools <categories>`

**Parameters:**
- `categories` (required): Comma-separated list of categories

**Examples:**
```bash
> load-toolkit-tools crypto,blockchain
> load-tools social,storage
```

### LLM Provider Management

#### `llm-status`
**Aliases:** `llm`, `providers`
**Description:** Show LLM provider configuration and availability
**Usage:** `llm-status`

**Output includes:**
- Available providers and their status
- Current default provider
- Model configurations
- Health status for each provider

**Examples:**
```bash
> llm-status
> llm
> providers
```

### Social Media

#### `telegram`
**Aliases:** `tg`
**Description:** Start the Telegram client
**Usage:** `telegram`

**Examples:**
```bash
> telegram
> tg
```

## Command Patterns

### Interactive Mode
Many commands support interactive mode when called without parameters:

```bash
> action chat
# Enters interactive chat mode

> config
# Shows current configuration
```

### Batch Operations
Some commands support batch operations:

```bash
> load-toolkit-tools crypto,blockchain,social
```

### Environment Variable References
Configuration commands support environment variable references:

```bash
> config api_key openai ${OPENAI_API_KEY}
```

## Error Handling

### Common Error Codes

| Error | Description | Solution |
|-------|-------------|----------|
| `Agent not found` | Specified agent doesn't exist | Use `list-agents` to see available agents |
| `Invalid command` | Command not recognized | Use `help` to see available commands |
| `Missing parameter` | Required parameter not provided | Check command syntax with `help <command>` |
| `Configuration error` | Invalid configuration | Use `validate-config` to check configuration |
| `API key missing` | Required API key not set | Set environment variables or use `config` command |

### Debug Mode
Enable debug mode for detailed error information:

```bash
export DEBUG=true
export LOG_LEVEL=debug
python main.py
```

## Best Practices

### Command Usage
- Use tab completion for command names and parameters
- Use aliases for frequently used commands
- Check command help before first use: `help <command>`
- Validate configuration after changes: `validate-config`

### Error Recovery
- Use `system-info` to diagnose issues
- Check `llm-status` for provider problems
- Reload configuration after changes: `reload-config`
- Start fresh chat session if needed: `new-chat`

### Performance
- Load only required toolkit categories
- Use appropriate agent for your use case
- Monitor system resources with `system-info`
- Clear chat history periodically with `new-chat`

## See Also

- [CLI Configuration](../../getting-started/configuration.md)
- [Agent API Reference](../agents/base-agent.md)
- [Tool API Reference](../tools/base-tool.md)
- [Troubleshooting Guide](../../troubleshooting/common-issues.md)

---

FILE: docs/api-reference/graph/base-node.md

# Base Node API Reference

The node system in SpoonOS provides the building blocks for graph execution. All nodes inherit from `BaseNode` and implement specific execution patterns.

## BaseNode Class

```python
from spoon_ai.graph import BaseNode
from abc import ABC
from typing import Generic, TypeVar, Dict, Any, Optional

State = TypeVar('State')

class BaseNode(ABC, Generic[State]):
    def __init__(self, name: str)
    self.name = name

    @abstractmethod
    async def __call__(self, state: State, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Execute the node logic"""
        pass

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}(name='{self.name}')"
```

## BaseNode Methods

### Constructor

#### `__init__(name: str)`

Initialize a node with a unique name.

**Parameters:**
- `name` (`str`): Unique identifier for the node

### Abstract Methods

#### `async __call__(state: State, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]`

Execute the node's logic. Must be implemented by subclasses.

**Parameters:**
- `state` (`State`): Current graph state
- `config` (`Optional[Dict[str, Any]]`): Optional configuration parameters

**Returns:**
- `Dict[str, Any]`: State updates from node execution

**Raises:**
- `NodeExecutionError`: When node execution fails

## RunnableNode Class

```python
from spoon_ai.graph import RunnableNode
from typing import Callable, Any

class RunnableNode(BaseNode[State]):
    def __init__(self, name: str, func: Callable[[State], Any])
    self.func = func

    async def __call__(self, state: State, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        # Execute wrapped function
```

### RunnableNode Features

- Wraps any callable (sync or async) function
- Handles multiple return types automatically
- Provides consistent error handling
- Supports both dict and tuple return formats

### Usage Examples

```python
# Simple function node
async def analyze_sentiment(state: MyState) -> Dict[str, Any]:
    text = state.get("text", "")
    sentiment = "positive" if "good" in text.lower() else "negative"
    return {"sentiment": sentiment, "confidence": 0.85}

node = RunnableNode("sentiment_analyzer", analyze_sentiment)

# Function returning tuple (updates, next_node)
def process_with_routing(state: MyState) -> tuple:
    result = {"processed": True}
    next_node = "success" if result["processed"] else "retry"
    return result, next_node

node = RunnableNode("processor", process_with_routing)
```

## ToolNode Class

```python
from spoon_ai.graph import ToolNode
from typing import List, Any

class ToolNode(BaseNode[State]):
    def __init__(self, name: str, tools: List[Any])
    self.tools = tools

    async def __call__(self, state: State, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        # Execute tools based on state
```

### ToolNode Features

- Executes multiple tools in sequence
- Extracts tool calls from state automatically
- Supports both sync and async tool execution
- Provides comprehensive error handling per tool

### Usage Examples

```python
from spoon_ai.tools import CalculatorTool, SearchTool

# Create tools
calculator = CalculatorTool()
search = SearchTool()
tools = [calculator, search]

# Create tool node
tool_node = ToolNode("tool_executor", tools)

# State with tool calls
state = {
    "tool_calls": [
        {"name": "calculator", "args": {"operation": "add", "a": 10, "b": 5}},
        {"name": "search", "args": {"query": "python tutorial", "limit": 5}}
    ]
}

# Execute tools
result = await tool_node(state)
# result["tool_results"] contains execution results
```

### Tool Call Format

```python
# Expected tool call format in state
{
    "tool_calls": [
        {
            "name": "tool_name",
            "args": {
                "param1": "value1",
                "param2": "value2"
            }
        }
    ]
}

# Tool result format
{
    "tool_results": [
        {
            "tool_call": {...},  # Original call
            "result": {...},     # Tool execution result
            "success": True,     # Execution success flag
            "error": None        # Error message if failed
        }
    ]
}
```

## ConditionNode Class

```python
from spoon_ai.graph import ConditionNode
from typing import Callable

class ConditionNode(BaseNode[State]):
    def __init__(self, name: str, condition_func: Callable[[State], str])
    self.condition_func = condition_func

    async def __call__(self, state: State, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        # Execute condition and return routing decision
```

### ConditionNode Features

- Evaluates conditions for routing decisions
- Supports both sync and async condition functions
- Returns routing decision in standardized format
- Provides clear error messages for condition failures

### Usage Examples

```python
# Simple condition function
def route_by_complexity(state: MyState) -> str:
    query = state.get("query", "")
    return "complex" if len(query) > 100 else "simple"

condition_node = ConditionNode("router", route_by_complexity)

# Async condition function
async def route_by_llm(state: MyState) -> str:
    # Use LLM to decide routing
    decision = await llm_manager.classify(state["query"])
    return decision

condition_node = ConditionNode("llm_router", route_by_llm)
```

### Condition Result Format

```python
# Condition node result
{
    "condition_result": "complex",  # The routing decision
    "next_node": "complex"         # Same as condition_result
}
```

## Custom Node Implementation

### Basic Custom Node

```python
from spoon_ai.graph import BaseNode

class CustomAnalysisNode(BaseNode[MyState]):
    def __init__(self, name: str, model_config: Dict[str, Any]):
        super().__init__(name)
        self.model_config = model_config

    async def __call__(self, state: MyState, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        # Custom logic here
        data = state.get("data", [])

        # Process data
        analysis = await self._analyze_data(data)

        # Return state updates
        return {
            "analysis_result": analysis,
            "processed_at": datetime.now().isoformat(),
            "model_used": self.model_config.get("model")
        }

    async def _analyze_data(self, data: List[Any]) -> Dict[str, Any]:
        # Implementation details
        pass
```

### Node with Configuration

```python
class ConfigurableNode(BaseNode[MyState]):
    def __init__(self, name: str, api_key: str, timeout: int = 30):
        super().__init__(name)
        self.api_key = api_key
        self.timeout = timeout

    async def __call__(self, state: MyState, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        # Merge instance config with call config
        effective_config = {**self.__dict__, **(config or {})}

        # Use configuration in execution
        result = await self._call_external_api(
            state["query"],
            api_key=effective_config["api_key"],
            timeout=effective_config["timeout"]
        )

        return {"api_result": result}
```

## Node Patterns and Best Practices

### State Transformation Pattern

```python
class TransformerNode(BaseNode[MyState]):
    async def __call__(self, state: MyState, config=None) -> Dict[str, Any]:
        # Transform input data
        raw_data = state.get("raw_data", [])
        transformed = [self._transform_item(item) for item in raw_data]

        return {
            "transformed_data": transformed,
            "transformation_count": len(transformed),
            "transformation_timestamp": datetime.now().isoformat()
        }
```

### Validation Node Pattern

```python
class ValidationNode(BaseNode[MyState]):
    def __init__(self, name: str, validators: List[Callable]):
        super().__init__(name)
        self.validators = validators

    async def __call__(self, state: MyState, config=None) -> Dict[str, Any]:
        errors = []
        for validator in self.validators:
            try:
                validator(state)
            except Exception as e:
                errors.append(str(e))

        return {
            "validation_passed": len(errors) == 0,
            "validation_errors": errors,
            "validated_at": datetime.now().isoformat()
        }
```

### Aggregation Node Pattern

```python
class AggregationNode(BaseNode[MyState]):
    async def __call__(self, state: MyState, config=None) -> Dict[str, Any]:
        # Aggregate results from parallel branches
        branch_results = state.get("branch_results", [])
        aggregated = self._aggregate_results(branch_results)

        return {
            "aggregated_result": aggregated,
            "branch_count": len(branch_results),
            "aggregation_method": "weighted_average"
        }
```

## Error Handling

### Node-Level Error Handling

```python
class ResilientNode(BaseNode[MyState]):
    async def __call__(self, state: MyState, config=None) -> Dict[str, Any]:
        try:
            result = await self._execute_logic(state)
            return {
                "result": result,
                "success": True,
                "error": None
            }
        except Exception as e:
            logger.error(f"Node {self.name} failed: {e}")
            return {
                "result": None,
                "success": False,
                "error": str(e),
                "error_type": type(e).__name__
            }
```

### Retry Logic

```python
class RetryNode(BaseNode[MyState]):
    def __init__(self, name: str, max_retries: int = 3):
        super().__init__(name)
        self.max_retries = max_retries

    async def __call__(self, state: MyState, config=None) -> Dict[str, Any]:
        for attempt in range(self.max_retries):
            try:
                return await self._execute_with_retry(state)
            except Exception as e:
                if attempt == self.max_retries - 1:
                    raise NodeExecutionError(
                        f"Failed after {self.max_retries} attempts",
                        node_name=self.name,
                        original_error=e,
                        state=state
                    ) from e
                await asyncio.sleep(2 ** attempt)  # Exponential backoff
```

## Testing Nodes

### Unit Testing Pattern

```python
import pytest
from unittest.mock import AsyncMock

@pytest.mark.asyncio
async def test_custom_node():
    node = CustomAnalysisNode("test_node", {"model": "test"})

    # Mock the internal method
    node._analyze_data = AsyncMock(return_value={"score": 0.95})

    state = {"data": [1, 2, 3]}
    result = await node(state)

    assert result["analysis_result"]["score"] == 0.95
    assert result["success"] is True
```

### Integration Testing

```python
@pytest.mark.asyncio
async def test_node_in_graph():
    # Create a simple graph with the node
    graph = StateGraph(MyState)
    graph.add_node("test_node", CustomAnalysisNode("test", {}))
    graph.set_entry_point("test_node")
    graph.add_edge("test_node", END)

    compiled = graph.compile()
    result = await compiled.invoke({"data": [1, 2, 3]})

    assert "analysis_result" in result
```

## Performance Considerations

### Async Best Practices

```python
class OptimizedNode(BaseNode[MyState]):
    async def __call__(self, state: MyState, config=None) -> Dict[str, Any]:
        # Use asyncio.gather for concurrent operations
        results = await asyncio.gather(
            self._fetch_data_a(state),
            self._fetch_data_b(state),
            self._process_data(state)
        )

        return {
            "combined_result": self._combine_results(results)
        }
```

### Memory Management

```python
class MemoryEfficientNode(BaseNode[MyState]):
    async def __call__(self, state: MyState, config=None) -> Dict[str, Any]:
        # Process data in chunks to manage memory
        data = state.get("large_dataset", [])
        chunk_size = 1000

        results = []
        for i in range(0, len(data), chunk_size):
            chunk = data[i:i + chunk_size]
            chunk_result = await self._process_chunk(chunk)
            results.extend(chunk_result)

            # Allow other tasks to run
            await asyncio.sleep(0)

        return {"processed_data": results}
```

## See Also

- [StateGraph API](state-graph.md) - How to use nodes in graphs
- [Graph Agent API](graph-agent.md) - Agent-level node execution
- [Graph System Overview](../../core-concepts/graph-system.md) - Node concepts and patterns

---

FILE: docs/api-reference/graph/graph-agent.md

# GraphAgent API Reference

The `GraphAgent` class provides a high-level agent interface for StateGraph execution, managing memory, state persistence, and conversation flow.

## Class Definition

```python
from spoon_ai.graph import GraphAgent, StateGraph
from typing import Optional

class GraphAgent:
    def __init__(
        self,
        name: str,
        graph: StateGraph,
        preserve_state: bool = False,
        memory: Optional[Memory] = None,
        memory_path: Optional[str] = None,
        session_id: Optional[str] = None,
        max_metadata_size: int = 1024,
        **kwargs
    )
```

## Constructor Parameters

### Required Parameters

- **name** (`str`): Unique identifier for the agent
- **graph** (`StateGraph`): Compiled StateGraph instance to execute

### Optional Parameters

- **preserve_state** (`bool`, default: `False`): Whether to maintain state between executions
- **memory** (`Optional[Memory]`): Custom memory instance (auto-created if None)
- **memory_path** (`Optional[str]`): Path for memory storage
- **session_id** (`Optional[str]`): Session identifier for memory
- **max_metadata_size** (`int`, default: `1024`): Maximum metadata size in bytes
- **kwargs**: Additional configuration options

## Core Methods

### Execution Methods

#### `async run(request: Optional[str] = None) -> str`

Execute the graph with a user request.

**Parameters:**
- `request` (`Optional[str]`): User input string

**Returns:**
- `str`: Graph execution result

**Raises:**
- `Exception`: Graph execution errors

**Example:**
```python
agent = GraphAgent(name="crypto_analyzer", graph=build_crypto_graph())

# Execute with a request
result = await agent.run("Analyze BTC price trends")
print(result)

# Execute without request (uses preserved state)
result = await agent.run()
```

#### `async chat(messages: List[Dict[str, Any]], **kwargs) -> Dict[str, Any]`

Process a conversation with multiple messages (for compatibility).

**Parameters:**
- `messages` (`List[Dict[str, Any]]`): List of conversation messages
- `**kwargs`: Additional execution parameters

**Returns:**
- `Dict[str, Any]`: Response with conversation metadata

**Example:**
```python
messages = [
    {"role": "user", "content": "What's the market doing?"},
    {"role": "assistant", "content": "Let me analyze..."}
]

response = await agent.chat(messages)
print(response["output"])
```

### Memory Management

#### `get_memory_statistics() -> Dict[str, Any]`

Get comprehensive memory usage statistics.

**Returns:**
- `Dict[str, Any]`: Memory statistics including:
  - `total_messages`: Number of stored messages
  - `session_id`: Current session identifier
  - `storage_path`: Memory storage location
  - `last_updated`: Last update timestamp
  - `file_size`: Memory file size in bytes

**Example:**
```python
stats = agent.get_memory_statistics()
print(f"Messages: {stats['total_messages']}")
print(f"Storage: {stats['file_size']} bytes")
```

#### `set_memory_metadata(key: str, value: Any)`

Set metadata in the agent's memory.

**Parameters:**
- `key` (`str`): Metadata key
- `value` (`Any`): Metadata value

**Example:**
```python
agent.set_memory_metadata("last_topic", "cryptocurrency")
agent.set_memory_metadata("user_preferences", {"theme": "dark"})
```

#### `clear_memory()`

Clear all messages and reset memory to initial state.

**Example:**
```python
agent.clear_memory()  # Reset conversation history
```

### State Management

#### `get_current_state() -> Optional[Dict[str, Any]]`

Get the current preserved state (if state preservation is enabled).

**Returns:**
- `Optional[Dict[str, Any]]`: Current state or None if not preserving state

**Example:**
```python
current_state = agent.get_current_state()
if current_state:
    print(f"Current step: {current_state.get('current_step', 0)}")
```

#### `reset_state()`

Reset the agent's preserved state.

**Example:**
```python
agent.reset_state()  # Clear preserved state for fresh start
```

### Execution Control

#### `get_execution_metadata() -> Dict[str, Any]`

Get metadata about the last execution.

**Returns:**
- `Dict[str, Any]`: Execution metadata including:
  - `execution_successful`: Whether last execution succeeded
  - `execution_time`: Timestamp of last execution
  - `last_request`: Truncated version of last request

**Example:**
```python
metadata = agent.get_execution_metadata()
if metadata.get("execution_successful"):
    print("Last execution succeeded")
```

## Advanced Usage

### State Preservation

```python
# Create agent with state preservation
agent = GraphAgent(
    name="persistent_agent",
    graph=my_graph,
    preserve_state=True,
    memory_path="./agent_memory"
)

# First execution
await agent.run("Start analysis")

# Second execution (continues from previous state)
await agent.run("Continue with next step")
```

### Custom Memory Configuration

```python
from spoon_ai.graph.agent import Memory

# Custom memory instance
custom_memory = Memory(
    storage_path="/custom/path",
    session_id="custom_session"
)

agent = GraphAgent(
    name="custom_memory_agent",
    graph=my_graph,
    memory=custom_memory
)
```

### Error Handling

```python
try:
    result = await agent.run("Complex analysis request")
    print(f"Success: {result}")
except Exception as e:
    print(f"Execution failed: {e}")

    # Check execution metadata for debugging
    metadata = agent.get_execution_metadata()
    print(f"Last successful: {metadata.get('execution_successful')}")

    # Reset state if needed
    agent.reset_state()
```

### Memory Analysis

```python
# Get detailed memory statistics
stats = agent.get_memory_statistics()
print(f"""
Memory Statistics:
- Total messages: {stats['total_messages']}
- Session: {stats['session_id']}
- Storage size: {stats['file_size']} bytes
- Last updated: {stats.get('last_updated')}
""")

# Set custom metadata for tracking
agent.set_memory_metadata("analysis_count", 42)
agent.set_memory_metadata("last_model_used", "gpt-4")
```

## Integration Patterns

### With LLM Manager

```python
from spoon_ai.llm import LLMManager

# Agent with LLM integration in graph
llm_manager = LLMManager()
agent = GraphAgent(
    name="llm_agent",
    graph=build_llm_integrated_graph(llm_manager)
)

response = await agent.run("Explain quantum computing")
```

### With Tool Manager

```python
from spoon_ai.tools import ToolManager

# Agent with tool execution capabilities
tool_manager = ToolManager()
agent = GraphAgent(
    name="tool_agent",
    graph=build_tool_integrated_graph(tool_manager)
)

result = await agent.run("Calculate 15 * 23 and search for Python tutorials")
```

### Multi-Agent Coordination

```python
# Multiple agents working together
analyzer_agent = GraphAgent("analyzer", analysis_graph)
summarizer_agent = GraphAgent("summarizer", summary_graph)

# Chain executions
analysis_result = await analyzer_agent.run("Analyze this data")
final_result = await summarizer_agent.run(f"Summarize: {analysis_result}")
```

## Memory Persistence

The GraphAgent automatically persists memory to disk:

```python
# Memory is saved to: ~/.spoon_ai/memory/{agent_name}_{timestamp}.json
agent = GraphAgent("my_agent", my_graph)

# Memory persists between sessions
await agent.run("First message")  # Creates memory file
# ... restart application ...
agent2 = GraphAgent("my_agent", my_graph)  # Loads previous memory
```

## Best Practices

### Memory Management
- Use descriptive session IDs for different conversation contexts
- Clear memory periodically to prevent unbounded growth
- Set meaningful metadata for analysis and debugging

### State Preservation
- Enable state preservation for multi-turn conversations
- Reset state when switching contexts
- Validate preserved state before reuse

### Error Recovery
- Always check execution metadata after failures
- Implement retry logic with exponential backoff
- Use checkpoints for complex operations

### Performance Optimization
- Monitor memory statistics regularly
- Use appropriate metadata size limits
- Consider memory cleanup for long-running agents

## See Also

- [StateGraph API](state-graph.md) - Underlying graph execution engine
- [Memory System](../../core-concepts/Short-term%20memory.md) - Memory management details
- [Agent Architecture](../../core-concepts/agents-detailed.md) - Agent design patterns

---

FILE: docs/api-reference/graph/index.md

# Graph System API Reference

The Graph System in SpoonOS provides a powerful, structured approach to building complex multi-step workflows through composable nodes and deterministic execution flows.

## Overview

SpoonOS's graph system enables:

- **Deterministic Execution**: Structured control flow with explicit nodes and edges
- **Intelligent Routing**: Multiple routing strategies (LLM-powered, rule-based, conditional)
- **Parallel Processing**: Concurrent execution with configurable join strategies
- **State Management**: Type-safe state handling with reducer-based merging
- **Memory Integration**: Persistent context across executions

## Core Components

### [StateGraph](state-graph.md)
The main graph execution engine providing workflow orchestration.

**Key Features:**
- Node and edge management
- Conditional routing
- Parallel execution groups
- LLM-powered routing
- Monitoring and metrics

```python
from spoon_ai.graph import StateGraph

graph = StateGraph(MyState)
graph.add_node("process", my_node)
graph.add_edge("process", "output")
compiled = graph.compile()
```

### [GraphAgent](graph-agent.md)
High-level agent interface for graph execution with memory management and conversation handling.

**Key Features:**
- Persistent memory management
- State preservation across executions
- Execution metadata tracking
- Error recovery and retry logic

```python
from spoon_ai.graph import GraphAgent

agent = GraphAgent("my_agent", graph, preserve_state=True)
result = await agent.run("Process this request")
```

### [Base Node Classes](base-node.md)
Building blocks for graph execution with specialized node types.

**Node Types:**
- `RunnableNode`: Wraps functions (sync/async)
- `ToolNode`: Executes tools from state
- `ConditionNode`: Handles routing decisions
- `BaseNode`: Abstract base for custom nodes

```python
from spoon_ai.graph import RunnableNode, ToolNode

# Function node
func_node = RunnableNode("analyzer", analyze_function)

# Tool node
tool_node = ToolNode("executor", [calculator_tool, search_tool])
```

## Quick Start

### Basic Graph Creation

```python
from typing import TypedDict
from spoon_ai.graph import StateGraph, END

class MyState(TypedDict):
    query: str
    result: str

async def process_query(state: MyState) -> dict:
    return {"result": f"Processed: {state['query']}"}

# Build graph
graph = StateGraph(MyState)
graph.add_node("processor", process_query)
graph.set_entry_point("processor")
graph.add_edge("processor", END)

# Execute
compiled = graph.compile()
result = await compiled.invoke({"query": "Hello Graph!"})
print(result["result"])  # "Processed: Hello Graph!"
```

### Agent with Memory

```python
from spoon_ai.graph import GraphAgent

# Create agent
agent = GraphAgent(
    name="chatbot",
    graph=my_graph,
    preserve_state=True
)

# Execute with memory persistence
response1 = await agent.run("Hello")
response2 = await agent.run("How are you?")  # Context preserved
```

## Advanced Patterns

### Conditional Routing

```python
def route_by_length(state: MyState) -> str:
    return "long" if len(state["query"]) > 50 else "short"

graph.add_conditional_edges(
    "processor",
    route_by_length,
    {"long": "detailed_analysis", "short": "quick_response"}
)
```

### Parallel Execution

```python
graph.add_parallel_group(
    "parallel_tasks",
    ["fetch_data", "analyze_trends", "generate_report"],
    {"join_strategy": "all_complete"}
)
```

### LLM-Powered Routing

```python
graph.enable_llm_routing({
    "model": "gpt-4",
    "temperature": 0.1
})
```

## State Management

### State Schema Definition

```python
from typing import TypedDict, Optional, Annotated

class AnalysisState(TypedDict):
    user_query: str
    analysis_type: str
    results: Annotated[Optional[Dict], None]
    confidence: Annotated[float, "0.0 to 1.0"]
    metadata: Annotated[Optional[Dict], None]
```

### State Updates

Nodes return dictionaries that update the graph state:

```python
async def analysis_node(state: AnalysisState) -> Dict[str, Any]:
    # Process state
    result = await analyze(state["user_query"])

    # Return state updates
    return {
        "results": result,
        "confidence": result.confidence,
        "analysis_type": "comprehensive"
    }
```

## Memory Integration

### Agent Memory

```python
agent = GraphAgent("analyzer", graph, preserve_state=True)

# Memory persists across executions
await agent.run("First analysis")
stats = agent.get_memory_statistics()
```

### Node-Level Memory

```python
async def load_context(state: MyState) -> Dict[str, Any]:
    # Load from persistent storage
    context = await load_user_context(state["user_id"])
    return {"context": context}

async def save_context(state: MyState) -> Dict[str, Any]:
    # Save to persistent storage
    await save_user_context(state["user_id"], state["context"])
    return {}
```

## Error Handling

### Node-Level Errors

```python
from spoon_ai.graph.exceptions import NodeExecutionError

async def robust_node(state: MyState) -> Dict[str, Any]:
    try:
        result = await risky_operation(state)
        return {"result": result, "success": True}
    except Exception as e:
        return {"error": str(e), "success": False}
```

### Graph-Level Recovery

```python
try:
    result = await compiled.invoke(state)
except Exception as e:
    # Handle graph execution errors
    print(f"Graph execution failed: {e}")
    # Implement recovery logic
```

## Monitoring and Debugging

### Execution Metrics

```python
graph.enable_monitoring(["execution_time", "success_rate"])
compiled = graph.compile()
result = await compiled.invoke(state)
metrics = compiled.get_execution_metrics()
```

### Agent Statistics

```python
stats = agent.get_memory_statistics()
metadata = agent.get_execution_metadata()
```

## Best Practices

### Node Design
- Keep nodes focused on single responsibilities
- Use descriptive node names
- Handle errors gracefully within nodes

### State Management
- Define clear state schemas
- Use type annotations for validation
- Keep state size manageable

### Routing Strategy
- Prefer conditional edges for deterministic routing
- Use LLM routing for complex decisions
- Combine multiple routing approaches

### Performance
- Enable monitoring to identify bottlenecks
- Use parallel groups for I/O operations
- Consider state serialization impact

## Integration Examples

### With LLM Manager

```python
from spoon_ai.llm import LLMManager

llm_manager = LLMManager()

async def llm_node(state: MyState) -> Dict[str, Any]:
    response = await llm_manager.generate(
        prompt=state["query"],
        model="gpt-4"
    )
    return {"llm_response": response}
```

### With Tool Manager

```python
from spoon_ai.tools import ToolManager

tool_manager = ToolManager()

async def tool_node(state: MyState) -> Dict[str, Any]:
    results = []
    for tool_call in state["tool_calls"]:
        result = await tool_manager.execute_tool(tool_call)
        results.append(result)
    return {"tool_results": results}
```

## Migration from Other Frameworks

### From LangChain/LangGraph

```python
# SpoonOS equivalent of LangGraph patterns
from spoon_ai.graph import StateGraph, END

# Similar API design
graph = StateGraph(State)
graph.add_node("node", my_function)
graph.add_edge("node", END)
```

### From Custom Workflow Engines

```python
# SpoonOS provides more structure and type safety
graph = StateGraph(TypedDictState)  # Type-safe state
graph.enable_monitoring()  # Built-in monitoring
agent = GraphAgent("name", graph)  # Memory and persistence
```

## Troubleshooting

### Common Issues

- **Node not found**: Ensure all referenced nodes are added to the graph
- **Edge validation errors**: Check that source/destination nodes exist
- **State serialization**: Keep state objects simple and serializable
- **Memory growth**: Clear agent memory periodically for long-running processes

### Debugging Tools

```python
# Enable detailed logging
import logging
logging.getLogger("spoon_ai.graph").setLevel(logging.DEBUG)

# Inspect graph structure
print(graph.get_graph())

# Monitor execution
graph.enable_monitoring()
metrics = compiled.get_execution_metrics()
```

## See Also

- [Graph System Overview](../../core-concepts/graph-system.md) - Conceptual introduction
- [Graph Crypto Analysis](../../examples/graph-crypto-analysis.md) - Complete example
- [Intent Graph Demo](../../examples/intent-graph-demo.md) - Advanced patterns
- [Performance Optimization](../../troubleshooting/performance.md) - Tuning guides

---

FILE: docs/api-reference/graph/state-graph.md

# StateGraph API Reference

The `StateGraph` class is the core of SpoonOS's graph execution engine, providing a structured way to build complex, multi-step workflows with deterministic control flow.

## Class Definition

```python
from spoon_ai.graph import StateGraph
from typing import TypeVar, TypedDict

State = TypeVar('State')

class StateGraph(Generic[State]):
    def __init__(
        self,
        state_schema: type,
        checkpointer: Optional[Any] = None,
        config_schema: Optional[type] = None
    )
```

## Constructor Parameters

### Required Parameters

- **state_schema** (`type`): Type definition for the graph state (usually a TypedDict class)

### Optional Parameters

- **checkpointer** (`Optional[Any]`): Checkpointer instance for state persistence
- **config_schema** (`Optional[type]`): Configuration schema for the graph

## Core Methods

### Graph Construction

#### `add_node(node_name: str, node: Union[BaseNode[State], Callable]) -> StateGraph`

Add a node to the graph. Nodes can be either `BaseNode` instances or callable functions.

**Parameters:**
- `node_name` (str): Unique identifier for the node
- `node` (Union[BaseNode, Callable]): Node implementation

**Returns:**
- `StateGraph`: Self for method chaining

**Example:**
```python
async def analyze_query(state: MyState) -> Dict[str, Any]:
    return {"analysis": "processed"}

graph = StateGraph(MyState)
graph.add_node("analyzer", analyze_query)
```

#### `add_edge(start_node: str, end_node: str) -> StateGraph`

Add an unconditional edge between nodes.

**Parameters:**
- `start_node` (str): Starting node name
- `end_node` (str): Ending node name

**Returns:**
- `StateGraph`: Self for method chaining

**Example:**
```python
graph.add_edge("analyzer", "summarizer")
```

#### `set_entry_point(node_name: str) -> StateGraph`

Set the graph's entry point node.

**Parameters:**
- `node_name` (str): Name of the entry point node

**Returns:**
- `StateGraph`: Self for method chaining

**Example:**
```python
graph.set_entry_point("analyzer")
```

### Compilation and Execution

#### `compile(checkpointer: Optional[Any] = None) -> CompiledGraph`

Compile the graph into an executable form.

**Parameters:**
- `checkpointer` (Optional[Any]): Optional checkpointer for execution

**Returns:**
- `CompiledGraph`: Compiled executable graph

**Raises:**
- `GraphConfigurationError`: If graph has configuration issues

**Example:**
```python
compiled_graph = graph.compile()
result = await compiled_graph.invoke({"query": "analyze market"})
```

## State Management

### State Schema Definition

```python
from typing import TypedDict, Optional, Annotated

class MyState(TypedDict):
    user_query: str
    analysis_result: Optional[str]
    confidence_score: Annotated[float, "0.0 to 1.0"]
    metadata: Annotated[Optional[Dict[str, Any]], None]
```

### State Updates

Nodes return dictionaries that update the graph state:

```python
async def analysis_node(state: MyState) -> Dict[str, Any]:
    # Process state
    result = await analyze(state["user_query"])

    # Return state updates
    return {
        "results": result,
        "confidence": result.confidence,
        "analysis_type": "comprehensive"
    }
```

## Execution Flow

### Basic Execution

```python
# Build graph
graph = StateGraph(MyState)
graph.add_node("process", my_node)
graph.add_edge("process", "output")
graph.set_entry_point("process")
graph.add_edge("process", END)

# Execute
compiled = graph.compile()
result = await compiled.invoke({
    "user_query": "Hello Graph!",
    "analysis_result": None
})

print(result["analysis_result"])
```

### With Conditional Routing

```python
def route_by_complexity(state: MyState) -> str:
    query_length = len(state.get("user_query", ""))
    return "deep_analysis" if query_length > 100 else "quick_answer"

graph.add_conditional_edges(
    "analyze",
    route_by_complexity,
    {
        "deep_analysis": "deep_analyzer",
        "quick_answer": "quick_responder"
    }
)
```

## See Also

- [Graph Agent API](graph-agent.md) - Agent wrapper for graphs
- [Base Node API](base-node.md) - Node implementation details
- [Graph System Overview](../../core-concepts/graph-system.md) - Conceptual overview

---

FILE: docs/api-reference/index.md

# API Reference

Complete API documentation for all SpoonOS components and modules.

## Core Components

### [Graph System](graph/index.md)
Powerful workflow orchestration engine for complex multi-step processes.

- **[StateGraph](graph/state-graph.md)**: Main graph execution engine with routing and parallel processing
- **[GraphAgent](graph/graph-agent.md)**: High-level agent interface with memory management
- **[Base Nodes](graph/base-node.md)**: Building blocks for graph execution (RunnableNode, ToolNode, ConditionNode)

**Key Features:**
- Deterministic control flow with explicit nodes and edges
- Intelligent routing (LLM-powered, rule-based, conditional)
- Parallel execution with configurable join strategies
- Type-safe state management with reducer-based merging
- Memory integration and persistence

```python
from spoon_ai.graph import StateGraph, GraphAgent

# Build a graph
graph = StateGraph(MyState)
graph.add_node("analyze", analyze_node)
graph.set_entry_point("analyze")

# Create agent with memory
agent = GraphAgent("analyzer", graph, preserve_state=True)
result = await agent.run("Analyze this data")
```

### [Agents](agents/base-agent.md)
Intelligent conversation and task execution systems.

- **[BaseAgent](agents/base-agent.md)**: Foundation class for all agents with LLM integration

**Key Features:**
- LLM manager integration for natural language processing
- Tool management and execution
- Conversation handling with message history
- System prompt management
- Extensible architecture for custom agent types

```python
from spoon_ai.agents import BaseAgent

agent = BaseAgent(
    name="assistant",
    system_prompt="You are a helpful assistant"
)
response = await agent.run("Hello, how can I help?")
```

### [Tools](tools/base-tool.md)
Extensible capability system for external integrations and computations.

- **[BaseTool](tools/base-tool.md)**: Abstract base class for all tools
- **[Built-in Tools](tools/builtin-tools.md)**: Pre-implemented tools for common tasks

**Key Features:**
- Standardized tool interface with async execution
- Parameter validation and error handling
- Tool discovery and management
- Integration with agent systems

```python
from spoon_ai.tools import BaseTool

class CustomTool(BaseTool):
    name = "custom_tool"
    description = "My custom tool"

    async def execute(self, param: str) -> str:
        return f"Processed: {param}"
```

### [CLI](cli/commands.md)
Command-line interface for SpoonOS management and operations.

- **[Commands](cli/commands.md)**: Available CLI commands and usage

**Key Features:**
- Project initialization and configuration
- Agent and tool management
- Development server controls
- Deployment and monitoring tools

```bash
# Initialize a new SpoonOS project
spoon init my-project

# Run development server
spoon dev

# Deploy to production
spoon deploy
```

## Integration Patterns

### Graph + Agent Integration

```python
from spoon_ai.graph import StateGraph, GraphAgent
from spoon_ai.agents import BaseAgent

# Create graph-based agent
graph = StateGraph(MyState)
# ... configure graph ...

agent = GraphAgent("graph_agent", graph)

# Use like regular agent
result = await agent.run("Execute complex workflow")
```

### Tool + Graph Integration

```python
from spoon_ai.graph import StateGraph, ToolNode
from spoon_ai.tools import CalculatorTool

# Add tools to graph
tools = [CalculatorTool()]
tool_node = ToolNode("calculator", tools)

graph = StateGraph(MyState)
graph.add_node("calculate", tool_node)
```

### Multi-Component Architecture

```python
# Complete SpoonOS application
from spoon_ai.agents import BaseAgent
from spoon_ai.tools import ToolManager
from spoon_ai.llm import LLMManager
from spoon_ai.graph import StateGraph

# Orchestrate all components
llm_manager = LLMManager()
tool_manager = ToolManager()
agent = BaseAgent(
    name="full_agent",
    llm_manager=llm_manager,
    tool_manager=tool_manager
)

# Add graph capabilities
graph = StateGraph(ComplexState)
graph_agent = GraphAgent("workflow", graph)
```

## Error Handling

All SpoonOS components use consistent error handling:

```python
from spoon_ai.graph.exceptions import GraphExecutionError
from spoon_ai.tools.exceptions import ToolError
from spoon_ai.agents.exceptions import AgentError

try:
    result = await agent.run("Complex task")
except GraphExecutionError as e:
    print(f"Graph execution failed: {e}")
except ToolError as e:
    print(f"Tool execution failed: {e}")
except AgentError as e:
    print(f"Agent execution failed: {e}")
```

## Configuration

### Environment Variables

```bash
# LLM Configuration
export SPOON_LLM_PROVIDER=openai
export SPOON_OPENAI_API_KEY=your_key

# Database Configuration
export SPOON_DATABASE_URL=postgresql://...

# Memory Configuration
export SPOON_MEMORY_PATH=./memory
```

### Configuration Files

```json
{
  "llm": {
    "provider": "openai",
    "model": "gpt-4",
    "temperature": 0.7
  },
  "tools": {
    "enabled": ["calculator", "search", "web"],
    "custom_path": "./tools"
  },
  "graph": {
    "monitoring": true,
    "max_parallel": 5
  }
}
```

## Performance Optimization

### Graph Optimization

```python
# Enable monitoring
graph.enable_monitoring(["execution_time", "memory_usage"])

# Use parallel execution
graph.add_parallel_group("parallel_ops", ["task1", "task2"])

# Optimize state size
class OptimizedState(TypedDict):
    essential_data: str  # Keep minimal state
```

### Agent Optimization

```python
# Configure memory limits
agent = BaseAgent(
    name="optimized",
    max_memory_items=1000,
    memory_cleanup_interval=3600
)

# Use streaming for large responses
response = await agent.run("Large analysis", stream=True)
```

## Migration Guides

### From LangChain

```python
# LangChain style
# chain = LLMChain(llm=llm, prompt=prompt)

# SpoonOS equivalent
agent = BaseAgent(name="migrated")
agent.set_system_prompt("Your prompt here")
response = await agent.run("Query")
```

### From Custom Frameworks

```python
# Custom workflow engine
# workflow.add_step(step1).add_step(step2)

# SpoonOS equivalent
graph = StateGraph(State)
graph.add_node("step1", step1_func)
graph.add_node("step2", step2_func)
graph.add_edge("step1", "step2")
```

## See Also

- **[Getting Started](../getting-started/quick-start.md)**: Quick start guide
- **[Core Concepts](../../core-concepts/)**: Understanding SpoonOS architecture
- **[Examples](../../examples/)**: Complete working examples
- **[Troubleshooting](../../troubleshooting/)**: Common issues and solutions

---

FILE: docs/api-reference/llm/config-manager.md

# Configuration Manager API Reference

The `ConfigurationManager` handles loading, validation, and management of LLM provider configurations from various sources including environment variables, configuration files, and runtime settings.

> **Note (Nov 2025):** The core SDK now defaults to environment-driven configuration. Use the `spoon-cli` configuration manager (or set environment variables manually) to sync `config.json` values before instantiating `ConfigurationManager()`.

## Class Definition

```python
from spoon_ai.llm import ConfigurationManager, ProviderConfig
from typing import Optional, Dict, Any, List

class ConfigurationManager:
    def __init__(self, config_path: Optional[str] = None)
```

## Constructor Parameters

### Optional Parameters

- **config_path** (`Optional[str]`): Path to configuration file (JSON/TOML)

## ProviderConfig Class

```python
from dataclasses import dataclass, field
from typing import Optional, Dict, Any

@dataclass
class ProviderConfig:
    name: str                                    # Provider name
    api_key: str                                 # API key
    base_url: Optional[str] = None              # Custom base URL
    model: str = ""                              # Default model
    max_tokens: int = 4096                       # Max tokens
    temperature: float = 0.3                     # Temperature setting
    timeout: int = 30                            # Request timeout
    retry_attempts: int = 3                      # Retry attempts
    custom_headers: Dict[str, str] = field(default_factory=dict)
    extra_params: Dict[str, Any] = field(default_factory=dict)

    def model_dump(self) -> Dict[str, Any]: ...  # Convert to dict
```

## Configuration Loading

### `load_from_file(file_path: str) -> Dict[str, Any]`

Load configuration from a JSON or TOML file.

**Parameters:**
- `file_path` (`str`): Path to configuration file

**Returns:**
- `Dict[str, Any]`: Loaded configuration

**Raises:**
- `ConfigurationError`: If file format is invalid or file not found

**Example:**
```python
import os
from spoon_ai.llm import ConfigurationManager

# Populate required environment variables before instantiating the manager
os.environ["OPENAI_API_KEY"] = "sk-..."

config_manager = ConfigurationManager()
```

### `load_from_env() -> Dict[str, Any]`

Load configuration from environment variables.

**Returns:**
- `Dict[str, Any]`: Configuration from environment

**Example:**
```python
# Environment variables:
# OPENAI_API_KEY=sk-...
# ANTHROPIC_API_KEY=sk-ant-...
# DEFAULT_LLM_PROVIDER=openai

config = config_manager.load_from_env()
```

Environment overrides also control provider priority:

- `DEFAULT_LLM_PROVIDER` selects the preferred provider (e.g. `anthropic`).
- `LLM_FALLBACK_CHAIN` lists comma-separated providers for cascading retries (e.g. `anthropic,openai,gemini`).

When using `spoon-cli`, these variables are exported automatically after `config.json` loads. If you instantiate the SDK directly, set them yourself before calling `ConfigurationManager()`.

### `merge_configs(base_config: Dict, override_config: Dict) -> Dict[str, Any]`

Merge two configurations with override priority.

**Parameters:**
- `base_config` (`Dict`): Base configuration
- `override_config` (`Dict`): Override configuration

**Returns:**
- `Dict[str, Any]`: Merged configuration

**Example:**
```python
base = {"provider": "openai", "model": "gpt-4"}
override = {"model": "gpt-4.1", "temperature": 0.7}

merged = config_manager.merge_configs(base, override)
# Result: {"provider": "openai", "model": "gpt-4.1", "temperature": 0.7}
```

## Provider Management

### `get_provider_config(provider_name: str) -> Optional[ProviderConfig]`

Get configuration for a specific provider.

**Parameters:**
- `provider_name` (`str`): Name of the provider

**Returns:**
- `Optional[ProviderConfig]`: Provider configuration or None

**Example:**
```python
openai_config = config_manager.get_provider_config("openai")
if openai_config:
    print(f"Model: {openai_config.model}")
```

### `set_provider_config(provider_name: str, config: Dict[str, Any]) -> None`

Set configuration for a specific provider.

**Parameters:**
- `provider_name` (`str`): Name of the provider
- `config` (`Dict[str, Any]`): Provider configuration

**Example:**
```python
config_manager.set_provider_config("openai", {
    "api_key": "sk-...",
    "model": "gpt-4.1",
    "temperature": 0.7
})
```

### `list_providers() -> List[str]`

List all configured providers.

**Returns:**
- `List[str]`: List of provider names

**Example:**
```python
providers = config_manager.list_providers()
print(f"Configured providers: {providers}")
# Output: ["openai", "anthropic", "gemini"]
```

### `validate_provider_config(provider_name: str, config: Dict[str, Any]) -> bool`

Validate configuration for a provider.

**Parameters:**
- `provider_name` (`str`): Name of the provider
- `config` (`Dict[str, Any]`): Configuration to validate

**Returns:**
- `bool`: True if valid, False otherwise

**Example:**
```python
is_valid = config_manager.validate_provider_config("openai", {
    "api_key": "sk-...",
    "model": "gpt-4.1"
})
```

## Global Settings

### `get_global_config() -> Dict[str, Any]`

Get global LLM configuration settings.

**Returns:**
- `Dict[str, Any]`: Global configuration

**Example:**
```python
global_config = config_manager.get_global_config()
print(f"Default provider: {global_config.get('default_provider')}")
```

### `set_global_config(config: Dict[str, Any]) -> None`

Set global LLM configuration settings.

**Parameters:**
- `config` (`Dict[str, Any]`): Global configuration

**Example:**
```python
config_manager.set_global_config({
    "default_provider": "openai",
    "default_model": "gpt-4.1",
    "timeout": 30,
    "retry_attempts": 3
})
```

## Configuration Files

### JSON Configuration Format

```json
{
  "llm": {
    "default_provider": "openai",
    "default_model": "gpt-4.1",
    "timeout": 30,
    "providers": {
      "openai": {
        "api_key": "sk-your_openai_key_here",
        "model": "gpt-4.1",
        "temperature": 0.7,
        "max_tokens": 4096
      },
      "anthropic": {
        "api_key": "sk-ant-your_anthropic_key_here",
        "model": "claude-sonnet-4-20250514",
        "temperature": 0.1
      },
      "gemini": {
        "api_key": "your_google_key_here",
        "model": "gemini-2.5-pro",
        "temperature": 0.1
      },
      "deepseek": {
        "api_key": "your_deepseek_key_here",
        "model": "deepseek-reasoner",
        "temperature": 0.2
      }
    }
  }
}
```

### TOML Configuration Format

```toml
[llm]
default_provider = "openai"
default_model = "gpt-4.1"
timeout = 30

[llm.providers.openai]
api_key = "sk-your_openai_key_here"
model = "gpt-4.1"
temperature = 0.7
max_tokens = 4096

[llm.providers.anthropic]
api_key = "sk-ant-your_anthropic_key_here"
model = "claude-sonnet-4-20250514"
temperature = 0.1

[llm.providers.gemini]
api_key = "your_google_key_here"
model = "gemini-2.5-pro"
temperature = 0.1

[llm.providers.deepseek]
api_key = "your_deepseek_key_here"
model = "deepseek-reasoner"
temperature = 0.2
```

### Environment Variables

```bash
# Provider API Keys
OPENAI_API_KEY=sk-your_openai_key_here
ANTHROPIC_API_KEY=sk-ant-your_anthropic_key_here
GEMINI_API_KEY=your_gemini_key_here
DEEPSEEK_API_KEY=your_deepseek_key_here
OPENROUTER_API_KEY=sk-or-your_openrouter_key_here

# Global Settings
DEFAULT_LLM_PROVIDER=gemini
DEFAULT_MODEL=gemini-2.5-pro
DEFAULT_TEMPERATURE=0.3
LLM_TIMEOUT=30
LLM_RETRY_ATTEMPTS=3

# Provider-specific overrides
OPENAI_MODEL=gpt-4.1
ANTHROPIC_MODEL=claude-sonnet-4-20250514
GEMINI_MODEL=gemini-2.5-pro
GEMINI_MAX_TOKENS=20000
```

## Configuration Priority

Configuration sources are loaded in the following priority order (highest to lowest):

1. **Runtime configuration** (set via API calls)
2. **Environment variables** (current process)
3. **Configuration file** (JSON/TOML)
4. **Default values** (built-in defaults)

## Validation and Error Handling

### `validate_config(config: Dict[str, Any]) -> List[str]`

Validate a complete configuration and return errors.

**Parameters:**
- `config` (`Dict[str, Any]`): Configuration to validate

**Returns:**
- `List[str]`: List of validation errors (empty if valid)

**Example:**
```python
errors = config_manager.validate_config(config)
if errors:
    for error in errors:
        print(f"Configuration error: {error}")
else:
    print("Configuration is valid")
```

### Error Types

```python
from spoon_ai.llm.errors import ConfigurationError

try:
    config = config_manager.load_from_file("invalid.json")
except ConfigurationError as e:
    print(f"Configuration error: {e}")
```

## Advanced Features

### Dynamic Configuration Updates

```python
# Update configuration at runtime
config_manager.set_provider_config("openai", {
    "model": "gpt-4.1",  # Switch model
    "temperature": 0.5   # Adjust temperature
})

# Changes take effect immediately for new requests
response = await llm_manager.chat(messages)
```

### Configuration Templates

```python
# Define configuration templates for different environments
templates = {
    "development": {
        "timeout": 60,
        "retry_attempts": 5,
        "providers": {
            "openai": {"model": "gpt-3.5-turbo"}  # Cost-effective for dev
        }
    },
    "production": {
        "timeout": 30,
        "retry_attempts": 3,
        "providers": {
            "openai": {"model": "gpt-4.1"}  # High-quality for prod
        }
    }
}

# Apply template
config_manager.apply_template(templates["production"])
```

### Configuration Encryption

```python
# Encrypt sensitive configuration values
encrypted_config = config_manager.encrypt_config({
    "api_key": "sk-very-secret-key"
})

# Decrypt when needed
decrypted = config_manager.decrypt_config(encrypted_config)
```

## Security Best Practices

### API Key Management

```python
# Never hardcode API keys
# Use environment variables or secure key management

import os
from spoon_ai.llm import ConfigurationManager

config_manager = ConfigurationManager()  # environment-first configuration

# Secure: Load from environment
config_manager.set_provider_config("openai", {
    "api_key": os.getenv("OPENAI_API_KEY"),
    "model": "gpt-4.1"
})

# Avoid: Hardcoded keys (NEVER DO THIS)
# config_manager.set_provider_config("openai", {
#     "api_key": "sk-1234567890abcdef",  # SECURITY RISK!
# })
```

### Configuration Validation

```python
# Always validate configurations before use
config = {
    "providers": {
        "openai": {
            "api_key": os.getenv("OPENAI_API_KEY"),
            "model": "gpt-4.1",
            "temperature": 0.7
        }
    }
}

errors = config_manager.validate_config(config)
if errors:
    raise ValueError(f"Invalid configuration: {errors}")

# Only proceed if configuration is valid
llm_manager = LLMManager(config_manager=config_manager)
```

## Integration Examples

### Programmatic Configuration

```python
from spoon_ai.llm import ConfigurationManager

config_manager = ConfigurationManager()  # defaults to environment variables

# Configure providers programmatically
providers = {
    "openai": {
        "api_key": os.getenv("OPENAI_API_KEY"),
        "model": "gpt-4.1",
        "temperature": 0.7
    },
    "anthropic": {
        "api_key": os.getenv("ANTHROPIC_API_KEY"),
        "model": "claude-sonnet-4-20250514",
        "temperature": 0.1
    }
}

for name, config in providers.items():
    config_manager.set_provider_config(name, config)

# Set global preferences
config_manager.set_global_config({
    "default_provider": "openai",
    "fallback_providers": ["anthropic"],
    "timeout": 30
})
```

## Best Practices

### Configuration Organization
- Use configuration files for complex setups
- Store sensitive data in environment variables
- Validate configurations before deployment

### Environment-Specific Configs
- Use different configurations for dev/staging/production
- Override settings via environment variables
- Use configuration templates for consistency

### Security
- Never commit API keys to version control
- Use secure key management systems
- Rotate API keys regularly
- Validate all configuration inputs

### Performance
- Cache configuration when possible
- Avoid loading configuration on every request
- Use efficient configuration formats (JSON preferred)

## See Also

- [LLMManager](llm-manager.md) - LLM orchestration layer
- [Provider Interface](provider-interface.md) - Provider abstraction
- [LLM Providers](../../core-concepts/llm-providers.md) - Supported providers overview

---

FILE: docs/api-reference/llm/index.md

# LLM System API Reference

The LLM (Large Language Model) system in SpoonOS provides a unified, provider-agnostic interface for working with multiple AI services including OpenAI, Anthropic, Google, and DeepSeek.

## Overview

SpoonOS's LLM system offers:

- > **Note (Nov 2025):** The core Python SDK reads provider settings from environment variables. The `spoon-cli` toolchain loads `config.json` and exports those values into the environment automatically. When using the SDK directly, set the relevant `*_API_KEY`, `*_BASE_URL`, and related environment variables before creating `ConfigurationManager()`.

- **Provider Agnosticism**: Unified API across all providers
- **Automatic Fallback**: Intelligent provider switching on failures
- **Load Balancing**: Distribute requests across multiple providers
- **Comprehensive Monitoring**: Usage tracking and performance metrics
- **Flexible Configuration**: Multiple configuration sources and validation
- **Advanced Features**: Streaming, function calling, and tool integration

## Core Components

### [LLMManager](llm-manager.md)
Central orchestrator for LLM operations with provider management, fallback, and load balancing.

**Key Features:**
- Unified chat and generation API
- Automatic provider fallback
- Load balancing and health monitoring
- Streaming and batch operations
- Comprehensive error handling

```python
from spoon_ai.llm import LLMManager

llm_manager = LLMManager()
response = await llm_manager.chat(messages)
```

### [Provider Interface](provider-interface.md)
Abstract interface that all LLM providers implement for consistent behavior.

**Key Features:**
- Standardized provider contract
- Comprehensive capability system
- Unified response format
- Built-in error handling patterns

```python
from spoon_ai.llm import LLMProviderInterface

class CustomProvider(LLMProviderInterface):
    async def chat(self, messages, **kwargs) -> LLMResponse:
        # Implementation
```

### [Configuration Manager](config-manager.md)
Handles configuration loading, validation, and management from multiple sources.

**Key Features:**
- Multi-source configuration (files, env vars, runtime)
- Provider-specific validation
- Secure credential management
- Configuration templates and merging

```python
import os
from spoon_ai.llm import ConfigurationManager

# Export provider settings into environment variables
os.environ["OPENAI_API_KEY"] = "sk-..."
os.environ["DEFAULT_LLM_PROVIDER"] = "openai"

config_manager = ConfigurationManager()
```

## Quick Start

### Basic Usage

```python
from spoon_ai.llm import LLMManager
from spoon_ai.schema import Message

# Initialize manager
llm_manager = LLMManager()

# Simple chat
messages = [Message(role="user", content="Hello!")]
response = await llm_manager.chat(messages)
print(response.content)
```


### Controlling Provider Priority

You can steer which provider is used firstâ€”and how the system falls backâ€”purely via environment variables:

```bash
# Prefer Anthropic by default
export DEFAULT_LLM_PROVIDER=anthropic

# Allow fallback to OpenAI, then Gemini
export LLM_FALLBACK_CHAIN="anthropic,openai,gemini"
```

On Windows PowerShell:

```powershell
$env:DEFAULT_LLM_PROVIDER = "anthropic"
$env:LLM_FALLBACK_CHAIN = "anthropic,openai,gemini"
```

After setting the variables, simply instantiate `ConfigurationManager()` as usual; no code changes are needed. The `spoon-cli` configuration workflow writes these variables for you whenever it loads `config.json`.

### Streaming Responses

```python
# Stream responses for real-time output
async for chunk in llm_manager.chat_stream(messages):
    print(chunk, end="", flush=True)
```

## Supported Providers

### OpenAI
- **Models**: GPT-4.1, GPT-4o, GPT-4o-mini, o1-preview, o1-mini
- **Features**: Function calling, streaming, embeddings
- **Best for**: General-purpose tasks, reasoning, code generation

### Anthropic (Claude)
- **Models**: Claude-Sonnet-4-20250514, Claude-3.5 Sonnet, Claude-3.5 Haiku
- **Features**: Large context windows, prompt caching, safety
- **Best for**: Long documents, analysis, safety-critical applications

### Google (Gemini)
- **Models**: Gemini-2.5-Pro, Gemini-2.0-Flash, Gemini-1.5-Pro
- **Features**: Multimodal, fast inference, large context
- **Best for**: Multimodal tasks, cost-effectiveness, long context

### DeepSeek
- **Models**: DeepSeek-Reasoner, DeepSeek-V3, DeepSeek-Chat
- **Features**: Advanced reasoning, code-specialized, cost-effective
- **Best for**: Complex reasoning, code generation, technical tasks

### OpenRouter
- **Models**: Access to multiple providers through single API
- **Features**: Model routing, cost optimization
- **Best for**: Experimentation, cost optimization

## Advanced Patterns

### Provider Fallback

```python
# Configure automatic fallback
llm_manager = LLMManager()
llm_manager.set_primary_provider("openai")
llm_manager.add_fallback_provider("anthropic")
llm_manager.add_fallback_provider("deepseek")

# Automatic fallback on failures
response = await llm_manager.chat(messages)
print(f"Used provider: {response.provider}")
```

### Load Balancing

```python
# Weighted load balancing
llm_manager.set_load_balancing_strategy("weighted")
llm_manager.set_provider_weight("openai", 0.6)
llm_manager.set_provider_weight("anthropic", 0.4)

# Requests distributed by weights
response = await llm_manager.chat(messages)
```

### Tool Integration

```python
# Function calling with tools
tools = [
    {
        "name": "get_weather",
        "description": "Get current weather",
        "parameters": {
            "type": "object",
            "properties": {"location": {"type": "string"}}
        }
    }
]

response = await llm_manager.chat_with_tools(messages, tools)
for tool_call in response.tool_calls:
    # Execute tool calls
    result = await execute_tool(tool_call)
```

### Batch Operations

```python
# Process multiple requests efficiently
batch_messages = [
    [Message(role="user", content="Summarize text A")],
    [Message(role="user", content="Translate text B")],
    [Message(role="user", content="Analyze text C")]
]

responses = await llm_manager.batch_chat(batch_messages)
for i, response in enumerate(responses):
    print(f"Response {i+1}: {response.content[:50]}...")
```

## Configuration

### Environment Variables

```bash
# API Keys (required - set at least one)
OPENAI_API_KEY=sk-your_key
ANTHROPIC_API_KEY=sk-ant-your_key
GEMINI_API_KEY=your_gemini_key
DEEPSEEK_API_KEY=your_key

# Global Settings (optional)
DEFAULT_LLM_PROVIDER=openai          # or anthropic / gemini / deepseek / openrouter
DEFAULT_MODEL=gpt-5.1
DEFAULT_TEMPERATURE=0.3
LLM_TIMEOUT=30
LLM_RETRY_ATTEMPTS=3

# Provider-specific overrides (optional)
GEMINI_MAX_TOKENS=20000
```

### Runtime Configuration

```python
from spoon_ai.llm import ConfigurationManager

config_manager = ConfigurationManager()  # uses environment variables by default

# Configure providers
config_manager.set_provider_config("openai", {
    "api_key": "sk-...",
    "model": "gpt-4.1",
    "temperature": 0.7
})

# Set global settings
config_manager.set_global_config({
    "default_provider": "openai",
    "timeout": 30
})
```

## Response Format

All LLM operations return a standardized `LLMResponse`:

```python
@dataclass
class LLMResponse:
    content: str                    # Generated text
    provider: str                   # Provider used
    model: str                      # Model used
    finish_reason: str              # Why generation stopped
    native_finish_reason: str       # Provider-specific reason
    tool_calls: List[ToolCall]      # Function calls (if any)
    usage: Dict[str, int]          # Token usage statistics
    metadata: Dict[str, Any]       # Additional metadata
    request_id: str                # Unique request ID
    duration: float                # Request duration
    timestamp: datetime            # Request time
```

## Error Handling

### Structured Error Types

```python
from spoon_ai.llm.errors import (
    LLMError,              # Base LLM error
    ProviderError,         # Provider-specific errors
    ConfigurationError,    # Configuration issues
    RateLimitError,        # Rate limiting
    AuthenticationError,   # Auth failures
    ModelNotFoundError,    # Invalid model
    TokenLimitError,       # Token limit exceeded
    NetworkError,          # Network issues
    ProviderUnavailableError  # Provider down
)

try:
    response = await llm_manager.chat(messages)
except RateLimitError:
    # Handle rate limiting
    await asyncio.sleep(60)
    response = await llm_manager.chat(messages)
except AuthenticationError:
    # Handle auth issues
    print("API key invalid")
except ProviderError as e:
    print(f"Provider {e.provider} failed: {e.message}")
```

### Automatic Recovery

```python
# Framework handles most errors automatically
llm_manager = LLMManager()

# Automatic retry with backoff
# Automatic fallback to other providers
# Automatic rate limit handling

response = await llm_manager.chat(messages)  # Robust by default
```

## Monitoring and Metrics

### Usage Tracking

```python
# Get comprehensive metrics
metrics = llm_manager.get_metrics()
print(f"Total requests: {metrics['total_requests']}")
print(f"Success rate: {metrics['success_rate']}%")
print(f"Total tokens: {metrics['total_tokens']}")
print(f"Total cost: ${metrics['total_cost']}")
```

### Provider Statistics

```python
# Per-provider metrics
stats = llm_manager.get_provider_stats()
for provider, data in stats.items():
    print(f"{provider}: {data['requests']} requests, {data['errors']} errors")
```

### Health Monitoring

```python
# Check provider health
health = await llm_manager.health_check()
print(f"Healthy providers: {health['healthy_providers']}")

# Individual provider health
health = await llm_manager.health_check("openai")
print(f"OpenAI healthy: {health['healthy']}")
```

## Custom Provider Implementation

### Basic Custom Provider

```python
from spoon_ai.llm import LLMProviderInterface, LLMResponse

class MyCustomProvider(LLMProviderInterface):
    async def initialize(self, config: Dict[str, Any]) -> None:
        self.api_key = config["api_key"]

    async def chat(self, messages, **kwargs) -> LLMResponse:
        # Your implementation
        response = await self._call_api(messages, **kwargs)
        return LLMResponse(
            content=response["content"],
            provider="custom",
            model="my-model",
            finish_reason="stop",
            native_finish_reason="stop",
            tool_calls=[],
            usage=response.get("usage"),
            metadata={},
            request_id="custom-123",
            duration=response.get("duration", 0.0)
        )

    def get_metadata(self) -> ProviderMetadata:
        return ProviderMetadata(
            name="custom",
            version="1.0",
            capabilities=[ProviderCapability.CHAT],
            max_tokens=4096,
            supports_system_messages=True
        )

    async def health_check(self) -> bool:
        try:
            # Test API connectivity
            return True
        except:
            return False

    async def cleanup(self) -> None:
        pass

    # Implement other required methods...
```

### Registering Custom Providers

```python
from spoon_ai.llm import register_provider

# Register your custom provider
register_provider("my_provider", MyCustomProvider)

# Now you can use it
llm_manager.set_primary_provider("my_provider")
```

## Best Practices

### Configuration Management
- Store API keys securely in environment variables
- Use configuration files for complex setups
- Validate configurations before deployment
- Use different configs for dev/staging/production

### Error Handling
- Let the framework handle common errors automatically
- Use specific error types for custom logic
- Implement proper fallback chains
- Monitor error rates and patterns

### Performance Optimization
- Use streaming for real-time applications
- Batch requests when possible
- Monitor token usage and costs
- Cache responses when appropriate

### Provider Selection
- Test multiple providers for your use case
- Consider cost vs. quality trade-offs
- Use fallbacks for production reliability
- Monitor provider performance regularly

## Migration Guide

### From Direct Provider APIs

```python
# Before: Direct OpenAI API
import openai
client = openai.OpenAI(api_key="sk-...")
response = client.chat.completions.create(...)

# After: SpoonOS LLM Manager
from spoon_ai.llm import LLMManager
llm_manager = LLMManager()
response = await llm_manager.chat(messages)  # Automatic provider selection
```

### From Other LLM Libraries

```python
# Before: LangChain
from langchain.llms import OpenAI
llm = OpenAI(model="gpt-4", temperature=0.7)

# After: SpoonOS
from spoon_ai.llm import LLMManager
llm_manager = LLMManager()
llm_manager.configure_provider("openai", {
    "model": "gpt-4.1",
    "temperature": 0.7
})
```

## Troubleshooting

### Common Issues

**Provider Connection Failed**
```python
# Check API keys
health = await llm_manager.health_check("openai")
if not health["healthy"]:
    print(f"Error: {health.get('error')}")

# Verify configuration
config = llm_manager.get_provider_config("openai")
print(f"API Key configured: {bool(config.api_key)}")
```

**Rate Limiting**
```python
# Increase timeout and retry settings
llm_manager.set_retry_policy(max_attempts=5, backoff_factor=2.0)
llm_manager.set_timeout(60)

# Use multiple providers to distribute load
llm_manager.add_fallback_provider("anthropic")
```

**High Latency**
```python
# Enable monitoring to identify bottlenecks
llm_manager.enable_monitoring(["execution_time", "success_rate"])

# Check metrics
metrics = llm_manager.get_metrics()
print(f"Average latency: {metrics['avg_latency']}s")

# Consider faster providers or models
llm_manager.set_primary_provider("gemini")  # Generally faster
```

**Configuration Errors**
```python
from spoon_ai.llm import ConfigurationManager

config_manager = ConfigurationManager()  # refreshes from environment variables
errors = config_manager.validate_config(your_config)
for error in errors:
    print(f"Config error: {error}")
```

## See Also

- [LLM Providers](../../core-concepts/llm-providers.md) - Supported providers and features
- [Configuration Guide](../../getting-started/configuration.md) - Setup and configuration
- [Error Handling](../../troubleshooting/common-issues.md) - Troubleshooting guide

---

FILE: docs/api-reference/llm/llm-manager.md

# LLMManager API Reference

The `LLMManager` class is the central orchestrator for LLM providers, providing unified access with automatic fallback, load balancing, and comprehensive error handling.

## Class Definition

```python
from spoon_ai.llm import LLMManager, LLMResponse
from spoon_ai.schema import Message
from typing import Optional, List, Dict, Any

class LLMManager:
    def __init__(
        self,
        config_manager: Optional[ConfigurationManager] = None,
        debug_logger: Optional[DebugLogger] = None,
        metrics_collector: Optional[MetricsCollector] = None,
        response_normalizer: Optional[ResponseNormalizer] = None,
        registry: Optional[LLMProviderRegistry] = None
    )
```

## Constructor Parameters

### Required Parameters

None - all parameters are optional with sensible defaults

### Optional Parameters

- **config_manager** (`Optional[ConfigurationManager]`): Configuration manager instance
- **debug_logger** (`Optional[DebugLogger]`): Debug logging instance
- **metrics_collector** (`Optional[MetricsCollector]`): Metrics collection instance
- **response_normalizer** (`Optional[ResponseNormalizer]`): Response normalization instance
- **registry** (`Optional[LLMProviderRegistry]`): Provider registry instance

## Core Methods

### Chat and Generation

#### `async chat(messages: List[Message], provider: Optional[str] = None, **kwargs) -> LLMResponse`

Send chat request with automatic provider selection and fallback.

**Parameters:**
- `messages` (`List[Message]`): List of conversation messages
- `provider` (`Optional[str]`): Specific provider to use (optional)
- `**kwargs`: Additional provider-specific parameters

**Returns:**
- `LLMResponse`: Standardized response with metadata

**Example:**
```python
from spoon_ai.schema import Message

messages = [
    Message(role="user", content="Hello, how are you?")
]

response = await llm_manager.chat(messages)
print(response.content)  # "I'm doing well, thank you!"
```

#### `async chat_stream(messages: List[Message], provider: Optional[str] = None, **kwargs) -> AsyncGenerator[str, None]`

Send streaming chat request for real-time responses.

**Parameters:**
- `messages` (`List[Message]`): List of conversation messages
- `provider` (`Optional[str]`): Specific provider to use (optional)
- `**kwargs`: Additional provider-specific parameters

**Yields:**
- `str`: Streaming response chunks

**Example:**
```python
async for chunk in llm_manager.chat_stream(messages):
    print(chunk, end="", flush=True)  # Real-time output
```

#### `async chat_with_tools(messages: List[Message], tools: List[Dict], provider: Optional[str] = None, **kwargs) -> LLMResponse`

Send chat request with tool/function calling support.

**Parameters:**
- `messages` (`List[Message]`): List of conversation messages
- `tools` (`List[Dict]`): Available tools/functions for the model
- `provider` (`Optional[str]`): Specific provider to use (optional)
- `**kwargs`: Additional provider-specific parameters

**Returns:**
- `LLMResponse`: Response with potential tool calls

**Example:**
```python
tools = [
    {
        "name": "get_weather",
        "description": "Get current weather",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string"}
            }
        }
    }
]

response = await llm_manager.chat_with_tools(messages, tools)
if response.tool_calls:
    # Handle tool calls
    for tool_call in response.tool_calls:
        print(f"Tool: {tool_call.name}, Args: {tool_call.arguments}")
```

#### `async completion(prompt: str, provider: Optional[str] = None, **kwargs) -> LLMResponse`

Send text completion request (legacy support).

**Parameters:**
- `prompt` (`str`): Text prompt for completion
- `provider` (`Optional[str]`): Specific provider to use (optional)
- `**kwargs`: Additional provider-specific parameters

**Returns:**
- `LLMResponse`: Completion response

**Example:**
```python
response = await llm_manager.completion("Once upon a time")
print(response.content)  # "Once upon a time, in a land far away..."
```

### Provider Management

#### `set_primary_provider(provider_name: str) -> None`

Set the primary provider for requests.

**Parameters:**
- `provider_name` (`str`): Name of the provider to use as primary

**Example:**
```python
llm_manager.set_primary_provider("openai")
```

#### `add_fallback_provider(provider_name: str, priority: int = 0) -> None`

Add a fallback provider with priority.

**Parameters:**
- `provider_name` (`str`): Name of the fallback provider
- `priority` (`int`): Priority level (higher = more important)

**Example:**
```python
llm_manager.add_fallback_provider("anthropic", priority=10)
llm_manager.add_fallback_provider("deepseek", priority=5)
```

#### `remove_provider(provider_name: str) -> None`

Remove a provider from the manager.

**Parameters:**
- `provider_name` (`str`): Name of the provider to remove

**Example:**
```python
llm_manager.remove_provider("openrouter")
```

### Configuration

#### `configure_provider(provider_name: str, config: Dict[str, Any]) -> None`

Configure a specific provider.

**Parameters:**
- `provider_name` (`str`): Name of the provider to configure
- `config` (`Dict[str, Any]`): Provider-specific configuration

**Example:**
```python
llm_manager.configure_provider("openai", {
    "model": "gpt-4.1",
    "temperature": 0.7,
    "max_tokens": 4096
})
```

#### `get_provider_config(provider_name: str) -> Optional[Dict[str, Any]]`

Get configuration for a specific provider.

**Parameters:**
- `provider_name` (`str`): Name of the provider

**Returns:**
- `Optional[Dict[str, Any]]`: Provider configuration or None if not found

**Example:**
```python
config = llm_manager.get_provider_config("openai")
print(f"Model: {config.get('model')}")
```

### Health and Monitoring

#### `async health_check(provider_name: Optional[str] = None) -> Dict[str, Any]`

Check health status of providers.

**Parameters:**
- `provider_name` (`Optional[str]`): Specific provider to check, or None for all

**Returns:**
- `Dict[str, Any]`: Health status information

**Example:**
```python
# Check all providers
health = await llm_manager.health_check()
print(f"Healthy providers: {health['healthy_providers']}")

# Check specific provider
health = await llm_manager.health_check("openai")
print(f"OpenAI healthy: {health['healthy']}")
```

#### `get_metrics() -> Dict[str, Any]`

Get usage metrics and statistics.

**Returns:**
- `Dict[str, Any]`: Comprehensive metrics data

**Example:**
```python
metrics = llm_manager.get_metrics()
print(f"Total requests: {metrics['total_requests']}")
print(f"Success rate: {metrics['success_rate']}%")
print(f"Total tokens: {metrics['total_tokens']}")
```

#### `get_provider_stats() -> Dict[str, Any]`

Get per-provider statistics.

**Returns:**
- `Dict[str, Any]`: Provider-specific statistics

**Example:**
```python
stats = llm_manager.get_provider_stats()
for provider, data in stats.items():
    print(f"{provider}: {data['requests']} requests, {data['errors']} errors")
```

### Load Balancing

#### `set_load_balancing_strategy(strategy: str) -> None`

Set load balancing strategy between providers.

**Parameters:**
- `strategy` (`str`): Strategy name ("round_robin", "weighted", "random")

**Example:**
```python
llm_manager.set_load_balancing_strategy("weighted")
```

#### `set_provider_weight(provider_name: str, weight: float) -> None`

Set weight for weighted load balancing.

**Parameters:**
- `provider_name` (`str`): Name of the provider
- `weight` (`float`): Weight value (higher = more requests)

**Example:**
```python
llm_manager.set_provider_weight("openai", 0.7)
llm_manager.set_provider_weight("anthropic", 0.3)
```

### Error Handling

#### `set_retry_policy(max_attempts: int, backoff_factor: float) -> None`

Configure retry policy for failed requests.

**Parameters:**
- `max_attempts` (`int`): Maximum retry attempts
- `backoff_factor` (`float`): Backoff multiplier for delays

**Example:**
```python
llm_manager.set_retry_policy(max_attempts=3, backoff_factor=2.0)
```

#### `set_timeout(timeout: float) -> None`

Set global timeout for requests.

**Parameters:**
- `timeout` (`float`): Timeout in seconds

**Example:**
```python
llm_manager.set_timeout(30.0)  # 30 second timeout
```

### Advanced Features

#### `async batch_chat(messages_list: List[List[Message]], provider: Optional[str] = None, **kwargs) -> List[LLMResponse]`

Send multiple chat requests in batch for efficiency.

**Parameters:**
- `messages_list` (`List[List[Message]]`): List of message lists
- `provider` (`Optional[str]`): Provider to use (optional)
- `**kwargs`: Additional parameters

**Returns:**
- `List[LLMResponse]`: List of responses

**Example:**
```python
batch_messages = [
    [Message(role="user", content="Summarize this text...")],
    [Message(role="user", content="Translate this...")],
    [Message(role="user", content="Analyze this...")]
]

responses = await llm_manager.batch_chat(batch_messages)
for i, response in enumerate(responses):
    print(f"Response {i+1}: {response.content[:100]}...")
```

#### `create_conversation_context() -> Dict[str, Any]`

Create a conversation context for maintaining state.

**Returns:**
- `Dict[str, Any]`: Context object for conversation management

**Example:**
```python
context = llm_manager.create_conversation_context()

# Use context in subsequent requests
response1 = await llm_manager.chat(messages1, context=context)
response2 = await llm_manager.chat(messages2, context=context)
```

## LLMResponse Structure

All methods return a standardized `LLMResponse` object:

```python
@dataclass
class LLMResponse:
    content: str                    # Response text
    provider: str                   # Provider used
    model: str                      # Model used
    finish_reason: str              # Why generation stopped
    native_finish_reason: str       # Provider-specific reason
    tool_calls: List[ToolCall]      # Function calls (if any)
    usage: Optional[Dict[str, int]] # Token usage statistics
    metadata: Dict[str, Any]        # Additional metadata
    request_id: str                 # Unique request identifier
    duration: float                 # Request duration in seconds
    timestamp: datetime             # When request was made
```

## Error Types

The LLMManager can raise several error types:

- `ProviderError`: Provider-specific failures
- `ConfigurationError`: Configuration issues
- `RateLimitError`: Rate limiting errors
- `AuthenticationError`: Authentication failures
- `ModelNotFoundError`: Invalid model specified
- `TokenLimitError`: Token limit exceeded
- `NetworkError`: Network connectivity issues

## Best Practices

### Provider Selection
- Use primary provider for critical functionality
- Configure fallbacks for reliability
- Monitor provider performance and costs

### Configuration
- Store API keys securely in environment variables
- Use configuration files for complex setups
- Validate configurations before deployment

### Error Handling
- Always handle provider failures gracefully
- Implement exponential backoff for retries
- Log errors for debugging and monitoring

### Performance
- Use streaming for real-time applications
- Batch requests when possible
- Monitor and optimize token usage

## Example: Complete Setup

```python
from spoon_ai.llm import LLMManager
from spoon_ai.schema import Message

# Initialize manager
llm_manager = LLMManager()

# Configure providers
llm_manager.configure_provider("openai", {
    "model": "gpt-4.1",
    "temperature": 0.7
})

llm_manager.configure_provider("anthropic", {
    "model": "claude-sonnet-4-20250514",
    "temperature": 0.1
})

# Set primary and fallbacks
llm_manager.set_primary_provider("openai")
llm_manager.add_fallback_provider("anthropic")

# Use with automatic fallback
messages = [Message(role="user", content="Hello!")]
response = await llm_manager.chat(messages)

print(f"Response from {response.provider}: {response.content}")
```

## See Also

- [LLM Provider Interface](provider-interface.md) - Provider abstraction layer
- [Configuration Manager](config-manager.md) - Configuration management
- [LLM Providers](../../core-concepts/llm-providers.md) - Supported providers overview

---

FILE: docs/api-reference/llm/provider-interface.md

# LLM Provider Interface API Reference

The `LLMProviderInterface` defines the standardized contract that all LLM providers must implement, ensuring consistent behavior across different AI services.

## Interface Definition

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any, AsyncGenerator
from spoon_ai.llm import LLMProviderInterface, ProviderCapability, ProviderMetadata, LLMResponse
from spoon_ai.schema import Message

class LLMProviderInterface(ABC):
    """Abstract base class defining the unified interface for all LLM providers."""
```

## Core Abstract Methods

### Initialization and Lifecycle

#### `async initialize(config: Dict[str, Any]) -> None`

Initialize the provider with configuration.

**Parameters:**
- `config` (`Dict[str, Any]`): Provider-specific configuration dictionary

**Raises:**
- `ConfigurationError`: If configuration is invalid

**Example:**
```python
await provider.initialize({
    "api_key": "sk-...",
    "model": "gpt-4",
    "temperature": 0.7
})
```

#### `async cleanup() -> None`

Clean up resources and connections when the provider is no longer needed.

**Example:**
```python
await provider.cleanup()  # Close connections, cleanup resources
```

### Chat and Generation

#### `async chat(messages: List[Message], **kwargs) -> LLMResponse`

Send chat request to the provider.

**Parameters:**
- `messages` (`List[Message]`): List of conversation messages
- `**kwargs`: Additional provider-specific parameters

**Returns:**
- `LLMResponse`: Standardized response object

**Raises:**
- `ProviderError`: If the request fails

**Example:**
```python
from spoon_ai.schema import Message

messages = [
    Message(role="system", content="You are a helpful assistant."),
    Message(role="user", content="Hello!")
]

response = await provider.chat(messages)
print(response.content)
```

#### `async chat_stream(messages: List[Message], **kwargs) -> AsyncGenerator[str, None]`

Send streaming chat request to the provider.

**Parameters:**
- `messages` (`List[Message]`): List of conversation messages
- `**kwargs`: Additional provider-specific parameters

**Yields:**
- `str`: Streaming response chunks

**Raises:**
- `ProviderError`: If the request fails

**Example:**
```python
async for chunk in provider.chat_stream(messages):
    print(chunk, end="", flush=True)
```

#### `async completion(prompt: str, **kwargs) -> LLMResponse`

Send completion request to the provider (legacy support).

**Parameters:**
- `prompt` (`str`): Text prompt for completion
- `**kwargs`: Additional provider-specific parameters

**Returns:**
- `LLMResponse`: Completion response

**Raises:**
- `ProviderError`: If the request fails

**Example:**
```python
response = await provider.completion("Once upon a time")
print(response.content)
```

#### `async chat_with_tools(messages: List[Message], tools: List[Dict], **kwargs) -> LLMResponse`

Send chat request with tool/function calling support.

**Parameters:**
- `messages` (`List[Message]`): List of conversation messages
- `tools` (`List[Dict]`): Available tools/functions for the model
- `**kwargs`: Additional provider-specific parameters

**Returns:**
- `LLMResponse`: Response with potential tool calls

**Raises:**
- `ProviderError`: If the request fails

**Example:**
```python
tools = [
    {
        "name": "get_weather",
        "description": "Get current weather",
        "parameters": {
            "type": "object",
            "properties": {"location": {"type": "string"}}
        }
    }
]

response = await provider.chat_with_tools(messages, tools)
for tool_call in response.tool_calls:
    print(f"Tool call: {tool_call.name}")
```

### Metadata and Health

#### `get_metadata() -> ProviderMetadata`

Get provider metadata and capabilities.

**Returns:**
- `ProviderMetadata`: Provider information and capabilities

**Example:**
```python
metadata = provider.get_metadata()
print(f"Provider: {metadata.name}")
print(f"Max tokens: {metadata.max_tokens}")
print(f"Capabilities: {[cap.value for cap in metadata.capabilities]}")
```

#### `async health_check() -> bool`

Check if provider is healthy and available.

**Returns:**
- `bool`: True if provider is healthy, False otherwise

**Example:**
```python
is_healthy = await provider.health_check()
if not is_healthy:
    print("Provider is currently unavailable")
```

## Data Structures

### ProviderCapability Enum

```python
from enum import Enum

class ProviderCapability(Enum):
    CHAT = "chat"                    # Basic chat functionality
    COMPLETION = "completion"        # Text completion
    TOOLS = "tools"                  # Function/tool calling
    STREAMING = "streaming"          # Streaming responses
    IMAGE_GENERATION = "image_generation"  # Image generation
    VISION = "vision"                # Vision/image understanding
```

### ProviderMetadata Class

```python
from dataclasses import dataclass
from typing import List, Dict

@dataclass
class ProviderMetadata:
    name: str                                    # Provider name
    version: str                                 # Provider version
    capabilities: List[ProviderCapability]       # Supported capabilities
    max_tokens: int                              # Maximum context tokens
    supports_system_messages: bool               # System message support
    rate_limits: Dict[str, int] = field(default_factory=dict)  # Rate limits
```

### LLMResponse Class

```python
from dataclasses import dataclass
from datetime import datetime
from typing import List, Dict, Any, Optional

@dataclass
class LLMResponse:
    content: str                           # Response text content
    provider: str                          # Provider name used
    model: str                             # Model name used
    finish_reason: str                     # Standardized finish reason
    native_finish_reason: str              # Provider-specific finish reason
    tool_calls: List[ToolCall]             # Function/tool calls
    usage: Optional[Dict[str, int]]        # Token usage statistics
    metadata: Dict[str, Any]               # Additional provider metadata
    request_id: str                        # Unique request identifier
    duration: float                        # Request duration in seconds
    timestamp: datetime                    # Request timestamp
```

## Implementing a Custom Provider

### Basic Provider Implementation

```python
from spoon_ai.llm import LLMProviderInterface, ProviderCapability, ProviderMetadata, LLMResponse
from spoon_ai.schema import Message

class CustomProvider(LLMProviderInterface):
    def __init__(self):
        self.api_key = None
        self.model = "custom-model"
        self.initialized = False

    async def initialize(self, config: Dict[str, Any]) -> None:
        self.api_key = config.get("api_key")
        if not self.api_key:
            raise ConfigurationError("API key is required")

        # Initialize connection, validate credentials, etc.
        self.initialized = True

    async def chat(self, messages: List[Message], **kwargs) -> LLMResponse:
        if not self.initialized:
            raise ProviderError("Provider not initialized")

        # Convert messages to provider format
        prompt = self._messages_to_prompt(messages)

        # Make API call
        response_data = await self._call_api(prompt, **kwargs)

        # Convert to standardized response
        return LLMResponse(
            content=response_data["content"],
            provider="custom",
            model=self.model,
            finish_reason="stop",
            native_finish_reason=response_data.get("finish_reason", "stop"),
            tool_calls=[],
            usage=response_data.get("usage"),
            metadata=response_data.get("metadata", {}),
            request_id=response_data.get("request_id", ""),
            duration=response_data.get("duration", 0.0)
        )

    async def chat_stream(self, messages: List[Message], **kwargs) -> AsyncGenerator[str, None]:
        if not self.initialized:
            raise ProviderError("Provider not initialized")

        # Implementation for streaming
        async for chunk in self._stream_api(messages, **kwargs):
            yield chunk

    async def completion(self, prompt: str, **kwargs) -> LLMResponse:
        # Convert to chat format for simplicity
        messages = [Message(role="user", content=prompt)]
        return await self.chat(messages, **kwargs)

    async def chat_with_tools(self, messages: List[Message], tools: List[Dict], **kwargs) -> LLMResponse:
        # If provider supports tools, implement tool calling
        # Otherwise, fall back to regular chat
        if ProviderCapability.TOOLS in self.get_metadata().capabilities:
            return await self._chat_with_tools_impl(messages, tools, **kwargs)
        else:
            return await self.chat(messages, **kwargs)

    def get_metadata(self) -> ProviderMetadata:
        return ProviderMetadata(
            name="custom",
            version="1.0.0",
            capabilities=[
                ProviderCapability.CHAT,
                ProviderCapability.COMPLETION,
                ProviderCapability.STREAMING
            ],
            max_tokens=4096,
            supports_system_messages=True,
            rate_limits={
                "requests_per_minute": 60,
                "tokens_per_minute": 10000
            }
        )

    async def health_check(self) -> bool:
        try:
            # Quick API call to test connectivity
            test_response = await self._call_api("test", max_tokens=1)
            return True
        except Exception:
            return False

    async def cleanup(self) -> None:
        # Close connections, cleanup resources
        self.initialized = False

    # Helper methods
    def _messages_to_prompt(self, messages: List[Message]) -> str:
        # Convert messages to provider-specific format
        return "\n".join([f"{msg.role}: {msg.content}" for msg in messages])

    async def _call_api(self, prompt: str, **kwargs) -> Dict[str, Any]:
        # Implement actual API call
        # This is a mock implementation
        return {
            "content": "Mock response",
            "finish_reason": "stop",
            "usage": {"prompt_tokens": 10, "completion_tokens": 20},
            "metadata": {},
            "request_id": "mock-123",
            "duration": 0.5
        }
```

### Provider Capabilities Implementation

#### Tools Support

```python
async def _chat_with_tools_impl(self, messages: List[Message], tools: List[Dict], **kwargs) -> LLMResponse:
    # Convert tools to provider format
    provider_tools = self._convert_tools_format(tools)

    # Make API call with tools
    response_data = await self._call_api_with_tools(messages, provider_tools, **kwargs)

    # Parse tool calls from response
    tool_calls = self._parse_tool_calls(response_data)

    return LLMResponse(
        content=response_data.get("content", ""),
        provider="custom",
        model=self.model,
        finish_reason="tool_calls" if tool_calls else "stop",
        native_finish_reason=response_data.get("finish_reason", "stop"),
        tool_calls=tool_calls,
        usage=response_data.get("usage"),
        metadata=response_data.get("metadata", {}),
        request_id=response_data.get("request_id", ""),
        duration=response_data.get("duration", 0.0)
    )
```

#### Streaming Support

```python
async def _stream_api(self, messages: List[Message], **kwargs) -> AsyncGenerator[str, None]:
    # Implement streaming API call
    async for chunk in self._streaming_call(messages, **kwargs):
        # Parse and yield content chunks
        if "content" in chunk:
            yield chunk["content"]
        elif chunk.get("finish_reason"):
            break  # End of stream
```

### Error Handling

```python
from spoon_ai.llm.errors import ProviderError, RateLimitError, AuthenticationError

async def chat(self, messages: List[Message], **kwargs) -> LLMResponse:
    try:
        return await self._call_api(messages, **kwargs)
    except HTTPError as e:
        if e.status == 401:
            raise AuthenticationError("Invalid API key", provider=self.name)
        elif e.status == 429:
            raise RateLimitError("Rate limit exceeded", provider=self.name)
        else:
            raise ProviderError(f"HTTP {e.status}: {e.message}", provider=self.name)
    except Exception as e:
        raise ProviderError(f"Unexpected error: {str(e)}", provider=self.name)
```

## Built-in Provider Implementations

### OpenAI Provider

```python
from spoon_ai.llm.providers import OpenAIProvider

provider = OpenAIProvider()
await provider.initialize({
    "api_key": "sk-...",
    "model": "gpt-4.1"
})
```

### Anthropic Provider

```python
from spoon_ai.llm.providers import AnthropicProvider

provider = AnthropicProvider()
await provider.initialize({
    "api_key": "sk-ant-...",
    "model": "claude-sonnet-4-20250514"
})
```

### Gemini Provider

```python
from spoon_ai.llm.providers import GeminiProvider

provider = GeminiProvider()
await provider.initialize({
    "api_key": "...",
    "model": "gemini-2.5-pro"
})
```

## Provider Registry

### Registering Custom Providers

```python
from spoon_ai.llm import LLMProviderRegistry, register_provider

# Register a custom provider
registry = LLMProviderRegistry.get_global_registry()
registry.register_provider("my_provider", MyCustomProvider)

# Or use the convenience function
register_provider("my_provider", MyCustomProvider)
```

### Provider Discovery

```python
from spoon_ai.llm import get_global_registry

registry = get_global_registry()

# List all available providers
providers = registry.list_providers()
print(f"Available providers: {providers}")

# Get a provider instance
provider_class = registry.get_provider("openai")
provider = provider_class()
```

## Best Practices

### Provider Implementation
- Always implement proper error handling and conversion
- Use standardized response formats
- Support all interface methods for full compatibility
- Implement health checks for reliability monitoring

### Capability Declaration
- Accurately declare supported capabilities
- Include realistic rate limits and token limits
- Specify supported models and features

### Error Handling
- Convert provider-specific errors to standard types
- Include helpful error messages and context
- Implement proper cleanup in error scenarios

### Performance
- Implement efficient streaming when supported
- Cache metadata and configuration when possible
- Use connection pooling for better performance

## See Also

- [LLMManager](llm-manager.md) - Central orchestration layer
- [Configuration Manager](config-manager.md) - Provider configuration
- [LLM Providers](../../core-concepts/llm-providers.md) - Supported providers overview

---

FILE: docs/api-reference/payments/x402.md

# x402 Payment APIs

This reference covers the building blocks that SpoonOS exposes for integrating x402 payments programmatically.

## Python services

### `X402PaymentService`

Located in `spoon_ai/payments/x402_service.py`. Responsibilities:

- Load consolidated configuration via `X402Settings.load()`.
- Build `PaymentRequirements` objects (`build_payment_requirements(request: X402PaymentRequest | None)`).
- Generate signed headers (`build_payment_header(requirements, max_value=None)`), routing to local `PRIVATE_KEY` or Turnkey as needed.
- Decode facilitator receipts (`decode_payment_response(header_value)`).
- Verify/settle payments against the facilitator (`verify_payment`, `settle_payment`, `verify_and_settle`).

Key data models:

| Model | Purpose |
| --- | --- |
| `X402PaymentRequest` | Declarative amount/resource/metadata input used to shape `PaymentRequirements`. |
| `X402PaymentOutcome` | Merges `verify` and `settle` results for the FastAPI paywall router. |
| `X402PaymentReceipt` | Parsed view of `X-PAYMENT-RESPONSE` headers including transaction hash, payer, and network. |

### Environment fallbacks

| Variable | Description |
| --- | --- |
| `PRIVATE_KEY` | Default local signer (0x-prefixed hex). |
| `X402_AGENT_PRIVATE_KEY` | Optional override for agents only. |
| `TURNKEY_*` | (`TURNKEY_BASE_URL`, `TURNKEY_API_PUBLIC_KEY`, `TURNKEY_API_PRIVATE_KEY`, `TURNKEY_ORG_ID`, `TURNKEY_SIGN_WITH`, `TURNKEY_ADDRESS`). If `PRIVATE_KEY` is empty but `TURNKEY_SIGN_WITH` exists, the service uses Turnkey for signing. |
| `X402_*` | Mirrors the `.env` entries from the core concepts page (facilitator URL, asset, network, amount, branding). |

## Built-in tools

### `x402_create_payment`

Generates a signed `X-PAYMENT` header.

Parameters:

| Name | Type | Notes |
| --- | --- | --- |
| `resource` | string | URL that the payment authorizes. |
| `amount_usdc` / `amount_atomic` | float / integer | Specify either USD (converted to atomic units) or exact atomic units. |
| `scheme`, `network`, `pay_to` | string | Optional overrides; default to service settings. |
| `timeout_seconds` | integer | Validity window override. |
| `currency`, `memo`, `payer`, `metadata`, `output_schema` | mixed | Enriched metadata baked into `requirements.extra`. |

Return payload:

```json
{
  "header": "<base64 X-PAYMENT>",
  "requirements": { "...": "..." }
}
```

### `x402_paywalled_request`

End-to-end helper that probes a paywalled HTTP endpoint, signs the payment, and retries automatically.

Extra outputs include:

- `paymentHeader` - the `X-PAYMENT` value used for the retry.
- `requirements` - final `PaymentRequirements` after merging paywall + overrides.
- `paymentResponse` - parsed receipt (if the upstream returned `X-PAYMENT-RESPONSE`).

Errors such as `invalid_exact_evm_payload_signature` or misconfigured environment variables are surfaced via `ToolResult.error`.

## CLI commands

Module: `python -m spoon_ai.payments.cli`.

| Command | Description |
| --- | --- |
| `requirements` | Dumps the default `PaymentRequirements` JSON for the current configuration (optionally filtered by `--network`). |
| `sign` | Produces a signed header for a specific `--resource` with optional overrides (`--amount-usdc`, `--memo`, `--currency`, `--metadata`, `--payer`, `--output-schema`). |

All commands respect the same `.env` resolution order (local key first, Turnkey fallback).

## HTTP interfaces

The `spoon_ai.payments.app` module exposes a FastAPI router with two endpoints:

| Endpoint | Method | Description |
| --- | --- | --- |
| `/x402/requirements` | `GET` | Returns current requirements plus metadata for UI discovery. |
| `/x402/invoke/{agent_name}` | `POST` | Accepts `{"prompt": "..."}` payloads. If the request lacks `X-PAYMENT`, the router responds with a 402 body that mirrors facilitator challenges. With a valid header, it verifies, optionally settles, and forwards the prompt to the named agent. On success it includes `X-PAYMENT-RESPONSE` in the response headers. |

## Integration tips

- Reuse `X402PaymentService.decode_payment_response()` anywhere you persist logs or analytics, so you can recover transaction hashes without re-parsing base64.
- Guard outbound payments by passing `max_value` to `build_payment_header` or `x402_create_payment`, which raises if the paywall demands more than you intend to pay.
- When wrapping third-party paywalls, always copy the facilitator-provided `asset`, `pay_to`, and `resource` values into your overrides; mismatches will cause signature rejection.

---

FILE: docs/api-reference/tools/base-tool.md

# Base Tool API Reference

The `BaseTool` class is the foundation for all tools in SpoonOS, providing a standardized interface for tool development and execution.

## Class Definition

```python
from spoon_ai.tools.base import BaseTool
from typing import Any, Dict, Optional

class BaseTool:
    name: str
    description: str

    def __init__(self, **kwargs):
        pass

    async def execute(self, **kwargs) -> Any:
        raise NotImplementedError
```

## Required Attributes

### `name: str`
Unique identifier for the tool. Must be a valid Python identifier.

### `description: str`
Human-readable description of what the tool does. Used by agents to understand tool capabilities.

## Methods

### Core Methods

#### `async execute(**kwargs) -> Any`

Execute the tool with provided parameters.

**Parameters:**
- `**kwargs`: Tool-specific parameters

**Returns:**
- `Any`: Tool execution result

**Raises:**
- `ToolError`: When tool execution fails
- `ValidationError`: When parameters are invalid

**Example:**
```python
class CalculatorTool(BaseTool):
    name = "calculator"
    description = "Perform basic arithmetic operations"

    async def execute(self, operation: str, a: float, b: float) -> float:
        if operation == "add":
            return a + b
        elif operation == "subtract":
            return a - b
        elif operation == "multiply":
            return a * b
        elif operation == "divide":
            if b == 0:
                raise ToolError("Division by zero")
            return a / b
        else:
            raise ValidationError(f"Unknown operation: {operation}")
```

### Validation Methods

#### `validate_parameters(self, **kwargs) -> Dict[str, Any]`

Validate and normalize input parameters.

**Parameters:**
- `**kwargs`: Raw input parameters

**Returns:**
- `Dict[str, Any]`: Validated and normalized parameters

**Example:**
```python
class WebSearchTool(BaseTool):
    name = "web_search"
    description = "Search the web for information"

    def validate_parameters(self, **kwargs) -> Dict[str, Any]:
        query = kwargs.get("query")
        if not query or not isinstance(query, str):
            raise ValidationError("Query must be a non-empty string")

        limit = kwargs.get("limit", 10)
        if not isinstance(limit, int) or limit < 1 or limit > 100:
            raise ValidationError("Limit must be an integer between 1 and 100")

        return {
            "query": query.strip(),
            "limit": limit
        }

    async def execute(self, **kwargs) -> Dict[str, Any]:
        params = self.validate_parameters(**kwargs)
        # Perform web search with validated parameters
        return await self._search(params["query"], params["limit"])
```

### Configuration Methods

#### `configure(self, config: Dict[str, Any]) -> None`

Configure the tool with runtime settings.

**Parameters:**
- `config`: Configuration dictionary

**Example:**
```python
class APITool(BaseTool):
    name = "api_tool"
    description = "Make API requests"

    def __init__(self):
        self.api_key = None
        self.base_url = None
        self.timeout = 30

    def configure(self, config: Dict[str, Any]) -> None:
        self.api_key = config.get("api_key")
        self.base_url = config.get("base_url")
        self.timeout = config.get("timeout", 30)

        if not self.api_key:
            raise ConfigurationError("API key is required")
```

## Tool Metadata

### Schema Definition

```python
class WeatherTool(BaseTool):
    name = "get_weather"
    description = "Get current weather for a location"

    # Define parameter schema
    schema = {
        "type": "object",
        "properties": {
            "location": {
                "type": "string",
                "description": "City name or coordinates"
            },
            "units": {
                "type": "string",
                "enum": ["celsius", "fahrenheit"],
                "default": "celsius",
                "description": "Temperature units"
            }
        },
        "required": ["location"]
    }

    async def execute(self, location: str, units: str = "celsius") -> Dict[str, Any]:
        # Implementation here
        pass
```

### Categories and Tags

```python
class CryptoTool(BaseTool):
    name = "get_crypto_price"
    description = "Get cryptocurrency price data"
    category = "crypto"
    tags = ["price", "market", "cryptocurrency"]
    version = "1.0.0"

    async def execute(self, symbol: str) -> Dict[str, Any]:
        # Implementation here
        pass
```

## Error Handling

### Exception Types

```python
from spoon_ai.tools.errors import (
    ToolError,
    ValidationError,
    ConfigurationError,
    ExecutionError
)

class RobustTool(BaseTool):
    name = "robust_tool"
    description = "Tool with comprehensive error handling"

    async def execute(self, **kwargs) -> Any:
        try:
            # Validate parameters
            params = self.validate_parameters(**kwargs)

            # Execute tool logic
            result = await self._perform_operation(params)

            return result

        except ValidationError:
            # Re-raise validation errors
            raise
        except ConfigurationError:
            # Re-raise configuration errors
            raise
        except Exception as e:
            # Wrap unexpected errors
            raise ExecutionError(f"Tool execution failed: {str(e)}") from e

    def validate_parameters(self, **kwargs) -> Dict[str, Any]:
        # Parameter validation logic
        pass

    async def _perform_operation(self, params: Dict[str, Any]) -> Any:
        # Core tool logic
        pass
```

## Async Patterns

### HTTP Requests

```python
import aiohttp
from typing import Dict, Any

class HTTPTool(BaseTool):
    name = "http_request"
    description = "Make HTTP requests"

    def __init__(self):
        self.session = None

    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def execute(self, url: str, method: str = "GET", **kwargs) -> Dict[str, Any]:
        if not self.session:
            self.session = aiohttp.ClientSession()

        try:
            async with self.session.request(method, url, **kwargs) as response:
                return {
                    "status": response.status,
                    "data": await response.text(),
                    "headers": dict(response.headers)
                }
        except Exception as e:
            raise ExecutionError(f"HTTP request failed: {str(e)}")
```

### Database Operations

```python
import asyncpg
from typing import List, Dict, Any

class DatabaseTool(BaseTool):
    name = "database_query"
    description = "Execute database queries"

    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self.pool = None

    async def initialize(self):
        """Initialize database connection pool"""
        self.pool = await asyncpg.create_pool(self.connection_string)

    async def execute(self, query: str, params: List[Any] = None) -> List[Dict[str, Any]]:
        if not self.pool:
            await self.initialize()

        try:
            async with self.pool.acquire() as connection:
                rows = await connection.fetch(query, *(params or []))
                return [dict(row) for row in rows]
        except Exception as e:
            raise ExecutionError(f"Database query failed: {str(e)}")

    async def cleanup(self):
        """Clean up database connections"""
        if self.pool:
            await self.pool.close()
```

## Testing Tools

### Unit Testing

```python
import pytest
from unittest.mock import AsyncMock, patch
from my_tool import CalculatorTool

@pytest.fixture
def calculator_tool():
    return CalculatorTool()

@pytest.mark.asyncio
async def test_addition(calculator_tool):
    result = await calculator_tool.execute(operation="add", a=5, b=3)
    assert result == 8

@pytest.mark.asyncio
async def test_division_by_zero(calculator_tool):
    with pytest.raises(ToolError, match="Division by zero"):
        await calculator_tool.execute(operation="divide", a=5, b=0)

@pytest.mark.asyncio
async def test_invalid_operation(calculator_tool):
    with pytest.raises(ValidationError, match="Unknown operation"):
        await calculator_tool.execute(operation="invalid", a=5, b=3)
```

### Integration Testing

```python
import pytest
from spoon_ai.tools import ToolManager
from my_tool import WebSearchTool

@pytest.mark.asyncio
async def test_tool_integration():
    # Test tool registration
    tool_manager = ToolManager()
    web_tool = WebSearchTool()

    tool_manager.register_tool(web_tool)
    assert "web_search" in tool_manager.get_tool_names()

    # Test tool execution through manager
    result = await tool_manager.execute_tool(
        "web_search",
        query="SpoonOS documentation",
        limit=5
    )

    assert isinstance(result, dict)
    assert "results" in result
```

## Performance Optimization

### Caching

```python
from functools import lru_cache
import asyncio
from typing import Dict, Any

class CachedTool(BaseTool):
    name = "cached_tool"
    description = "Tool with caching support"

    def __init__(self):
        self._cache = {}
        self._cache_ttl = 300  # 5 minutes

    async def execute(self, **kwargs) -> Any:
        # Create cache key
        cache_key = self._create_cache_key(**kwargs)

        # Check cache
        cached_result = self._get_from_cache(cache_key)
        if cached_result is not None:
            return cached_result

        # Execute and cache result
        result = await self._perform_operation(**kwargs)
        self._set_cache(cache_key, result)

        return result

    def _create_cache_key(self, **kwargs) -> str:
        """Create cache key from parameters"""
        import hashlib
        import json

        key_data = json.dumps(kwargs, sort_keys=True)
        return hashlib.md5(key_data.encode()).hexdigest()

    def _get_from_cache(self, key: str) -> Any:
        """Get value from cache if not expired"""
        import time

        if key in self._cache:
            value, timestamp = self._cache[key]
            if time.time() - timestamp < self._cache_ttl:
                return value
            else:
                del self._cache[key]

        return None

    def _set_cache(self, key: str, value: Any) -> None:
        """Set value in cache with timestamp"""
        import time
        self._cache[key] = (value, time.time())
```

### Connection Pooling

```python
class PooledTool(BaseTool):
    name = "pooled_tool"
    description = "Tool with connection pooling"

    _connection_pool = None
    _pool_size = 10

    @classmethod
    async def get_connection_pool(cls):
        """Get or create connection pool"""
        if cls._connection_pool is None:
            cls._connection_pool = await create_connection_pool(
                size=cls._pool_size
            )
        return cls._connection_pool

    async def execute(self, **kwargs) -> Any:
        pool = await self.get_connection_pool()

        async with pool.acquire() as connection:
            # Use connection for tool operation
            result = await self._perform_operation(connection, **kwargs)
            return result
```

## Best Practices

### Tool Design
- Keep tools focused on a single responsibility
- Use descriptive names and clear descriptions
- Implement proper parameter validation
- Handle errors gracefully
- Document expected inputs and outputs

### Performance
- Use async/await for I/O operations
- Implement caching for expensive operations
- Use connection pooling for database/API tools
- Set appropriate timeouts
- Monitor resource usage

### Security
- Validate all input parameters
- Sanitize user inputs
- Use secure communication protocols
- Implement proper authentication
- Log security-relevant events

### Testing
- Write comprehensive unit tests
- Test error conditions
- Use mocking for external dependencies
- Perform integration testing
- Test with realistic data

## See Also

- [Built-in Tools Reference](./builtin-tools.md)
- [MCP Protocol Reference](../../core-concepts/mcp-protocol.md)
- [Tool Development Guide](../../how-to-guides/add-custom-tools.md)
- [Agent Integration](../agents/base-agent.md)"}

---

FILE: docs/api-reference/tools/builtin-tools.md

# Built-in Tools API Reference

Comprehensive reference for all SpoonOS built-in tools, their configuration, and usage patterns.

## Overview

Built-in tools are core capabilities provided by the **spoon-toolkit** package. These tools are directly integrated into SpoonOS and provide access to blockchain data, market analysis, and external APIs.

## Environment Variables Setup

Configure required environment variables in your `.env` file or system environment:

```bash
# Crypto APIs
OKX_API_KEY=your_okx_api_key
OKX_SECRET_KEY=your_okx_secret_key
OKX_API_PASSPHRASE=your_okx_passphrase
OKX_PROJECT_ID=your_okx_project_id

# Price APIs
COINGECKO_API_KEY=your_coingecko_key
BITQUERY_API_KEY=your_bitquery_key

# Blockchain
RPC_URL=https://eth.llamarpc.com
```

**Note**: Never hardcode API keys in your source code. Always use environment variables for sensitive data.

## Tool Categories

### Crypto Data Tools

#### CryptoPowerDataCEXTool
**Description:** CEX market data and analytics
**Module:** `spoon_toolkits.crypto.crypto_powerdata.tools`

**Configuration:**
```json
{
  "name": "crypto_powerdata_cex",
  "type": "builtin",
  "description": "CEX market data and analytics",
  "enabled": true,
  "env": {
    "OKX_API_KEY": "${OKX_API_KEY}",
    "OKX_SECRET_KEY": "${OKX_SECRET_KEY}",
    "OKX_API_PASSPHRASE": "${OKX_API_PASSPHRASE}",
    "OKX_PROJECT_ID": "${OKX_PROJECT_ID}"
  },
  "config": {
    "timeout": 30,
    "max_retries": 3,
    "cache_duration": 300,
    "default_exchange": "binance",
    "default_timeframe": "1h",
    "max_limit": 500
  }
}
```

**Methods:**
- `get_market_data(symbol, timeframe, limit)`: Get OHLCV data
- `get_order_book(symbol, depth)`: Get order book data
- `get_trades(symbol, limit)`: Get recent trades
- `get_ticker(symbol)`: Get 24h ticker statistics

**Usage Example:**
```python
from spoon_toolkits.crypto.crypto_powerdata.tools import CryptoPowerDataCEXTool

tool = CryptoPowerDataCEXTool(
    exchange="binance",
    symbol="BTC/USDT",
    timeframe="1h",
    limit=100
)

result = await tool.execute(action="get_market_data")
```

#### GetTokenPriceTool
**Description:** Get current token prices from DEX
**Module:** `spoon_toolkits.crypto.crypto_data_tools.price_data`

**Configuration:**
```json
{
  "name": "get_token_price",
  "type": "builtin",
  "description": "Get current token prices from DEX",
  "enabled": true,
  "env": {
    "RPC_URL": "${RPC_URL}"
  },
  "config": {
    "timeout": 30,
    "max_retries": 3,
    "exchange": "uniswap"
  }
}
```

**Methods:**
- `get_price(token_address)`: Get current token price
- `get_price_by_symbol(symbol)`: Get price by token symbol
- `get_multiple_prices(addresses)`: Get prices for multiple tokens

#### Get24hStatsTool
**Description:** Get 24-hour price statistics from DEX
**Module:** `spoon_toolkits.crypto.crypto_data_tools.price_data`

**Configuration:**
```json
{
  "name": "get_24h_stats",
  "type": "builtin",
  "description": "Get 24-hour price statistics from DEX",
  "enabled": true,
  "env": {
    "RPC_URL": "${RPC_URL}"
  },
  "config": {
    "timeout": 30,
    "max_retries": 3,
    "exchange": "uniswap"
  }
}
```

**Methods:**
- `get_24h_stats(token_address)`: Get 24h price change, volume, etc.
- `get_price_change(token_address, period)`: Get price change for period

#### GetKlineDataTool
**Description:** Get k-line (candlestick) data from DEX
**Module:** `spoon_toolkits.crypto.crypto_data_tools.price_data`

**Configuration:**
```json
{
  "name": "get_kline_data",
  "type": "builtin",
  "description": "Get k-line (candlestick) data from DEX",
  "enabled": true,
  "env": {
    "RPC_URL": "${RPC_URL}"
  },
  "config": {
    "timeout": 30,
    "max_retries": 3,
    "exchange": "uniswap"
  }
}
```

**Methods:**
- `get_klines(symbol, interval, limit)`: Get candlestick data
- `get_historical_data(symbol, start_time, end_time)`: Get historical OHLCV

### Blockchain Tools

#### ChainbaseTools
**Description:** Comprehensive blockchain data API
**Module:** `spoon_toolkits.chainbase`

**Configuration:**
```json
{
  "name": "chainbase_tools",
  "type": "builtin",
  "enabled": true,
  "env": {
    "CHAINBASE_API_KEY": "${CHAINBASE_API_KEY}"
  },
  "config": {
    "default_chain": "ethereum",
    "rate_limit": 100,
    "timeout": 30
  }
}
```

**Methods:**
- `get_account_balance(address, chain)`: Get account balance
- `get_transaction_history(address, limit)`: Get transaction history
- `get_token_metadata(contract_address)`: Get token information
- `get_nft_metadata(contract_address, token_id)`: Get NFT metadata

#### ThirdWebTools
**Description:** Web3 development tools via ThirdWeb API
**Module:** `spoon_toolkits.third_web`

**Configuration:**
```json
{
  "name": "thirdweb_tools",
  "type": "builtin",
  "enabled": true,
  "env": {
    "THIRDWEB_API_KEY": "${THIRDWEB_API_KEY}"
  },
  "config": {
    "default_chain": "ethereum",
    "timeout": 30
  }
}
```

**Methods:**
- `deploy_contract(contract_type, params)`: Deploy smart contract
- `interact_contract(address, method, params)`: Interact with contract
- `get_contract_events(address, event_name)`: Get contract events

### Neo Blockchain Tools

#### NeoTools
**Description:** Complete Neo ecosystem tools
**Module:** `spoon_toolkits.neo`

**Configuration:**
```json
{
  "name": "neo_tools",
  "type": "builtin",
  "enabled": true,
  "env": {
    "NEO_RPC_URL": "${NEO_RPC_URL}"
  },
  "config": {
    "network": "mainnet",
    "timeout": 30
  }
}
```

**Methods:**
- `get_address_info(address)`: Get Neo address information
- `get_asset_info(asset_id)`: Get asset details
- `get_contract_info(contract_hash)`: Get contract information
- `get_transaction_info(tx_hash)`: Get transaction details

### Storage Tools

#### DecentralizedStorageTools
**Description:** Decentralized storage integration (AIOZ, 4EVERLAND, OORT)
**Module:** `spoon_toolkits.storage`

**Configuration:**
```json
{
  "name": "storage_tools",
  "type": "builtin",
  "enabled": true,
  "env": {
    "AIOZ_API_KEY": "${AIOZ_API_KEY}",
    "FOUREVERLAND_API_KEY": "${FOUREVERLAND_API_KEY}",
    "OORT_API_KEY": "${OORT_API_KEY}"
  },
  "config": {
    "default_provider": "aioz",
    "timeout": 60
  }
}
```

**Methods:**
- `upload_file(file_path, provider)`: Upload file to storage
- `download_file(file_hash, provider)`: Download file from storage
- `list_files(provider)`: List stored files
- `delete_file(file_hash, provider)`: Delete file from storage


## Agent Integration Examples

### Basic Trading Agent

```python
import os
from spoon_ai.agents import SpoonReactAI
from spoon_toolkits.crypto.crypto_powerdata.tools import CryptoPowerDataCEXTool
from spoon_toolkits.crypto.crypto_data_tools.price_data import GetTokenPriceTool

# Initialize tools
crypto_tool = CryptoPowerDataCEXTool(
    exchange="binance",
    symbol="BTC/USDT",
    timeframe="1h",
    limit=100
)

price_tool = GetTokenPriceTool(
    exchange="uniswap"
)

# Create agent with tools
trading_agent = SpoonReactAI(
    name="trading_agent",
    tools=[crypto_tool, price_tool],
    config={
        "max_steps": 10,
        "temperature": 0.3
    }
)

# Use agent
response = await trading_agent.run("Analyze BTC market data")
```

### Complete Configuration Example

```json
{
  "default_agent": "trading_agent",
  "agents": {
    "trading_agent": {
      "class": "SpoonReactAI",
      "description": "Trading agent with built-in tools",
      "aliases": ["trader"],
      "config": {
        "max_steps": 10,
        "temperature": 0.3
      },
      "tools": [
        {
          "name": "crypto_powerdata_cex",
          "type": "builtin",
          "description": "CEX market data",
          "enabled": true,
          "env": {
            "OKX_API_KEY": "${OKX_API_KEY}",
            "OKX_SECRET_KEY": "${OKX_SECRET_KEY}",
            "OKX_API_PASSPHRASE": "${OKX_API_PASSPHRASE}"
          },
          "config": {
            "timeout": 30,
            "max_retries": 3
          }
        },
        {
          "name": "get_token_price",
          "type": "builtin",
          "description": "Price lookup",
          "enabled": true,
          "env": {
            "COINGECKO_API_KEY": "${COINGECKO_API_KEY}"
          },
          "config": {
            "timeout": 30,
            "max_retries": 3
          }
        },
        {
          "name": "security_tools",
          "type": "builtin",
          "description": "Token security analysis",
          "enabled": true,
          "env": {
            "GOPLUSLABS_API_KEY": "${GOPLUSLABS_API_KEY}"
          },
          "config": {
            "timeout": 30,
            "max_retries": 3
          }
        }
      ]
    }
  }
}
```

## Configuration Patterns

### Environment Variable References
Use `${VARIABLE_NAME}` syntax to reference environment variables:

```json
{
  "env": {
    "API_KEY": "${MY_API_KEY}",
    "SECRET": "${MY_SECRET}"
  }
}
```

### Tool Grouping
Group related tools by functionality:

```json
{
  "tools": [
    {
      "name": "crypto_powerdata_cex",
      "type": "builtin",
      "category": "market_data"
    },
    {
      "name": "get_token_price",
      "type": "builtin",
      "category": "market_data"
    },
    {
      "name": "security_tools",
      "type": "builtin",
      "category": "security"
    }
  ]
}
```

### Conditional Tool Loading
Enable tools based on environment:

```json
{
  "name": "advanced_tool",
  "type": "builtin",
  "enabled": "${ENABLE_ADVANCED_TOOLS:-false}",
  "env": {
    "API_KEY": "${ADVANCED_API_KEY}"
  }
}
```

## Error Handling

### Common Issues

| Error | Cause | Solution |
|-------|-------|----------|
| `Tool not found` | Tool name misspelled | Check tool name in configuration |
| `API key missing` | Environment variable not set | Set required environment variables |
| `Timeout error` | API response too slow | Increase timeout in tool config |
| `Rate limit exceeded` | Too many API calls | Implement rate limiting or caching |
| `Invalid configuration` | Malformed JSON | Validate configuration syntax |

### Debug Mode
Enable debug logging for tools:

```json
{
  "config": {
    "debug": true,
    "log_level": "debug",
    "log_requests": true
  }
}
```

### Retry Configuration
Configure retry behavior:

```json
{
  "config": {
    "max_retries": 3,
    "retry_delay": 1.0,
    "exponential_backoff": true,
    "retry_on_errors": ["timeout", "rate_limit"]
  }
}
```

## Performance Optimization

### Caching
Enable caching for expensive operations:

```json
{
  "config": {
    "cache_enabled": true,
    "cache_duration": 300,
    "cache_size": 1000
  }
}
```

### Connection Pooling
Optimize HTTP connections:

```json
{
  "config": {
    "connection_pool_size": 10,
    "connection_timeout": 30,
    "keep_alive": true
  }
}
```

### Batch Operations
Process multiple requests efficiently:

```json
{
  "config": {
    "batch_size": 100,
    "batch_timeout": 60,
    "parallel_requests": 5
  }
}
```

## Security Best Practices

### API Key Management
- Never hardcode API keys in configuration files
- Use environment variables for all sensitive data
- Rotate API keys regularly
- Use different keys for different environments

### Access Control
- Limit tool permissions to minimum required
- Use read-only keys when possible
- Monitor API usage and set alerts
- Implement rate limiting

### Data Protection
- Encrypt sensitive data at rest
- Use HTTPS for all API communications
- Validate all input parameters
- Log security events

## See Also

- [Base Tool API](./base-tool.md)
- [MCP Protocol](../../core-concepts/mcp-protocol.md)
- [Custom Tool Development](../../how-to-guides/add-custom-tools.md)
- [Agent Configuration](../agents/base-agent.md)"}

---

FILE: docs/cli/advanced-features.md

# Advanced CLI Features

This guide covers advanced  features including MCP integration, custom agents, scripting, and automation capabilities.

## Model Context Protocol (MCP) Integration

### MCP Overview

MCP allows  to integrate with external tools and services through standardized protocols.

### Configuring MCP Servers

Add MCP server configurations to your `config.json`:

```json
{
  "mcp_servers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/Users/username/Documents"]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": {
        "GITHUB_PERSONAL_ACCESS_TOKEN": "ghp_your_token_here"
      }
    },
    "slack": {
      "command": "uvx",
      "args": ["mcp-server-slack"],
      "env": {
        "SLACK_BOT_TOKEN": "xoxb-your-bot-token"
      }
    }
  }
}
```

### MCP Agent Configuration

Create agents with MCP support:

```json
{
  "agents": {
    "mcp_agent": {
      "class_name": "SpoonReactMCP",
      "description": "Agent with full MCP integration",
      "mcp_servers": ["filesystem", "github", "slack"],
      "config": {
        "llm_provider": "anthropic",
        "model_name": "claude-3-sonnet-20240229"
      }
    }
  }
}
```

### Testing MCP Connections

```bash
# Start spoon-cli and run:
# Validate MCP server configurations
validate-config --check-servers

# Load MCP-enabled agent
load-agent mcp_agent

# List available MCP tools
action list_mcp_tools
```

## Custom Agent Development

### Agent Configuration Structure

```json
{
  "agents": {
    "custom_agent": {
      "class_name": "SpoonReactAI",
      "description": "Description of your custom agent",
      "aliases": ["ca", "custom"],
      "config": {
        "llm_provider": "anthropic",
        "model_name": "claude-3-sonnet-20240229",
        "temperature": 0.7,
        "max_tokens": 4000,
        "custom_parameter": "value"
      },
      "tools": [
        "web_search",
        "calculator",
        "custom_tool_1",
        "custom_tool_2"
      ],
      "system_prompt": "Custom system prompt for specialized behavior"
    }
  }
}
```

### Specialized Agent Types

#### Research Agent

```json
{
  "research_agent": {
    "class_name": "SpoonReactAI",
    "description": "Specialized research and analysis agent",
    "config": {
      "llm_provider": "openai",
      "model_name": "gpt-4-turbo-preview",
      "temperature": 0.3
    },
    "tools": [
      "web_search",
      "academic_search",
      "data_analysis",
      "citation_manager"
    ]
  }
}
```

#### Trading Agent

```json
{
  "trading_agent": {
    "class_name": "SpoonReactAI",
    "description": "Cryptocurrency trading assistant",
    "config": {
      "llm_provider": "anthropic",
      "model_name": "claude-3-sonnet-20240229",
      "temperature": 0.2
    },
    "tools": [
      "crypto_price_lookup",
      "dex_swap",
      "wallet_balance",
      "market_data",
      "technical_indicators"
    ]
  }
}
```

#### Code Review Agent

```json
{
  "code_review_agent": {
    "class_name": "SpoonReactAI",
    "description": "Code review and development assistant",
    "config": {
      "llm_provider": "openai",
      "model_name": "gpt-4",
      "temperature": 0.1
    },
    "tools": [
      "file_operations",
      "git_operations",
      "code_analysis",
      "testing_tools"
    ]
  }
}
```

## Scripting and Automation

### Command Chaining

Execute multiple commands in sequence:

```bash
# Setup and start session (these would be run in spoon-cli interactive mode)
# load-agent research_agent
# load-toolkit-tools web_search academic
# action chat
```

### Batch Operations

Create scripts for automated workflows:

```bash
#!/bin/bash
# research_workflow.sh

# Note: This script would need to be run in spoon-cli interactive mode
# Start spoon-cli first, then run these commands:
# load-agent research_agent
# load-toolkit-tools web_search data_analysis
# load-docs ./research_papers/
# action chat

# For automation, you would need to:
echo "Starting automated research analysis..."
# spoon-cli action chat << EOF
Please analyze the loaded documents and provide a comprehensive summary of the key findings, methodologies, and conclusions.
EOF
```

### Environment-Specific Configurations

Use different configurations for different environments:

```json
{
  "profiles": {
    "development": {
      "default_agent": "debug_agent",
      "llm": {
        "timeout": 60,
        "retry_attempts": 5
      },
      "logging": {
        "level": "DEBUG"
      }
    },
    "production": {
      "default_agent": "production_agent",
      "llm": {
        "timeout": 30,
        "retry_attempts": 2
      },
      "logging": {
        "level": "WARNING"      }
    }
  }
}
```

Activate profiles:

```bash
export SPOON_CLI_PROFILE="production"
# Start spoon-cli and run:
system-info
```

## Advanced Tool Integration

### Custom Tool Development

#### Tool Factory Pattern

```json
{
  "tool_factories": {
    "custom_api_tool": {
      "factory": "my_package.tools.CustomAPIToolFactory",
      "config": {
        "api_endpoint": "https://api.example.com",
        "api_key": "${API_KEY}"
      }
    }
  }
}
```

#### Dynamic Tool Loading

```bash
# Start spoon-cli and run:
# Load tools from specific categories
load-toolkit-tools crypto blockchain

# Load all available tools (use with caution)
load-toolkit-tools all

# Load tools with specific configuration
load-toolkit-tools web_search --timeout 30 --max-results 20
```

### Tool Configuration

Advanced tool configuration in `config.json`:

```json
{
  "tool_configs": {
    "web_search": {
      "max_results": 10,
      "timeout": 30,
      "user_agent": "SpoonAI/1.0",
      "proxies": {
        "http": "http://proxy.company.com:8080",
        "https": "https://proxy.company.com:8080"
      }
    },
    "crypto_price_lookup": {
      "cache_timeout": 300,
      "preferred_exchanges": ["binance", "coinbase", "kraken"]
    }
  }
}
```

## Performance Optimization

### LLM Configuration Tuning

```json
{
  "llm": {
    "default_provider": "anthropic",
    "fallback_chain": ["anthropic", "openai", "deepseek"],
    "timeout": 30,
    "retry_attempts": 3,
    "rate_limiting": {
      "requests_per_minute": 60,
      "burst_limit": 10
    },
    "caching": {
      "enabled": true,
      "ttl": 3600,
      "max_size": "1GB"
    }
  }
}
```

### Memory Management

```json
{
  "memory": {
    "max_chat_history": 100,
    "compress_old_messages": true,
    "auto_save_interval": 300,
    "cleanup_temp_files": true
  }
}
```

### Parallel Processing

```json
{
  "parallel_processing": {
    "max_concurrent_requests": 5,
    "thread_pool_size": 10,
    "async_timeout": 60
  }
}
```

## Logging and Debugging

### Advanced Logging Configuration

```json
{
  "logging": {
    "level": "INFO",
    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    "file": "spoon_cli.log",
    "max_file_size": "10MB",
    "backup_count": 5,
    "handlers": {
      "console": {
        "level": "WARNING",
        "format": "%(levelname)s: %(message)s"
      },
      "file": {
        "level": "DEBUG",
        "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
      }
    }
  }
}
```

### Debug Mode

Enable detailed debugging:

```bash
export SPOON_CLI_DEBUG=1

# Or use command-line flag when starting spoon-cli
# spoon-cli --debug

# Then run:
# system-info
```

### Performance Profiling

```bash
# Enable performance logging
export SPOON_CLI_PROFILE=1

# Start spoon-cli and run:
# action chat

# Check performance metrics
# system-info --performance
```

## Security Features

### API Key Management

```json
{
  "security": {
    "api_key_rotation": {
      "enabled": true,
      "interval_days": 30
    },
    "key_validation": {
      "enabled": true,
      "cache_timeout": 3600
    },
    "audit_logging": {
      "enabled": true,
      "log_sensitive_operations": false
    }
  }
}
```

### Network Security

```json
{
  "network": {
    "ssl_verification": true,
    "proxy": "http://proxy.company.com:8080",
    "timeout": 30,
    "retry_on_failure": true,
    "trusted_domains": [
      "*.openai.com",
      "*.anthropic.com",
      "*.googleapis.com"
    ]
  }
}
```

## Integration Features

### Webhook Support

Configure webhooks for external integrations:

```json
{
  "webhooks": {
    "telegram_bot": {
      "enabled": true,
      "token": "${TELEGRAM_BOT_TOKEN}",
      "allowed_users": ["user1", "user2"]
    },
    "slack_integration": {
      "enabled": false,
      "webhook_url": "${SLACK_WEBHOOK_URL}",
      "channel": "#ai-notifications"
    }
  }
}
```

### API Server Mode

Run as an API server:

```bash
# Start spoon-cli and run:
serve --host 0.0.0.0 --port 8000
```

### Database Integration

```json
{
  "database": {
    "type": "postgresql",
    "url": "${DATABASE_URL}",
    "connection_pool": {
      "min_size": 5,
      "max_size": 20
    }
  }
}
```

## Custom Extensions

### Plugin System

Load custom plugins:

```json
{
  "plugins": {
    "my_custom_plugin": {
      "path": "/path/to/my/plugin",
      "enabled": true,
      "config": {
        "custom_setting": "value"
      }
    }
  }
}
```

### Custom Commands

Extend CLI with custom commands:

```python
# custom_commands.py
from spoon_cli.commands import SpoonCommand

def my_custom_command(input_list):
    # Custom command implementation
    print("Custom command executed!")

# Register command
custom_command = SpoonCommand(
    name="my-command",
    description="My custom command",
    handler=my_custom_command
)
```

## Monitoring and Analytics

### Usage Analytics

```json
{
  "analytics": {
    "enabled": true,
    "metrics": {
      "commands_used": true,
      "response_times": true,
      "error_rates": true
    },
    "reporting": {
      "interval": "1h",
      "destination": "console"
    }
  }
}
```

### Health Monitoring

```bash
# Start spoon-cli and run:
# Continuous health monitoring
monitor --interval 60

# Export metrics
export-metrics --format json --output metrics.json
```

## Next Steps

- [Troubleshooting](./troubleshooting.md) - Solve advanced issues
- [Configuration Guide](./configuration.md) - Master configuration options
- [API Reference](../api-reference/cli/commands.md) - Complete command reference

---

FILE: docs/cli/basic-usage.md

# Basic CLI Usage

This guide covers the fundamental operations of spoon-cli, from starting your first session to performing common tasks.

## Starting the CLI

### Interactive Mode

The most common way to use spoon-cli is through interactive mode:

```bash
spoon-cli
```

This starts the interactive command-line interface where you can enter commands and chat with agents.

### Interactive Commands

Once in the spoon-cli interactive mode, you can run various commands:

```bash
# Load an agent and start chat
action chat

# List available agents
list-agents

# Show system information
system-info
```

## Core Commands

### Help System

Get help on available commands:

```bash
# Show all commands
help

# Get help for specific command
help load-agent
```

### Agent Management

#### List Available Agents

```bash
list-agents
```

Shows all configured agents with their descriptions and aliases.

#### Load an Agent

```bash
load-agent react
# or using alias
load-agent r
```

Loads the specified agent for use in subsequent operations.

#### Check Current Agent

The prompt shows the currently loaded agent:

```
Spoon AI (react) >
```

### Chat Operations

#### Start Chat Mode

```bash
action chat
```

Starts an interactive chat session with the current agent.

#### Create New Chat

```bash
new-chat
```

Clears the current chat history and starts fresh.

#### List Chat Histories

```bash
list-chats
```

Shows available saved chat sessions.

#### Load Previous Chat

```bash
load-chat react_session_001
```

Loads a previously saved chat session.

### Configuration Management

#### View Configuration

```bash
config
```

Shows all current configuration settings.

#### Set Configuration Values

```bash
# Set API key
config api_key openai "sk-your-key-here"

# Set default agent
config default_agent "my_agent"

# Set LLM provider
config llm.default_provider "anthropic"
```

#### Reload Configuration

```bash
reload-config
```

Reloads configuration after making changes to config files.

## Interactive Chat Mode

### Starting Chat

Once you enter chat mode (`action chat`), you'll see:

```
Spoon AI (react) > action chat
Starting chat with react
Type your message and press Enter to send.
Press Ctrl+C or Ctrl+D to exit chat mode.
Chat log will be saved to: chat_logs/chat_react_20241201_143022.txt

You >
```

### Basic Chat Interaction

```bash
You > Hello, can you help me analyze some cryptocurrency data?

react: I'd be happy to help you analyze cryptocurrency data. What specific cryptocurrencies or data are you interested in? I have access to various crypto analysis tools including price lookups, market data, and technical indicators.
```

### Special Commands in Chat

While in chat mode, you can use special commands:

- `Ctrl+C` or `Ctrl+D`: Exit chat mode
- Multi-line input: Continue typing, press Enter twice to send

### Chat History

All conversations are automatically saved to timestamped files in the `chat_logs/` directory.

## Tool Integration

### List Available Toolkits

```bash
list-toolkit-categories
```

Shows all available tool categories.

### List Tools in Category

```bash
list-toolkit-tools crypto
```

Shows tools available in the crypto category.

### Load Toolkit Tools

```bash
load-toolkit-tools crypto web_search
```

Loads tools from specified categories into the current agent.

## Document Operations

### Load Documents

```bash
load-docs /path/to/documents
load-docs /path/to/file.pdf
load-docs /path/to/folder --glob "*.txt"
```

Loads documents into the current agent for analysis and querying.

## System Information

### Health Check

```bash
system-info
```

Provides comprehensive system information including:
- Environment variables status
- Configuration file status
- Available agents and tools
- API key configuration
- Health score and recommendations

### LLM Status

```bash
llm-status
```

Shows LLM provider configuration and availability status.

## Blockchain Operations

### Token Information

```bash
token-info 0xA0b86a33E6441e88C5F2712C3E9b74E39E9f6E5a
token-by-symbol ETH
```

Get information about specific tokens.

### Transfer Tokens

```bash
transfer 0x742d35Cc6634C0532925a3b844Bc454e4438f44e 1.5
transfer 0x742d35Cc6634C0532925a3b844Bc454e4438f44e 100 USDC
```

Transfer native tokens or ERC-20 tokens.

### Token Swapping

```bash
swap ETH USDC 1.0
swap UNI WETH 100 --slippage 0.5
```

Swap tokens using integrated DEX aggregator.

## Social Media Integration

### Telegram Bot

```bash
telegram
```

Starts the Telegram bot client for social media interactions.

## Configuration Validation

### Validate Configuration

```bash
validate-config
```

Checks configuration for issues and missing requirements.

### Check Migration Status

```bash
check-config
```

Checks if configuration needs migration to new format.

### Migrate Configuration

```bash
migrate-config
```

Migrates legacy configuration to the new unified format.

## Command-line Options

### Global Options

```bash
--help              # Show help
--version           # Show version
--config FILE       # Use specific config file
--debug             # Enable debug mode
```

### Command-specific Options

```bash
migrate-config --dry-run    # Preview migration
validate-config --check-env # Check environment variables
```

## Examples

### Complete Workflow

```bash
# Start spoon-cli
spoon-cli

# Then run these commands in interactive mode:
# 1. Check system status
system-info

# 2. Configure API keys
config api_key openai "sk-your-key"
config api_key anthropic "sk-ant-your-key"

# 3. List and load agent
list-agents
load-agent react

# 4. Load useful tools
load-toolkit-tools crypto web_search

# 5. Start chatting
action chat
```

### Crypto Analysis Session

```bash
# Load crypto-focused agent
load-agent crypto_analyzer

# Load crypto tools
load-toolkit-tools crypto

# Analyze specific token
action chat
# Then ask: "Analyze the current market sentiment for BTC and ETH"
```

### Document Analysis

```bash
# Load agent and documents
load-agent react
load-docs ./research_papers/

# Start analysis
action chat
# Then ask: "Summarize the key findings from the loaded documents"
```

## Keyboard Shortcuts

### Main CLI
- `Ctrl+C`: Interrupt current operation
- `Ctrl+D`: Exit CLI (at main prompt)
- `â†‘/â†“`: Navigate command history
- `Tab`: Auto-complete commands

### Chat Mode
- `Ctrl+C`: Exit chat mode
- `Ctrl+D`: Exit chat mode
- `â†‘/â†“`: Navigate message history

## Best Practices

### Session Management

1. **Use descriptive agent names** for different tasks
2. **Save important chats** with meaningful filenames
3. **Regularly update configuration** as needs change

### Performance

1. **Load only needed tools** to reduce startup time
2. **Use appropriate models** for your tasks
3. **Monitor system resources** with `system-info`

### Security

1. **Never share API keys** in chat sessions
2. **Use environment variables** for sensitive data
3. **Regularly rotate API keys**

## Next Steps

- [Advanced Features](./advanced-features.md) - Learn advanced CLI capabilities
- [Configuration Guide](./configuration.md) - Deep dive into configuration options
- [Troubleshooting](./troubleshooting.md) - Solve common issues

---

FILE: docs/cli/configuration.md

# CLI Configuration

The  uses a flexible, multi-layered configuration system that allows you to customize behavior through various methods. This guide covers all configuration options and methods.

## Configuration Hierarchy

Configurations are applied in the following order (later sources override earlier ones):

1. **Built-in defaults** - Predefined sensible defaults
2. **Global config file** - `~/.spoon/config.json` or `~/.config/spoon/config.json`
3. **Project config file** - `./config.json` in current directory
4. **Environment variables** - Shell environment variables
5. **Command-line arguments** - Flags and options passed to commands

## Configuration File

The primary configuration method is through a JSON configuration file. Create `config.json` in your project root or home directory.

### Basic Configuration

```json
{
  "default_agent": "react",
  "api_keys": {
    "openai": "sk-your-openai-key-here",
    "anthropic": "sk-ant-your-anthropic-key-here"
  },
  "agents": {
    "my_agent": {
      "class_name": "SpoonReactAI",
      "description": "My custom agent",
      "config": {
        "llm_provider": "openai",
        "model_name": "gpt-4",
        "temperature": 0.7
      },
      "tools": ["web_search", "calculator"]
    }
  }
}
```

### Advanced Configuration

```json
{
  "default_agent": "react",
  "api_keys": {
    "openai": "sk-...",
    "anthropic": "sk-ant-...",
    "deepseek": "sk-...",
    "gemini": "your-gemini-key"
  },
  "agents": {
    "react": {
      "class_name": "SpoonReactAI",
      "description": "Standard React agent",
      "aliases": ["r"],
      "config": {
        "llm_provider": "anthropic",
        "model_name": "claude-3-sonnet-20240229",
        "temperature": 0.7,
        "max_tokens": 4000
      },
      "tools": ["web_search", "file_operations", "calculator"]
    },
    "mcp_agent": {
      "class_name": "SpoonReactMCP",
      "description": "Agent with MCP support",
      "config": {
        "llm_provider": "openai",
        "model_name": "gpt-4-turbo-preview"
      },
      "mcp_servers": ["filesystem", "github"]
    }
  },
  "mcp_servers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/Users/username/Documents"]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": {
        "GITHUB_PERSONAL_ACCESS_TOKEN": "ghp_..."
      }
    },
    "slack": {
      "command": "uvx",
      "args": ["mcp-server-slack"],
      "env": {
        "SLACK_BOT_TOKEN": "xoxb-..."
      }
    }
  },
  "llm": {
    "fallback_chain": ["anthropic", "openai", "deepseek"],
    "timeout": 30,
    "retry_attempts": 3
  },
  "logging": {
    "level": "INFO",
    "file": "spoon_cli.log",
    "max_file_size": "10MB",
    "backup_count": 5
  }
}
```

## Environment Variables

Environment variables provide an alternative way to configure sensitive information like API keys.

### LLM Provider API Keys

```bash
# Primary LLM providers
export OPENAI_API_KEY="sk-proj-..."
export ANTHROPIC_API_KEY="sk-ant-api03-..."
export DEEPSEEK_API_KEY="sk-..."

# Additional providers
export GEMINI_API_KEY="AIza..."
export OPENROUTER_API_KEY="sk-or-v1-..."
export TAVILY_API_KEY="tvly-..."
```

### Blockchain Configuration

```bash
export PRIVATE_KEY="0x..."
export RPC_URL="https://mainnet.infura.io/v3/YOUR_PROJECT_ID"
export CHAIN_ID="1"
export SCAN_URL="https://etherscan.io"
```

### Application Settings

```bash
export SPOON_CLI_CONFIG_FILE="/path/to/custom/config.json"
export SPOON_CLI_LOG_LEVEL="DEBUG"
export SPOON_CLI_DEFAULT_AGENT="my_agent"
```

## Configuration Management Commands

The CLI provides built-in commands to manage configuration:

### View Configuration

```bash
# Start spoon-cli and run:
# Show all configuration
config

# Show specific key
config api_keys.openai

# Show LLM status
llm-status
```

### Modify Configuration

```bash
# Start spoon-cli and run:
# Set API key
config api_key openai "sk-your-key"

# Set configuration value
config default_agent "my_custom_agent"

# Set nested configuration
config llm.timeout 60
```

### Configuration Validation

```bash
# Start spoon-cli and run:
# Validate current configuration
validate-config

# Check for migration needs
check-config

# Migrate legacy configuration
# Start spoon-cli and run:
migrate-config
```

## Configuration Sections

### Agents Configuration

Define custom agents with specific configurations:

```json
{
  "agents": {
    "crypto_trader": {
      "class_name": "SpoonReactAI",
      "description": "Cryptocurrency trading assistant",
      "config": {
        "llm_provider": "anthropic",
        "model_name": "claude-3-sonnet-20240229",
        "temperature": 0.3,
        "max_tokens": 2000
      },
      "tools": [
        "crypto_price_lookup",
        "dex_swap",
        "wallet_balance",
        "market_data"
      ]
    }
  }
}
```

### MCP Servers Configuration

Configure Model Context Protocol servers for extended capabilities:

```json
{
  "mcp_servers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/allowed/path"],
      "env": {}
    },
    "git": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-git", "--repository", "/path/to/repo"]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": {
        "GITHUB_PERSONAL_ACCESS_TOKEN": "ghp_..."
      }
    }
  }
}
```

### LLM Configuration

Configure LLM provider settings and fallback chains:

```json
{
  "llm": {
    "default_provider": "anthropic",
    "fallback_chain": ["anthropic", "openai", "deepseek"],
    "timeout": 30,
    "retry_attempts": 3,
    "providers": {
      "openai": {
        "base_url": null,
        "organization": null
      },
      "anthropic": {
        "max_tokens": 4096
      }
    }
  }
}
```

## Configuration Profiles

Use different configurations for different environments:

```json
{
  "profiles": {
    "development": {
      "default_agent": "debug_agent",
      "llm": {
        "timeout": 60,
        "retry_attempts": 5
      }
    },
    "production": {
      "default_agent": "production_agent",
      "logging": {
        "level": "WARNING"
      }
    }
  }
}
```

Activate a profile:

```bash
export SPOON_CLI_PROFILE="production"

# Then start spoon-cli with the profile active
```

## Configuration File Locations

The CLI searches for configuration files in the following order:

1. **Project-specific**: `./config.json`
2. **User-specific**: `~/.spoon/config.json`
3. **System-wide**: `/etc/spoon/config.json`
4. **XDG Base Directory**: `~/.config/spoon/config.json`

## Security Best Practices

### API Keys Management

1. **Never commit API keys** to version control
2. **Use environment variables** for sensitive data
3. **Rotate keys regularly**
4. **Use different keys** for different environments

### File Permissions

```bash
# Set restrictive permissions on config files
chmod 600 config.json
chmod 600 .env
```

### Environment Separation

Use different configurations for different environments:

```bash
# Development
cp config.json config.dev.json
export SPOON_CLI_CONFIG_FILE="config.dev.json"

# Production
cp config.json config.prod.json
export SPOON_CLI_CONFIG_FILE="config.prod.json"
```

## Troubleshooting Configuration

### Common Issues

1. **Configuration not loading**
   ```bash
# Start spoon-cli and run:
validate-config
# Check file syntax and permissions
   ```

2. **API keys not working**
   ```bash
# Start spoon-cli and run:
llm-status
# Verify key format and validity
   ```

3. **MCP servers not connecting**
   ```bash
# Start spoon-cli and run:
validate-config --check-servers
# Check server commands and environment variables
   ```

### Debug Configuration

Enable debug logging to troubleshoot configuration issues:

```bash
export SPOON_CLI_DEBUG=1
# Start spoon-cli and run:
config
```

## Next Steps

After configuring :
- [Basic Usage](./basic-usage.md) - Learn basic CLI operations
- [Advanced Features](./advanced-features.md) - Explore advanced capabilities
- [Troubleshooting](./troubleshooting.md) - Solve common issues

---

FILE: docs/cli/installation.md

# CLI Installation

The  is the command-line interface for SpoonAI, providing an easy way to interact with AI agents, manage configurations, and perform various operations.

## Installation Methods

### Method 1: Install from PyPI (Recommended)

```bash
pip install spoon-cli
```

This installs the latest stable version of  along with all necessary dependencies.

### Method 2: Install from Source

If you want to install from source or contribute to development:

```bash
# Clone the repository
git clone https://github.com/XSpoonAi/spoon-cli.git
cd spoon-cli/
# Install in development mode
pip install -e .
```

## System Requirements

### Minimum Requirements

- **Python**: 3.12 or higher
- **Operating System**: Linux, macOS, or Windows
- **Memory**: 4GB RAM minimum, 8GB recommended
- **Disk Space**: 500MB for installation

### Recommended Requirements

- **Python**: 3.12+
- **Memory**: 16GB RAM
- **CPU**: Multi-core processor

## Dependencies

The  automatically installs the following core dependencies:

- **spoon-ai-sdk**: Core SpoonAI framework
- **spoon-toolkits**: Additional tool collections
- **prompt_toolkit**: Enhanced command-line interface
- **python-dotenv**: Environment variable management
- **fastmcp**: MCP (Model Context Protocol) support
- **pydantic**: Data validation
- **websockets**: WebSocket communication

## Verification

After installation, verify the installation by checking the version:

```bash
# Check version
spoon-cli --version
```

Or start the interactive CLI:

```bash
# Start spoon-cli
spoon-cli
```

## Post-Installation Setup

### 1. Configure Environment Variables

Create a `.env` file in your project directory or set environment variables:

```bash
# LLM API Keys (choose at least one)
export OPENAI_API_KEY="sk-your-openai-key"
export ANTHROPIC_API_KEY="sk-ant-your-anthropic-key"
export DEEPSEEK_API_KEY="your-deepseek-key"

# Optional: Blockchain operations
export PRIVATE_KEY="your-wallet-private-key"
export RPC_URL="https://mainnet.infura.io/v3/YOUR_PROJECT_ID"
```

### 2. Create Configuration File

Create a `config.json` file for advanced configuration (optional but recommended):

```json
{
  "default_agent": "react",
  "api_keys": {
    "openai": "sk-your-key-here"
  }
}
```

### 3. Test Installation

Run the system health check:

```bash
# Start spoon-cli and run:
system-info
```

This command will show:
- System information
- Environment variable status
- Configuration file status
- Available agents and tools

## Troubleshooting Installation

### Common Issues

1. **Python Version Too Old**

```bash
python --version
# Should show 3.12 or higher
```

2. **Permission Denied**

```bash
# Use user installation
pip install --user
# Or use virtual environment
python -m venv spoon_env
source spoon_env/bin/activate  # Linux/macOS
# spoon_env\Scripts\activate    # Windows
pip install
```

3. **Dependency Conflicts**

```bash
# Upgrade pip
pip install --upgrade pip

# Install in isolated environment
pip install --isolated
```

### Windows-Specific Issues

On Windows, you might need to:

1. **Add Python to PATH** during installation
2. **Use Command Prompt or PowerShell** instead of bash
3. **Install Microsoft Visual C++ Redistributable** if required

## Upgrading

To upgrade to the latest version:

```bash
pip install --upgrade spoon-cli
```

## Uninstalling

To remove spoon-cli:

```bash
pip uninstall spoon-cli
```

Note: This will not remove configuration files or chat histories you may have created.

## Next Steps

After installation, proceed to:
- [Configuration Guide](./configuration.md) - Learn how to configure
- [Basic Usage](./basic-usage.md) - Start using
- [Advanced Features](./advanced-features.md) - Explore advanced capabilities

---

FILE: docs/cli/troubleshooting.md

# CLI Troubleshooting

This guide helps you diagnose and resolve common issues with . Follow the systematic approach to identify and fix problems.

## Quick Diagnosis

### System Health Check

Run the comprehensive health check first:

```bash
# Start  and run:
system-info
```

This command shows:
- Environment variables status
- Configuration file validity
- API key configuration
- Agent and tool availability
- Overall health score

### LLM Provider Status

Check LLM provider configuration:

```bash
# Start  and run:
llm-status
```

This shows:
- Available providers
- API key status (masked)
- Default provider
- Fallback chain configuration

## Configuration Issues

### Configuration File Problems

#### Invalid JSON Syntax

**Symptoms:**
- Configuration loading errors
- CLI fails to start

**Diagnosis:**
```bash
# Start spoon-cli and run:
validate-config
```

**Solutions:**
1. Check JSON syntax with online validator
2. Ensure proper quoting of strings
3. Validate file encoding (should be UTF-8)

#### Configuration Not Loading

**Symptoms:**
- Settings not applied
- Default values used instead

**Diagnosis:**
```bash
# Check if config file exists and is readable
ls -la config.json

# Start spoon-cli and run:
check-config
```

**Solutions:**
1. Ensure `config.json` is in current directory or `~/.spoon/config.json`
2. Check file permissions: `chmod 644 config.json`
3. Validate JSON format

### Environment Variable Issues

#### API Keys Not Recognized

**Symptoms:**
- LLM provider unavailable
- Authentication errors

**Diagnosis:**
```bash
# Check environment variables
env | grep -E "(OPENAI|ANTHROPIC|DEEPSEEK)"

# Test specific provider
 llm-status
```

**Solutions:**
1. Set environment variables correctly:
   ```bash
   export OPENAI_API_KEY="sk-your-key-here"
   export ANTHROPIC_API_KEY="sk-ant-your-key-here"
   ```

2. Use `.env` file for persistent configuration:
   ```bash
   echo "OPENAI_API_KEY=sk-your-key-here" > .env
   ```

3. Restart shell or reload environment

### Migration Issues

#### Legacy Configuration Detected

**Symptoms:**
- Migration warnings
- Some features not working

**Diagnosis:**
```bash
# Start spoon-cli and run:
check-config
```

**Solutions:**
```bash
# Preview migration
# Start spoon-cli and run:
migrate-config --dry-run

# Perform migration
# Start spoon-cli and run:
migrate-config

# Validate after migration
# Start spoon-cli and run:
validate-config
```

## Agent Loading Issues

### Agent Not Found

**Symptoms:**
- "Agent not found" error
- Cannot load specific agent

**Diagnosis:**
```bash
# Start spoon-cli and run:
# List available agents
list-agents

# Check configuration
config agents
```

**Solutions:**
1. Verify agent name in configuration
2. Check agent class name is correct
3. Ensure required dependencies are installed

### Tool Loading Failures

**Symptoms:**
- Tools not available after loading
- Agent lacks expected capabilities

**Diagnosis:**
```bash
# Start spoon-cli and run:
# Check tool loading status
tool-status

# List available toolkits
list-toolkit-categories
```

**Solutions:**
1. Reload agent configuration:
   ```bash
# Start spoon-cli and run:
reload-config
   ```

2. Reinstall toolkits:
   ```bash
   pip install --upgrade spoon-toolkits
   ```

3. Check network connectivity for external tools

## Network and Connectivity Issues

### MCP Server Connection Problems

**Symptoms:**
- MCP tools unavailable
- Server connection timeouts

**Diagnosis:**
```bash
# Start spoon-cli and run:
# Validate MCP configuration
validate-config --check-servers

# Test specific server
# Start spoon-cli and run:
load-agent mcp_agent
```

**Solutions:**
1. Verify server commands are installed:
   ```bash
   which npx
   which uvx
   ```

2. Check server environment variables
3. Update server configurations in `config.json`

### API Rate Limiting

**Symptoms:**
- Requests failing with rate limit errors
- Intermittent connectivity issues

**Solutions:**
1. Implement request throttling:
   ```json
   {
     "llm": {
       "rate_limiting": {
         "requests_per_minute": 30,
         "burst_limit": 5
       }
     }
   }
   ```

2. Switch to different provider temporarily
3. Upgrade API plan for higher limits

## Performance Issues

### Slow Startup

**Symptoms:**
- CLI takes long time to start
- Agent loading is slow

**Diagnosis:**
```bash
# Enable profiling
export SPOON_CLI_PROFILE=1
 system-info
```

**Solutions:**
1. Load fewer tools initially
2. Use faster LLM providers
3. Enable caching:
   ```json
   {
     "llm": {
       "caching": {
         "enabled": true,
         "ttl": 3600
       }
     }
   }
   ```

### High Memory Usage

**Symptoms:**
- System running out of memory
- Performance degradation

**Solutions:**
1. Reduce chat history size:
   ```json
   {
     "memory": {
       "max_chat_history": 50,
       "compress_old_messages": true
     }
   }
   ```

2. Enable memory cleanup:
   ```json
   {
     "memory": {
       "cleanup_temp_files": true,
       "auto_save_interval": 300
     }
   }
   ```

### Slow Response Times

**Diagnosis:**
```bash
# Start spoon-cli and run:
# Check LLM provider status
llm-status

# Test response time
time spoon-cli action chat <<< "Hello"
```

**Solutions:**
1. Switch to faster models
2. Adjust timeout settings:
   ```json
   {
     "llm": {
       "timeout": 60,
       "retry_attempts": 2
     }
   }
   ```

3. Enable response streaming for better perceived performance

## Installation Issues

### Import Errors

**Symptoms:**
- "Module not found" errors
- CLI fails to start

**Diagnosis:**
```bash
# Check Python path
python -c "import spoon_ai; print(spoon_ai.__file__)"

# Verify installation
pip list | grep spoon
```

**Solutions:**
1. Reinstall packages:
   ```bash
   pip uninstall  spoon-ai-sdk spoon-toolkits
   pip install    ```

2. Check Python version compatibility
3. Use virtual environment to avoid conflicts

### Permission Issues

**Symptoms:**
- Cannot write configuration files
- Installation fails

**Solutions:**
1. Use user installation:
   ```bash
   pip install --user    ```

2. Fix directory permissions:
   ```bash
   chmod 755 ~/.spoon/
   chmod 644 ~/.spoon/config.json
   ```

## Platform-Specific Issues

### Windows Issues

#### Command Prompt Problems

**Symptoms:**
- Interactive features not working
- Display issues

**Solutions:**
1. Use PowerShell instead of CMD
2. Install Windows Terminal
3. Enable ANSI color support

#### Path Issues

**Solutions:**
1. Add Python to PATH during installation
2. Use absolute paths in configuration
3. Avoid spaces in installation paths

### macOS Issues

#### SIP (System Integrity Protection)

**Solutions:**
1. Install in user directory:
   ```bash
   pip install --user    ```

2. Use Homebrew for Python:
   ```bash
   brew install python
   ```

### Linux Issues

#### Shared Library Problems

**Diagnosis:**
```bash
ldd $(which python) | grep "not found"
```

**Solutions:**
1. Install missing system libraries
2. Use system package manager:
   ```bash
   # Ubuntu/Debian
   sudo apt-get install python3-dev build-essential

   # CentOS/RHEL
   sudo yum install python3-devel gcc
   ```

## Debugging Techniques

### Enable Debug Mode

```bash
# Enable debug logging
export SPOON_CLI_DEBUG=1

# Or use flag
 --debug system-info
```

### Log Analysis

```bash
# Check log files
tail -f spoon_cli.log

# Increase log verbosity
export SPOON_CLI_LOG_LEVEL=DEBUG
```

### Network Debugging

```bash
# Test network connectivity
curl -I https://api.openai.com/v1/models

# Check proxy settings
env | grep -i proxy
```

### Configuration Debugging

```bash
# Show full configuration with debug
 --debug config

# Validate with detailed output
 validate-config --verbose
```

## Common Error Messages

### "Configuration validation failed"

**Cause:** Invalid configuration syntax or missing required fields

**Solution:**
```bash
 validate-config
# Fix reported issues
 reload-config
```

### "Agent class not found"

**Cause:** Invalid agent class name in configuration

**Solution:**
- Check class name spelling
- Ensure correct module imports
- Update to valid class names

### "Tool loading failed"

**Cause:** Tool dependencies not available

**Solution:**
```bash
pip install --upgrade spoon-toolkits
 reload-config
```

### "MCP server connection timeout"

**Cause:** Network issues or server configuration problems

**Solution:**
```bash
 validate-config --check-servers
# Fix server configurations
 reload-config
```

## Getting Help

### Community Resources

1. **GitHub Issues:** Report bugs and request features
2. **Documentation:** Check the full documentation
3. **Discord/Forum:** Community discussions

### Debug Information Collection

When reporting issues, include:

```bash
# System information
 system-info

# Configuration (mask sensitive data)
 config

# Error logs
tail -n 50 spoon_cli.log

# Python environment
python --version
pip list | grep spoon
```

## Preventive Maintenance

### Regular Updates

```bash
# Update CLI and dependencies
pip install --upgrade  spoon-ai-sdk spoon-toolkits

# Update configuration if needed
 migrate-config
```

### Configuration Backup

```bash
# Backup configuration
cp config.json config.json.backup

# Backup environment
env | grep SPOON > spoon_env.backup
```

### Health Monitoring

```bash
# Regular health checks
 system-info

# Monitor for issues
 validate-config
```

## Emergency Recovery

### Reset Configuration

```bash
# Remove corrupted config
rm config.json

# Start with minimal config
echo '{"default_agent": "react"}' > config.json

# Test basic functionality
 list-agents
```

### Clean Reinstall

```bash
# Complete cleanup
pip uninstall  spoon-ai-sdk spoon-toolkits
rm -rf ~/.spoon/
rm -rf chat_logs/

# Fresh install
pip install
# Basic setup
 config api_key openai "your-key"
```

## Next Steps

- [Installation](./installation.md) - Reinstall if needed
- [Configuration](./configuration.md) - Review configuration options
- [Basic Usage](./basic-usage.md) - Verify basic functionality works

---

FILE: docs/core-concepts/agents-detailed.md

---
sidebar_position: 5
---

# Agents

This guide provides a comprehensive walkthrough for developing and configuring agents in the SpoonOS Core Developer Framework (SCDF). We will use practical examples to illustrate key concepts, including agent definition, tool integration, and execution.

Agents are the core building blocks of SpoonOS. They combine language models with tools to create intelligent, autonomous systems that can reason, plan, and take actions.

## What is an Agent?

An agent in SpoonOS is an intelligent entity that:

- **Reasons** about problems and situations
- **Plans** sequences of actions to achieve goals
- **Executes** actions using available tools
- **Learns** from interactions and feedback
- **Adapts** to new situations and requirements

## Agent Architecture

SpoonOS agents follow the **ReAct (Reasoning + Acting)** pattern:

```
Thought â†’ Action â†’ Observation â†’ Thought â†’ Action â†’ ...
```

### Core Components

#### Built-in Agent Classes

SpoonOS provides two main agent classes:

- **`SpoonReactAI`**: Standard agent for blockchain operations
- **`SpoonReactMCP`**: Enhanced agent with MCP protocol support

#### Tool Integration

Agents can use various types of tools:

- **Built-in Tools**: Python classes integrated directly into the framework
- **MCP Tools**: External tools accessed via Model Context Protocol
- **Custom Tools**: User-defined tools for specific functionality

### Technical Implementation

#### Creating Custom Agents

```python
from spoon_ai.agents import SpoonReactMCP
from spoon_ai.tools import ToolManager

class SpoonMacroAnalysisAgent(SpoonReactMCP):
    name: str = "SpoonMacroAnalysisAgent"
    system_prompt: str = (
        '''You are a cryptocurrency market analyst. Your task is to provide a comprehensive
        macroeconomic analysis of a given token.

        To do this, you will perform the following steps:
        1. Use the `crypto_power_data_cex` tool to get the latest candlestick data and
           technical indicators.
        2. Use the `tavily-search` tool to find the latest news and market sentiment.
        3. Synthesize the data from both tools to form a holistic analysis.'''
    )

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.available_tools = ToolManager([])
```

#### MCP Tool Configuration

**Stdio-based Transport:**
```python
from spoon_ai.tools import MCPTool

tavily_tool = MCPTool(
    name="tavily-search",
    description="Performs a web search using the Tavily API.",
    mcp_config={
        "command": "npx",
        "args": ["--yes", "tavily-mcp"],
        "env": {"TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")}
    }
)
```

**HTTP Transport:**
```python
firecrawl_tool = MCPTool(
    name="firecrawl-scraper",
    description="Advanced web scraping tool",
    mcp_config={
        "url": "https://mcp.firecrawl.dev/scrape",
        "headers": {
            "Authorization": f"Bearer {os.getenv('FIRECRAWL_API_KEY')}",
            "Content-Type": "application/json"
        }
    }
)
```

1. **LLM (Language Model)** - The "brain" that provides reasoning capabilities
2. **Tools** - External capabilities the agent can use
3. **Memory** - Context and conversation history
4. **System Prompt** - Instructions that define the agent's behavior

#### Built-in Tools Integration

```python
from spoon_toolkits.crypto.crypto_powerdata.tools import CryptoPowerDataCEXTool
from spoon_toolkits.crypto.crypto_data_tools.price_data import GetTokenPriceTool

# Initialize built-in tools
crypto_tool = CryptoPowerDataCEXTool(
    exchange="binance",
    symbol="BTC/USDT",
    timeframe="1h",
    limit=100
)

price_tool = GetTokenPriceTool(
    exchange="uniswap"
)

# Add tools to agent
agent.available_tools.add_tools(crypto_tool, price_tool)
```
5. **Execution Loop** - The ReAct cycle that drives agent behavior

## Agent Types

### 1. SpoonReactAI

The standard ReAct agent for tool-enabled interactions:

```python
from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot
from spoon_ai.tools import ToolManager

class MyAgent(SpoonReactAI):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Configure LLM
        self.llm = ChatBot(model_name="gpt-4.1")

        # Set system prompt
        self.system_prompt = "You are a helpful AI assistant."

        # Configure execution parameters
        self.max_steps = 10

        # Set up tools (if any)
        self.available_tools = ToolManager([])
```

### 2. SpoonReactMCP

MCP-enabled agent with dynamic tool loading:

```python
from spoon_ai.agents import SpoonReactMCP
from spoon_ai.chat import ChatBot

# Agent with MCP tool support
agent = SpoonReactMCP(
    llm=ChatBot(),
    system_prompt="You are an expert research assistant.",
    max_steps=15
)

# When using the CLI (main.py / spoon-cli), agents and MCP tools are defined in the CLI's `config.json` file.
# When using the SDK directly (as in this example), configure tools and agents in Python code instead of reading config.json.
```

## Creating Custom Agents

### Basic Custom Agent

```python
from spoon_ai.agents import SpoonReactAI
from spoon_ai.tools.base import BaseTool
from spoon_ai.tools import ToolManager
from spoon_ai.chat import ChatBot

class WeatherTool(BaseTool):
    name: str = "get_weather"
    description: str = "Get current weather for a location"
    parameters: dict = {
        "type": "object",
        "properties": {
            "location": {"type": "string", "description": "City name"}
        },
        "required": ["location"]
    }

    async def execute(self, location: str) -> str:
        # Implement weather API call
        return f"Weather in {location}: Sunny, 72Â°F"

class WeatherAgent(SpoonReactAI):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Configure LLM
        self.llm = ChatBot()

        # Set system prompt
        self.system_prompt = """
        You are a weather assistant. Use the get_weather tool to provide
        accurate weather information for any location the user asks about.
        """

        # Set up tools
        weather_tool = WeatherTool()
        self.available_tools = ToolManager([weather_tool])
```

### Advanced Agent with Multiple Tools

```python
from spoon_ai.agents import SpoonReactAI
from spoon_ai.tools import ToolManager
from spoon_ai.tools.mcp_tool import MCPTool
from spoon_ai.chat import ChatBot
import os

class ResearchAgent(SpoonReactAI):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Configure LLM
        self.llm = ChatBot(model_name="gpt-4.1")

        # Set up MCP tools
        tools = []

        # Web search tool (requires TAVILY_API_KEY)
        if os.getenv("TAVILY_API_KEY"):
            search_tool = MCPTool(
                name="web_search",
                description="Search the web for current information",
                mcp_config={
                    "command": "npx",
                    "args": ["-y", "tavily-mcp"],
                    "env": {"TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")},
                    "transport": "stdio"
                }
            )
            tools.append(search_tool)

        self.available_tools = ToolManager(tools)
        self.system_prompt = """
        You are a research assistant with access to web search tools.

        When asked to research a topic:
        1. Search for current information using available tools
        2. Analyze and synthesize findings
        3. Provide well-structured summaries
        4. Always cite your sources

        Be thorough and accurate in your research.
        """
```

## Agent Configuration

### System Prompts

System prompts define your agent's personality and behavior:

```python
# Task-specific prompt
system_prompt = """
You are a crypto trading assistant. Your role is to:
- Analyze market data and trends
- Provide trading insights and recommendations
- Help users understand market conditions
- Always include risk warnings in trading advice
"""

# Personality-driven prompt
system_prompt = """
You are a friendly and enthusiastic coding mentor. You:
- Explain concepts clearly with examples
- Encourage learning and experimentation
- Provide constructive feedback
- Use emojis and positive language
"""
```

### Agent Parameters

```python
from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot

class ConfigurableAgent(SpoonReactAI):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Configure LLM with specific parameters
        self.llm = ChatBot(
            model_name="gpt-4.1",
            temperature=0.7,        # Creativity level
            max_tokens=4096,        # Response length limit
            timeout=60              # Request timeout
        )

        # Execution settings
        self.max_steps = 20         # Maximum reasoning steps
        self.verbose = True         # Show reasoning steps

        # Agent behavior settings
        self.system_prompt = "You are a configurable AI assistant."
```

## Agent Lifecycle

### 1. Initialization

```python
from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot

# Create agent instance
agent = SpoonReactAI(
    llm=ChatBot(model_name="gpt-4.1"),
    system_prompt="You are a helpful assistant.",
    max_steps=15
)
```

### 2. Execution

```python
# Single interaction
result = await agent.run("What's the weather in New York?")
print(result)

# Multiple interactions
questions = [
    "What is SpoonOS?",
    "How do I create an agent?",
    "What tools are available?"
]

for question in questions:
    response = await agent.run(question)
    print(f"Q: {question}")
    print(f"A: {response}
")
```

### 3. Configuration Management

```python
# Access agent configuration
print(f"Max steps: {agent.max_steps}")
print(f"System prompt: {agent.system_prompt}")

# Modify configuration
agent.max_steps = 20
agent.system_prompt = "You are an expert assistant."

# Check available tools
if hasattr(agent, 'available_tools'):
    tools = agent.available_tools.list_tools()
    print(f"Available tools: {tools}")
```

## Best Practices

### 1. Clear System Prompts

```python
# Good: Specific and actionable
system_prompt = """
You are a code review assistant. For each code submission:
1. Check for syntax errors and bugs
2. Suggest improvements for readability
3. Identify security vulnerabilities
4. Recommend best practices
5. Provide specific, actionable feedback
"""

# Avoid: Vague or generic
system_prompt = "You are a helpful assistant."
```

### 2. Appropriate Tool Selection

```python
# Match tools to agent purpose
class DataAnalysisAgent(ToolCallAgent):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Data-focused tools
        tools = [
            PandasTool(),      # Data manipulation
            PlotlyTool(),      # Visualization
            StatsTool(),       # Statistical analysis
            DatabaseTool(),    # Data access
        ]

        self.available_tools = ToolManager(tools)
```

### 3. Error Handling

```python
class RobustAgent(ToolCallAgent):
    async def run(self, user_input: str) -> str:
        try:
            return await super().run(user_input)
        except Exception as e:
            # Log error and provide graceful fallback
            self.logger.error(f"Agent execution failed: {e}")
            return "I encountered an error. Please try rephrasing your request."
```

### 4. Performance Optimization

```python
# Use appropriate model for task complexity
simple_agent = ToolCallAgent(
    llm=ChatBot(model_name="gpt-3.5-turbo")  # Faster, cheaper
)

complex_agent = ToolCallAgent(
    llm=ChatBot(model_name="gpt-4.1")        # More capable
)

# Set reasonable limits
agent.max_steps = 10        # Prevent infinite loops
agent.timeout = 60.0        # Prevent hanging
```

## Testing Agents

### Unit Testing

```python
import pytest
from unittest.mock import AsyncMock

@pytest.mark.asyncio
async def test_weather_agent():
    # Mock the LLM
    mock_llm = AsyncMock()
    mock_llm.chat.return_value = "The weather in Paris is sunny."

    # Create agent with mock
    agent = WeatherAgent(llm=mock_llm)

    # Test execution
    result = await agent.run("What's the weather in Paris?")
    assert "sunny" in result.lower()
```

### Integration Testing

```python
@pytest.mark.asyncio
async def test_agent_with_real_llm():
    # Test with actual LLM (requires API key)
    agent = WeatherAgent(llm=ChatBot())

    result = await agent.run("What's the weather in London?")
    assert len(result) > 0
    assert "weather" in result.lower()
```

## Monitoring and Debugging

### Logging

```python
import logging

# Enable agent logging
logging.basicConfig(level=logging.DEBUG)

# Agent will log reasoning steps
agent = MyAgent(llm=ChatBot(), verbose=True)
```

### Performance Metrics

```python
import time

start_time = time.time()
result = await agent.run("Complex query here")
execution_time = time.time() - start_time

print(f"Agent executed in {execution_time:.2f} seconds")
print(f"Used {agent.step_count} reasoning steps")
```

## Advanced Features

### MCP Tool Integration

#### Single MCP Tool Integration

```python
from spoon_ai.agents import SpoonReactMCP
from spoon_ai.tools.mcp_tool import MCPTool
from spoon_ai.tools import ToolManager
from spoon_ai.chat import ChatBot
import os

class MCPEnabledAgent(SpoonReactMCP):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Configure LLM
        self.llm = ChatBot(model_name="gpt-4.1")

        # Configure stdio MCP tool
        search_tool = MCPTool(
            name="tavily_search",
            description="Advanced web search capabilities",
            mcp_config={
                "command": "npx",
                "args": ["-y", "tavily-mcp"],
                "env": {"TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")},
                "timeout": 30,
                "retry_attempts": 3
            }
        )

        # Configure HTTP MCP tool
        context7_tool = MCPTool(
            name="context7_docs",
            description="Access Context7 documentation and libraries",
            mcp_config={
                "url": "https://mcp.context7.com/mcp",
                "timeout": 30,
                "retry_attempts": 2,
                "headers": {
                    "User-Agent": "SpoonOS-Agent/1.0"
                }
            }
        )

        # Create tool manager
        self.available_tools = ToolManager([search_tool, context7_tool])

        self.system_prompt = """
        You are a research assistant with access to multiple MCP tools:
        - tavily_search: Web search functionality
        - context7_docs: Access Context7 documentation and library information

        When using tools:
        1. Choose the appropriate tool for the task
        2. Clearly define search objectives and keywords
        3. Analyze the reliability of search results
        4. Provide comprehensive analysis reports
        """

# Usage example
async def use_single_mcp():
    agent = MCPEnabledAgent()
    result = await agent.run("Search for the latest developments in SpoonOS framework and review related documentation")
    return result
```

#### Comprehensive MCP Tool Integration

```python
from spoon_ai.agents import SpoonReactMCP
from spoon_ai.tools.mcp_tool import MCPTool
from spoon_ai.tools import ToolManager

class ComprehensiveMCPAgent(SpoonReactMCP):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Configure multiple MCP tools
        mcp_tools = []

        # Web search tool
        if os.getenv("TAVILY_API_KEY"):
            mcp_tools.append(MCPTool(
                name="web_search",
                description="Web search and information gathering",
                mcp_config={
                    "command": "npx",
                    "args": ["-y", "tavily-mcp"],
                    "env": {"TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")}
                }
            ))

        # GitHub tools
        if os.getenv("GITHUB_TOKEN"):
            mcp_tools.append(MCPTool(
                name="github_management",
                description="GitHub repository and code management",
                mcp_config={
                    "command": "npx",
                    "args": ["-y", "@modelcontextprotocol/server-github"],
                    "env": {"GITHUB_TOKEN": os.getenv("GITHUB_TOKEN")},
                    "transport": "stdio"
                }
            ))

        # Filesystem tools
        mcp_tools.append(MCPTool(
            name="file_operations",
            description="File and directory operations",
            mcp_config={
                "command": "npx",
                "args": ["-y", "@modelcontextprotocol/server-filesystem"],
                "transport": "stdio"
            }
        ))

        # Database tools
        mcp_tools.append(MCPTool(
            name="database_operations",
            description="SQLite database operations",
            mcp_config={
                "command": "npx",
                "args": ["-y", "@modelcontextprotocol/server-sqlite"],
                "env": {"DATABASE_PATH": "./agent_data.db"},
                "transport": "stdio"
            }
        ))

        # HTTP MCP tool - Context7 documentation service
        mcp_tools.append(MCPTool(
            name="context7_docs",
            description="Context7 documentation and library information access",
            mcp_config={
                "url": "https://mcp.context7.com/mcp",
                "transport": "http",
                "timeout": 30,
                "retry_attempts": 2,
                "headers": {
                    "User-Agent": "SpoonOS-ComprehensiveAgent/1.0"
                }
            }
        ))

        # SSE MCP tool - Firecrawl web scraping service
        if os.getenv("FIRECRAWL_API_KEY"):
            mcp_tools.append(MCPTool(
                name="firecrawl_scraper",
                description="Advanced web scraping and content extraction service",
                mcp_config={
                    "url": f"https://mcp.firecrawl.dev/{os.getenv('FIRECRAWL_API_KEY')}/sse",
                    "transport": "sse",
                    "timeout": 60,
                    "retry_attempts": 3,
                    "reconnect_interval": 5,
                    "max_reconnect_attempts": 10,
                    "headers": {
                        "Accept": "text/event-stream",
                        "User-Agent": "SpoonOS-ComprehensiveAgent/1.0",
                        "Cache-Control": "no-cache"
                    }
                }
            ))

        # Create tool manager
        self.available_tools = ToolManager(mcp_tools)

        self.system_prompt = """
        You are a comprehensive AI assistant with the following MCP tools:

        1. web_search: Search for latest information and resources
        2. github_management: Manage GitHub repositories and code
        3. file_operations: Handle files and directories
        4. database_operations: Operate SQLite databases
        5. context7_docs: Access Context7 documentation and library information
        6. firecrawl_scraper: Advanced web scraping and content extraction service

        Workflow:
        1. Analyze user requirements and determine which tools to use
        2. Use multiple tools in logical sequence to complete tasks
        3. Integrate results from all tools to provide comprehensive solutions

        Always ensure operation safety and accuracy.
        """

# Usage example
async def use_comprehensive_mcp():
    agent = ComprehensiveMCPAgent(llm=ChatBot())

    task = """
    Please help me complete a project analysis task:
    1. Search for the latest trends in 'AI agent frameworks'
    2. Review Context7 related documentation and library information
    3. Create project directory structure locally
    4. Create database to store analysis results
    5. If possible, review related GitHub projects
    """

    result = await agent.run(task)
    return result
```

### Advanced Parameter Configuration

#### Detailed Agent Parameter Configuration

```python
class AdvancedConfigAgent(SpoonReactMCP):
    def __init__(self, **kwargs):
        # Advanced LLM configuration
        llm_config = {
            "model_name": "gpt-4.1",
            "temperature": 0.3,          # Creativity control
            "max_tokens": 4096,          # Maximum response length
            "top_p": 0.9,               # Nucleus sampling parameter
            "frequency_penalty": 0.1,    # Frequency penalty
            "presence_penalty": 0.1,     # Presence penalty
            "timeout": 60,              # Request timeout
            "retry_attempts": 3,         # Retry attempts
            "stream": False             # Whether to stream responses
        }

        super().__init__(
            llm=ChatBot(**llm_config),
            **kwargs
        )

        # Agent execution parameters
        self.max_iterations = 20        # Maximum reasoning steps
        self.max_execution_time = 300   # Maximum execution time (seconds)
        self.enable_memory = True       # Enable conversation memory
        self.memory_window = 10         # Memory window size
        self.enable_reflection = True   # Enable reflection mechanism
        self.reflection_threshold = 5   # Reflection trigger threshold

        # Tool execution parameters
        self.tool_timeout = 30          # Tool execution timeout
        self.tool_retry_attempts = 2    # Tool retry attempts
        self.parallel_tool_execution = False  # Parallel tool execution
        self.tool_error_handling = "graceful"  # Tool error handling strategy

        # Security and limitation parameters
        self.enable_safety_checks = True      # Enable security checks
        self.max_tool_calls_per_iteration = 3 # Maximum tool calls per iteration
        self.restricted_operations = [        # Restricted operations
            "file_delete",
            "system_command",
            "network_access"
        ]

        self.system_prompt = """
        You are an advanced AI agent with the following configuration:

        Execution parameters:
        - Maximum reasoning steps: {max_iterations}
        - Memory enabled: {enable_memory}
        - Reflection enabled: {enable_reflection}

        Security settings:
        - Security checks: Enabled
        - Operation restrictions: Configured
        - Error handling: Graceful mode

        Please complete tasks efficiently within these parameter constraints.
        """.format(
            max_iterations=self.max_iterations,
            enable_memory=self.enable_memory,
            enable_reflection=self.enable_reflection
        )

# Dynamic parameter adjustment
class DynamicConfigAgent(SpoonReactMCP):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.performance_metrics = {
            "success_rate": 0.0,
            "avg_execution_time": 0.0,
            "tool_usage_efficiency": 0.0
        }

    async def run(self, user_input: str) -> str:
        """Execution with dynamic parameter adjustment"""
        # Adjust parameters based on historical performance
        self._adjust_parameters()

        # Execute task
        start_time = time.time()
        try:
            result = await super().run(user_input)
            success = True
        except Exception as e:
            result = f"Execution failed: {str(e)}"
            success = False

        # Update performance metrics
        execution_time = time.time() - start_time
        self._update_metrics(success, execution_time)

        return result

    def _adjust_parameters(self):
        """Adjust parameters based on performance metrics"""
        if self.performance_metrics["success_rate"] < 0.8:
            # Low success rate, increase retry attempts and timeout
            self.max_iterations = min(self.max_iterations + 2, 25)
            self.tool_timeout = min(self.tool_timeout + 10, 60)

        if self.performance_metrics["avg_execution_time"] > 120:
            # Execution time too long, reduce maximum steps
            self.max_iterations = max(self.max_iterations - 1, 10)

        if self.performance_metrics["tool_usage_efficiency"] < 0.6:
            # Low tool usage efficiency, enable parallel execution
            self.parallel_tool_execution = True

    def _update_metrics(self, success: bool, execution_time: float):
        """Update performance metrics"""
        # Simplified metric update logic
        alpha = 0.1  # Learning rate

        self.performance_metrics["success_rate"] = (
            (1 - alpha) * self.performance_metrics["success_rate"] +
            alpha * (1.0 if success else 0.0)
        )

        self.performance_metrics["avg_execution_time"] = (
            (1 - alpha) * self.performance_metrics["avg_execution_time"] +
            alpha * execution_time
        )
```

### Advanced Configuration File Settings

```json
{
  "agents": {
    "advanced_mcp_agent": {
      "class": "SpoonReactMCP",
      "description": "Advanced MCP agent configuration example",
      "config": {
        "max_steps": 20,
        "temperature": 0.3,
        "max_execution_time": 300,
        "enable_memory": true,
        "memory_window": 10,
        "enable_reflection": true,
        "reflection_threshold": 5,
        "safety_checks": true,
        "parallel_tool_execution": false,
        "tool_timeout": 30,
        "tool_retry_attempts": 2,
        "max_tool_calls_per_iteration": 3,
        "system_prompt": "You are an advanced AI agent..."
      },
      "llm_config": {
        "model_name": "gpt-4.1",
        "temperature": 0.3,
        "max_tokens": 4096,
        "top_p": 0.9,
        "frequency_penalty": 0.1,
        "presence_penalty": 0.1,
        "timeout": 60,
        "retry_attempts": 3
      },
      "tools": [
        {
          "name": "tavily_search",
          "type": "mcp",
          "enabled": true,
          "priority": "high",
          "config": {
            "timeout": 30,
            "retry_attempts": 3,
            "cache_duration": 300,
            "rate_limit": 100
          },
          "mcp_server": {
            "command": "npx",
            "args": ["-y", "tavily-mcp"],
            "env": {"TAVILY_API_KEY": "your-tavily-key"},
            "transport": "stdio",
            "health_check_interval": 60,
            "auto_restart": true
          }
        },
        {
          "name": "github_tools",
          "type": "mcp",
          "enabled": true,
          "priority": "medium",
          "config": {
            "timeout": 45,
            "retry_attempts": 2,
            "default_branch": "main",
            "max_file_size": "10MB"
          },
          "mcp_server": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-github"],
            "env": {"GITHUB_TOKEN": "your-github-token"},
            "transport": "stdio"
          }
        },
        {
          "name": "context7_docs",
          "type": "mcp",
          "enabled": true,
          "priority": "medium",
          "config": {
            "timeout": 30,
            "retry_attempts": 2,
            "rate_limit": 50,
            "cache_duration": 600
          },
          "mcp_server": {
            "url": "https://mcp.context7.com/mcp",
            "transport": "http",
            "headers": {
              "User-Agent": "SpoonOS-Agent/1.0"
            }
          }
        },
        {
          "name": "firecrawl_scraper",
          "type": "mcp",
          "enabled": true,
          "priority": "high",
          "config": {
            "timeout": 60,
            "retry_attempts": 3,
            "reconnect_interval": 5,
            "max_reconnect_attempts": 10,
            "rate_limit": 30,
            "cache_duration": 300
          },
          "mcp_server": {
            "url": "https://mcp.firecrawl.dev/{FIRECRAWL_API_KEY}/sse",
            "transport": "sse",
            "headers": {
              "Accept": "text/event-stream",
              "User-Agent": "SpoonOS-Agent/1.0",
              "Cache-Control": "no-cache"
            }
          }
        },
        {
          "name": "crypto_powerdata_cex",
          "type": "builtin",
          "enabled": true,
          "priority": "high",
          "config": {
            "timeout": 30,
            "max_retries": 3,
            "cache_duration": 300,
            "rate_limit": 200,
            "default_currency": "USD",
            "precision": 8
          },
          "env": {
            "OKX_API_KEY": "your_okx_api_key",
            "OKX_SECRET_KEY": "your_okx_secret_key",
            "OKX_API_PASSPHRASE": "your_okx_passphrase"
          }
        }
      ],
      "monitoring": {
        "enable_metrics": true,
        "log_level": "INFO",
        "performance_tracking": true,
        "error_reporting": true,
        "health_checks": true
      },
      "security": {
        "restricted_operations": [
          "file_delete",
          "system_command",
          "network_access"
        ],
        "api_rate_limits": {
          "requests_per_minute": 100,
          "tokens_per_hour": 50000
        },
        "data_privacy": {
          "log_user_inputs": false,
          "encrypt_sensitive_data": true,
          "data_retention_days": 30
        }
      }
    }
  }
}
```

## Next Steps

### ðŸ“š **Agent Implementation Examples**

#### ðŸŽ¯ [Intent Graph Demo](../examples/intent-graph-demo.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py)

**Advanced agent patterns demonstrated:**
- Long-lived agent architecture with persistent memory
- Intelligent routing and decision-making workflows
- Advanced state management and context preservation
- Production-ready error handling and monitoring

#### ðŸ” [MCP Spoon Search Agent](../examples/mcp-spoon-search-agent.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

**MCP integration patterns:**
- Dynamic tool discovery and loading
- Multi-tool orchestration and coordination
- Real-time web search integration
- Advanced error handling for distributed systems

#### ðŸ“Š [Graph Crypto Analysis](../examples/graph-crypto-analysis.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py)

**Domain-specific agent development:**
- Cryptocurrency market analysis workflows
- Real-time data processing and technical indicators
- LLM-driven investment recommendations
- Financial data validation and error handling

### ðŸ› ï¸ **Development Resources**

- **[Tools System](./tools.md)** - Complete tool integration guide
- **[Custom Tools](../how-to-guides/add-custom-tools.md)** - Build specialized tools
- **[MCP Protocol](./mcp-protocol.md)** - Dynamic tool loading and execution
- **[Graph System](./graph-system.md)** - Advanced workflow orchestration

### ðŸ“– **Additional Documentation**

- **[API Reference](../api-reference/agents/base-agent.md)** - Complete agent API documentation
- **[Performance Optimization](../troubleshooting/performance.md)** - Agent performance tuning
- **[Troubleshooting](../troubleshooting/common-issues.md)** - Common issues and solutions

Ready to build more sophisticated agents? Check out the [Tools](./tools.md) documentation! ðŸ› ï¸

---

FILE: docs/core-concepts/agents.md

# Agents

Agents are the core intelligence layer of SpoonOS, providing reasoning, planning, and action execution capabilities.

## What are Agents?

An agent is an AI system that can:

- **Reason** about problems and goals
- **Plan** sequences of actions to achieve objectives
- **Execute** actions using available tools
- **Learn** from feedback and adapt behavior

## Agent Types

### ReAct Agents

ReAct (Reasoning + Acting) agents follow a thought-action-observation loop:

```python
from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot

# Create a ReAct agent
agent = SpoonReactAI(
    llm=ChatBot(model_name="gpt-4.1", llm_provider="openai")
)

# Run the agent
response = await agent.run("Get the current Bitcoin price")
```

**Advantages:**

- Simple and intuitive reasoning process
- Good for straightforward tasks
- Lower computational overhead

**Best for:**

- Single-step tasks
- Direct API calls
- Simple automation

### Graph Agents

Graph agents use structured workflows with nodes and edges:

```python
from spoon_ai.agents import GraphAgent
from spoon_ai.graph import create_graph

# Create a graph workflow
graph = create_graph()
agent = GraphAgent(graph=graph)

# Execute complex workflow
result = await agent.execute("Analyze market trends and execute trades")
```

**Advantages:**

- Complex multi-step workflows
- Parallel execution capabilities
- Better error handling and recovery

**Best for:**

- Multi-step processes
- Conditional logic
- Parallel operations

## Agent Architecture

### Core Components

1. **LLM Provider** - The language model powering the agent
2. **Tool Manager** - Manages available tools and execution
3. **Memory System** - Stores conversation history and context
4. **Prompt System** - Handles system prompts and instructions

### Agent Lifecycle

```mermaid
graph TD
    A[Initialize Agent] --> B[Load Tools]
    B --> C[Receive Input]
    C --> D[Reason About Task]
    D --> E[Select Action]
    E --> F[Execute Tool]
    F --> G[Observe Result]
    G --> H{Task Complete?}
    H -->|No| D
    H -->|Yes| I[Return Response]
```

## Creating Custom Agents

### Basic Agent Setup

```python
from spoon_ai.agents.base import BaseAgent
from spoon_ai.tools import ToolManager

class CustomAgent(BaseAgent):
    def __init__(self, llm, tools=None):
        super().__init__(llm)
        self.tool_manager = ToolManager(tools or [])

    async def run(self, message: str) -> str:
        # Custom agent logic here
        return await self.process_message(message)
```

### Agent Configuration

```python
# Configure agent with specific tools
from spoon_ai.tools.crypto_tools import CryptoTools
from spoon_ai.tools.web3_tools import Web3Tools

agent = SpoonReactAI(
    llm=ChatBot(model_name="gpt-4.1", llm_provider="openai"),
    tools=[CryptoTools(), Web3Tools()]
)
```

## Best Practices

### Tool Selection
- Choose tools that match your use case
- Avoid tool overload - too many tools can confuse the agent
- Test tool combinations thoroughly

### Prompt Engineering
- Provide clear, specific instructions
- Include examples of desired behavior
- Set appropriate constraints and guidelines

### Error Handling

- Leverage framework's automatic retry mechanisms
- Use built-in fallback strategies
- Rely on framework's structured error handling

### Framework Error Handling

SpoonOS agents benefit from built-in error resilience:

```python
# Framework handles errors automatically
agent = SpoonReactAI(
    llm=ChatBot(model_name="gpt-4.1", llm_provider="openai"),
    tools=[CryptoTools(), Web3Tools()]
)

# Automatic handling includes:
# - LLM provider failures with fallback
# - Tool execution errors with retry
# - Network issues with graceful degradation
response = await agent.run("Get Bitcoin price and analyze trends")
```

## Performance Considerations

### Memory Usage
- ReAct agents: Lower memory footprint
- Graph agents: Higher memory for complex workflows

### Execution Speed
- Simple tasks: ReAct agents are faster
- Complex workflows: Graph agents are more efficient

### Scalability
- ReAct: Better for high-frequency, simple tasks
- Graph: Better for complex, long-running processes

## Next Steps

### ðŸ“š **Agent Implementation Examples**

#### ðŸŽ¯ [Intent Graph Demo](../examples/intent-graph-demo.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py)

**What it demonstrates:**
- Complete Graph agent implementation with intelligent routing
- Long-lived agent architecture with persistent memory
- Advanced state management and context preservation
- Production-ready error handling and recovery

**Key features:**
- Dynamic query routing based on user intent (general_qa â†’ short_term_trend â†’ macro_trend â†’ deep_research)
- True parallel execution across multiple data sources
- Memory persistence and conversation context
- Real-time performance monitoring and metrics

**Best for learning:**
- Graph agent architecture patterns
- Long-running process management
- Advanced memory and state handling
- Production deployment considerations

#### ðŸ” [MCP Spoon Search Agent](../examples/mcp-spoon-search-agent.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

**What it demonstrates:**
- MCP-enabled agent with dynamic tool discovery
- Web search integration with cryptocurrency analysis
- Multi-tool orchestration and data synthesis
- Real-world agent deployment patterns

**Key features:**
- Tavily MCP server integration for web search
- Crypto PowerData tools for market analysis
- Unified analysis combining multiple data sources
- Dynamic tool loading and validation

**Best for learning:**
- MCP protocol implementation
- Multi-tool agent architecture
- Real-time data integration patterns
- Error handling in distributed systems

#### ðŸ“Š [Graph Crypto Analysis](../examples/graph-crypto-analysis.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py)

**What it demonstrates:**
- Specialized cryptocurrency analysis agent
- LLM-driven decision making throughout the workflow
- Real-time market data processing and analysis
- Investment recommendation generation

**Key features:**
- Real Binance API integration (no simulated data)
- Technical indicator calculation (RSI, MACD, EMA, Bollinger Bands)
- Multi-timeframe analysis and correlation
- Risk assessment and market sentiment analysis

**Best for learning:**
- Domain-specific agent development
- Financial data processing patterns
- LLM-driven workflow automation
- Real API integration in agents

### ðŸ› ï¸ **Development Guides**

- **[Tools System](./tools.md)** - Complete guide to available tools and integrations
- **[LLM Providers](./llm-providers.md)** - Configure and optimize language models
- **[Build Your First Agent](../how-to-guides/build-first-agent.md)** - Step-by-step agent development tutorial

### ðŸ“– **Advanced Topics**

- **[Graph System](../core-concepts/graph-system.md)** - Advanced workflow orchestration
- **[MCP Protocol](../core-concepts/mcp-protocol.md)** - Dynamic tool discovery and execution
- **[API Reference](../api-reference/agents/base-agent.md)** - Complete agent API documentation
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py)

**What it demonstrates:**
- Complete Graph agent implementation with intelligent routing
- Long-lived agent architecture with persistent memory
- Advanced state management and context preservation
- Production-ready error handling and recovery

**Key features:**
- Dynamic query routing based on user intent (general_qa â†’ short_term_trend â†’ macro_trend â†’ deep_research)
- True parallel execution across multiple data sources
- Memory persistence and conversation context
- Real-time performance monitoring and metrics

**Best for learning:**
- Graph agent architecture patterns
- Long-running process management
- Advanced memory and state handling
- Production deployment considerations

#### ðŸ” [MCP Spoon Search Agent](../examples/mcp-spoon-search-agent.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

**What it demonstrates:**
- MCP-enabled agent with dynamic tool discovery
- Web search integration with cryptocurrency analysis
- Multi-tool orchestration and data synthesis
- Real-world agent deployment patterns

**Key features:**
- Tavily MCP server integration for web search
- Crypto PowerData tools for market analysis
- Unified analysis combining multiple data sources
- Dynamic tool loading and validation

**Best for learning:**
- MCP protocol implementation
- Multi-tool agent architecture
- Real-time data integration patterns
- Error handling in distributed systems

#### ðŸ“Š [Graph Crypto Analysis](../examples/graph-crypto-analysis.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py)

**What it demonstrates:**
- Specialized cryptocurrency analysis agent
- LLM-driven decision making throughout the workflow
- Real-time market data processing and analysis
- Investment recommendation generation

**Key features:**
- Real Binance API integration (no simulated data)
- Technical indicator calculation (RSI, MACD, EMA, Bollinger Bands)
- Multi-timeframe analysis and correlation
- Risk assessment and market sentiment analysis

**Best for learning:**
- Domain-specific agent development
- Financial data processing patterns
- LLM-driven workflow automation
- Real API integration in agents

### ðŸ› ï¸ **Development Guides**

- **[Tools System](./tools.md)** - Complete guide to available tools and integrations
- **[LLM Providers](./llm-providers.md)** - Configure and optimize language models
- **[Build Your First Agent](../how-to-guides/build-first-agent.md)** - Step-by-step agent development tutorial

### ðŸ“– **Advanced Topics**

- **[Graph System](../core-concepts/graph-system.md)** - Advanced workflow orchestration
- **[MCP Protocol](../core-concepts/mcp-protocol.md)** - Dynamic tool discovery and execution
- **[API Reference](../api-reference/agents/base-agent.md)** - Complete agent API documentation

---

FILE: docs/core-concepts/graph-system.md

# Graph System

The Graph System in SpoonOS enables complex, multi-step workflows through a modern declarative execution engine that supports intelligent routing, parallel execution, and state management. Built around `StateGraph` with powerful declarative building tools.

## What you get

- **Declarative graph construction**: `GraphTemplate`, `NodeSpec`, and `EdgeSpec` for modular workflows
- **High-level API integration**: `HighLevelGraphAPI` for automatic parameter inference and intent analysis
- **Intelligent routing**: LLM router, rule-based, and conditional functions with priority systems
- **Advanced parallel execution**: concurrent branches with join strategies, timeouts, and retry policies
- **Type-safe state management**: Pydantic-based configuration and reducer-based merging
- **Memory integration**: persistent context across runs with automatic memory updates

---

## Declarative Graph Building (Recommended)

The modern approach uses `GraphTemplate` for declarative construction, making graphs more maintainable and reusable.

```python
from typing import TypedDict, Dict, Any, Optional, Annotated
from spoon_ai.graph.builder import (
    DeclarativeGraphBuilder, GraphTemplate, NodeSpec, EdgeSpec,
    ParallelGroupSpec, ParallelGroupConfig, HighLevelGraphAPI
)
from spoon_ai.graph.config import GraphConfig
from spoon_ai.graph import StateGraph, END


class MyState(TypedDict):
    user_query: str
    intent: str
    result: str
    memory: Annotated[Optional[Dict[str, Any]], None]


async def analyze_intent(state: MyState, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """LLM-powered intent analysis with automatic parameter inference"""
    query = state.get("user_query", "").lower()
    intent = "greet" if "hello" in query else "other"
    return {"intent": intent}


async def generate_result(state: MyState, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """Generate response based on detected intent"""
    intent = state.get("intent", "other")
    if intent == "greet":
        return {"result": "Hi! How can I help?"}
    return {"result": "Let me analyze that..."}


def build_declarative_graph() -> StateGraph:
    """Build graph using declarative templates"""

    # Define nodes with specifications
    nodes = [
        NodeSpec("analyze_intent", analyze_intent),
        NodeSpec("generate_result", generate_result),
    ]

    # Define edges
    edges = [
        EdgeSpec("analyze_intent", "generate_result"),
        EdgeSpec("generate_result", END),
    ]

    # Define parallel groups (if needed)
    parallel_groups = []

    # Configure graph settings
    config = GraphConfig(max_iterations=100)

    # Create template
    template = GraphTemplate(
        entry_point="analyze_intent",
        nodes=nodes,
        edges=edges,
        parallel_groups=parallel_groups,
        config=config
    )

    # Build graph
    builder = DeclarativeGraphBuilder(MyState)
    graph = builder.build(template)

    # Enable monitoring
    if hasattr(graph, "enable_monitoring"):
        graph.enable_monitoring([
            "execution_time",
            "llm_response_quality",
            "routing_performance"
        ])

    return graph


# High-level API usage
async def run_with_high_level_api(query: str) -> Dict[str, Any]:
    """Use HighLevelGraphAPI for automatic parameter inference"""
    api = HighLevelGraphAPI(MyState)

    intent, initial_state = await api.build_initial_state(query)

    # Build and run graph
    graph = build_declarative_graph()
    compiled = graph.compile()

    return await compiled.invoke(initial_state)
```

Execute:

```python
# Simple execution
compiled = build_declarative_graph().compile()
result = await compiled.invoke({"user_query": "hello graph"})
print(result["result"])  # Hi! How can I help?

# Advanced execution with high-level API
result = await run_with_high_level_api("analyze crypto trends")
```

---

## Intelligent Routing

SpoonOS offers advanced routing capabilities with priority-based decision making. Routes are evaluated in order: LLM router â†’ intelligent rules â†’ conditional edges â†’ regular edges.

### 1) High-Level API Router (Recommended)

```python
# Using HighLevelGraphAPI for automatic intent-based routing
async def route_with_high_level_api(state: MyState) -> str:
    api = HighLevelGraphAPI(MyState)
    intent = await api.intent_analyzer.analyze(state.get("user_query", ""))
    # Route based on the detected intent category
    return intent.category
```

### 2) LLM-Powered Router

```python
# Configure LLM router with priority system
graph.enable_llm_routing(config={
    "model": "gpt-4",
    "temperature": 0.1,
    "max_tokens": 64
})
```

### 3) Conditional Edges (function-based)

```python
def route_after_intent(state: MyState) -> str:
    return "path_a" if state.get("intent") == "greet" else "path_b"

graph.add_conditional_edges(
    "analyze_intent",
    route_after_intent,
    {"path_a": "generate_result", "path_b": "fallback"}
)
```

### 4) Rules and Patterns

```python
# Add routing rules with priorities
graph.add_routing_rule(
    "analyze_intent",
    lambda s, q: "price" in q,
    target_node="fetch_prices",
    priority=10
)
graph.add_pattern_routing(
    "analyze_intent",
    r"buy|sell|trade",
    target_node="make_decision",
    priority=5
)
```

---

## Advanced Parallel Execution

Define parallel groups with sophisticated control strategies for optimal performance.

```python
# Configure parallel group with advanced settings
parallel_config = ParallelGroupConfig(
    join_strategy="all_complete",  # all, quorum, any_first
    error_strategy="collect_errors",  # ignore_errors, fail_fast, collect_errors
    timeout=30,
    retry_policy={
        "max_retries": 3,
        "backoff_factor": 2.0,
        "circuit_breaker_threshold": 5
    },
    max_in_flight=10
)

graph.add_parallel_group(
    "fetch_group",
    ["fetch_prices", "fetch_social", "fetch_news"],
    config=parallel_config
)
```

Advanced join strategies:
- **`all_complete`**: Wait for all branches (default)
- **`quorum`**: Wait for majority (e.g., 2 out of 3)
- **`any_first`**: Return first successful result

---

## Memory Integration

### A) High-Level API Memory (Recommended)

Use `HighLevelGraphAPI` for automatic memory management.

```python
async def load_memory_with_api(state: MyState) -> Dict[str, Any]:
    api = HighLevelGraphAPI()
    memory = await api.memory_manager.load_context(state.get("user_name", "default"))
    return {"memory": memory}

async def update_memory_with_api(state: MyState) -> Dict[str, Any]:
    api = HighLevelGraphAPI()
    await api.memory_manager.update_context(
        state.get("user_name", "default"),
        {"last_intent": state.get("intent", "unknown")}
    )
    return {}
```

### B) Node-Level Memory (Custom)

Add memory nodes for fine-grained control.

```python
async def load_memory(state: MyState) -> Dict[str, Any]:
    # Custom memory loading logic
    return {"memory": {"preferences": {}, "history": []}}

async def update_memory(state: MyState) -> Dict[str, Any]:
    # Custom memory update logic
    return {"memory": state.get("memory", {})}
```

---

## Configuration-Driven Design

Use `GraphConfig` for comprehensive graph configuration.

```python
# Configure graph behavior
from spoon_ai.graph.config import RouterConfig

config = GraphConfig(
    max_iterations=100,
    router=RouterConfig(
        allow_llm=True,
        default_target="generate_result"
    )
)

template = GraphTemplate(
    entry_point="analyze_intent",
    nodes=nodes,
    edges=edges,
    parallel_groups=parallel_groups,
    config=config
)

# Enable monitoring on the graph instance
graph = DeclarativeGraphBuilder(MyState).build(template)
graph.enable_monitoring(["execution_time", "routing_performance"])
```

---

## Monitoring and Metrics

```python
# Enable comprehensive monitoring
graph.enable_monitoring([
    "execution_time",
    "success_rate",
    "routing_performance",
    "llm_response_quality",
    "parallel_branch_efficiency"
])

compiled = graph.compile()
result = await compiled.invoke({"user_query": "..."})

# Get detailed metrics
metrics = compiled.get_execution_metrics()
print(f"Execution time: {metrics.get('execution_time', 0)}s")
print(f"LLM calls: {metrics.get('llm_calls', 0)}")
print(f"Routing accuracy: {metrics.get('routing_accuracy', 0)}%")
```

---

## End-to-End Declarative Example

Complete example using declarative templates and high-level API.

```python
from spoon_ai.graph import StateGraph, END
from spoon_ai.graph.builder import (
    DeclarativeGraphBuilder, GraphTemplate, NodeSpec, EdgeSpec,
    ParallelGroupSpec, HighLevelGraphAPI
)
from spoon_ai.graph.config import GraphConfig


class CryptoAnalysisState(TypedDict):
    user_query: str
    symbol: str
    timeframes: List[str]
    market_data: Dict[str, Any]
    analysis_result: str
    memory: Annotated[Optional[Dict[str, Any]], None]


async def fetch_market_data(state: CryptoAnalysisState, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """Fetch market data for multiple timeframes in parallel"""
    symbol = state.get("symbol", "BTC")
    timeframes = state.get("timeframes", ["1h", "4h"])

    # Parallel fetching logic here
    return {"market_data": {"symbol": symbol, "data": "..."}}


async def analyze_market(state: CryptoAnalysisState, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """LLM-powered market analysis"""
    # Analysis logic here
    return {"analysis_result": "Market analysis complete"}


def build_crypto_analysis_graph() -> StateGraph:
    """Build complete crypto analysis workflow"""

    nodes = [
        NodeSpec("fetch_market_data", fetch_market_data, parallel_group="data_collection"),
        NodeSpec("analyze_market", analyze_market),
    ]

    edges = [
        EdgeSpec("fetch_market_data", "analyze_market"),
        EdgeSpec("analyze_market", END),
    ]

    parallel_groups = [
        ParallelGroupSpec(
            name="data_collection",
            nodes=["fetch_market_data"],
            config=ParallelGroupConfig(join_strategy="all_complete")
        )
    ]

    config = GraphConfig(
        max_iterations=100,
        enable_monitoring=True,
        monitoring_metrics=["execution_time", "data_quality"]
    )

    template = GraphTemplate(
        entry_point="fetch_market_data",
        nodes=nodes,
        edges=edges,
        parallel_groups=parallel_groups,
        config=config
    )

    builder = DeclarativeGraphBuilder(CryptoAnalysisState)
    return builder.build(template)


# High-level API integration
async def run_crypto_analysis(query: str) -> Dict[str, Any]:
    """Complete analysis using high-level API"""
    api = HighLevelGraphAPI(CryptoAnalysisState)

    # Automatic parameter inference
    intent, initial_state = await api.build_initial_state(query)
    initial_state.update(await api.parameter_inference.infer_parameters(query, intent))

    # Build and execute graph
    graph = build_crypto_analysis_graph()
    compiled = graph.compile()

    return await compiled.invoke(initial_state)
```

---

## Memory System Integration

The graph runtime builds on the SpoonOS Memory System to persist context, metadata, and execution state across runs. Every compiled graph can attach a `Memory` store so routers, reducers, and agents reason over accumulated history without bespoke plumbing.

### Overview

- Persistent JSON-backed storage keyed by `session_id`
- Chronological message history with metadata enrichment
- Query helpers for search and time-based filtering
- Automatic wiring inside `GraphAgent` and high-level APIs

### Core Components

```python
from spoon_ai.graph.agent import Memory

# Use default storage path (~/.spoon_ai/memory)
default_memory = Memory()

# Customize location and session isolation
scoped_memory = Memory(storage_path="./custom_memory", session_id="my_session")
```

- **Persistent storage** keeps transcripts and state checkpoints on disk
- **Session management** separates contexts per agent or user
- **Metadata fields** let reducers store structured state
- **Search helpers** (`search_messages`, `get_recent_messages`) surface relevant history

### Basic Usage Patterns

```python
message = {"role": "user", "content": "Hello, how can I help?"}
scoped_memory.add_message(message)

all_messages = scoped_memory.get_messages()
recent = scoped_memory.get_recent_messages(hours=24)
metadata = scoped_memory.get_metadata("last_topic")
```

Use metadata to thread routing hints and conversation topics, and prune history with retention policies or manual cleanup (`memory.clear()`).

### Graph Workflow Integration

`GraphAgent` wires memory automatically and exposes statistics for monitoring:

```python
from spoon_ai.graph import GraphAgent, StateGraph

agent = GraphAgent(
    name="crypto_analyzer",
    graph=my_graph,
    memory_path="./agent_memory",
    session_id="crypto_session"
)

result = await agent.run("Analyze BTC trends")
stats = agent.get_memory_statistics()
print(stats["total_messages"])
```

Switch between sessions to isolate experiments (`agent.load_session("research_session")`) or inject custom `Memory` subclasses for domain-specific validation.

### Advanced Patterns

- Call `memory.get_statistics()` to monitor file size, last update time, and record counts
- Implement custom subclasses to enforce schemas or add enrichment hooks
- Use time-window retrieval for reducers that need the most recent facts only
- Build automated cleanup jobs for oversized stores (>10MB) to keep execution tight

### Troubleshooting

```python
import json
try:
    with open(scoped_memory.session_file, "r") as fh:
        json.load(fh)
except json.JSONDecodeError:
    scoped_memory.clear()  # Reset corrupted memory files
```

Conflicts typically trace back to duplicated session IDsâ€”compose unique identifiers with timestamps or agent names to avoid contention.

---

## Best Practices

- **Use declarative templates**: `GraphTemplate` + `NodeSpec` for maintainable workflows
- **Leverage high-level API**: `HighLevelGraphAPI` for automatic parameter inference
- **Configure parallel execution**: Use `ParallelGroupConfig` for optimal performance
- **Implement proper error handling**: Use retry policies and circuit breakers
- **Monitor performance**: Enable metrics and use `get_execution_metrics()`
- **Keep state bounded**: Configure `state_reducer_max_list_length` to prevent memory issues

---

## Next Steps

### ðŸ“š **Hands-on Examples**

#### ðŸŽ¯ [Declarative Crypto Analysis](../examples/graph-crypto-analysis.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py)

**What it demonstrates:**
- Complete end-to-end cryptocurrency analysis pipeline using declarative templates
- LLM-driven decision making from data collection to investment recommendations
- Real-time technical indicator calculation (RSI, MACD, EMA) with PowerData toolkit
- Multi-timeframe analysis with advanced parallel processing
- High-level API integration for automatic parameter inference

**Key features:**
- Declarative `GraphTemplate` construction
- `HighLevelGraphAPI` for intent analysis and parameter inference
- Real Binance API integration with error recovery
- Comprehensive market analysis with actionable insights

#### ðŸ”§ [Declarative Intent Graph Demo](../examples/intent-graph-demo.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py)

**What it demonstrates:**
- Intelligent query routing system using `HighLevelGraphAPI`
- True parallel execution across multiple timeframes
- Advanced memory management with persistent context
- LLM-powered routing decisions and summarization
- Declarative graph construction with fresh node implementations

**Key features:**
- `GraphTemplate` and `NodeSpec` for modular workflow construction
- `ParameterInferenceEngine` for automatic parameter extraction
- Dynamic workflow routing based on user intent
- Concurrent data fetching for optimal performance

### ðŸ› ï¸ **Integration Guides**

- **[Tools Integration](./tools.md)** - Learn how to integrate external capabilities and APIs
- **[Agent Architecture](./agents.md)** - Understand when to wrap graphs with long-lived agents
- **[MCP Protocol](../core-concepts/mcp-protocol.md)** - Explore dynamic tool discovery and execution

### ðŸ“– **Additional Resources**
- **[State Management](../api-reference/graph/state-graph.md)** - Reducer configuration guide
- **[Agents Detailed](./agents-detailed.md)** - Long-lived agent design patterns
- **[Graph Builder API](../api-reference/graph/)** - Complete declarative API documentation
- **[Performance Optimization](../troubleshooting/performance.md)** - Graph performance tuning guides
- **[Troubleshooting](../troubleshooting/common-issues.md)** - Common issues and solutions

---

FILE: docs/core-concepts/llm-providers.md

# LLM Providers

SpoonOS supports multiple language model providers through a unified interface, enabling seamless switching between different AI models.

## Supported Providers

### OpenAI

- **Models**: GPT-4.1 (default), GPT-4o, GPT-4o-mini, o1-preview, o1-mini
- **Features**: Function calling, streaming, embeddings, reasoning models
- **Best for**: General-purpose tasks, reasoning, code generation

```python
from spoon_ai.chat import ChatBot

# OpenAI configuration with default model
llm = ChatBot(
    model_name="gpt-4.1",  # Framework default
    llm_provider="openai",
    temperature=0.7
)
```

### Anthropic (Claude)

- **Models**: Claude-Sonnet-4-20250514 (default), Claude-3.5 Sonnet, Claude-3.5 Haiku
- **Features**: Large context windows, prompt caching, safety features
- **Best for**: Long documents, analysis, safety-critical applications

```python
# Anthropic configuration with default model
llm = ChatBot(
    model_name="claude-sonnet-4-20250514",  # Framework default
    llm_provider="anthropic",
    temperature=0.1
)
```

### Google (Gemini)

- **Models**: Gemini-2.5-Pro (default), Gemini-2.0-Flash, Gemini-1.5-Pro
- **Features**: Multimodal capabilities, fast inference, large context
- **Best for**: Multimodal tasks, cost-effective solutions, long context

```python
# Google configuration with default model
llm = ChatBot(
    model_name="gemini-2.5-pro",  # Framework default
    llm_provider="gemini",
    temperature=0.1
)
```

### DeepSeek

- **Models**: DeepSeek-Reasoner (default), DeepSeek-V3, DeepSeek-Chat
- **Features**: Advanced reasoning, code-specialized models, cost-effective
- **Best for**: Complex reasoning, code generation, technical tasks

```python
# DeepSeek configuration with default model
llm = ChatBot(
    model_name="deepseek-reasoner",  # Framework default
    llm_provider="deepseek",
    temperature=0.2
)
```

### OpenRouter

- **Models**: Access to multiple providers through one API
- **Features**: Model routing, cost optimization
- **Best for**: Experimentation, cost optimization

```python
# OpenRouter configuration
llm = ChatBot(
    model_name="anthropic/claude-3-opus",
    llm_provider="openrouter",
    temperature=0.7
)
```

## Unified LLM Manager

The LLM Manager provides provider-agnostic access with automatic fallback:

```python
from spoon_ai.llm.manager import LLMManager

# Initialize with multiple providers
llm_manager = LLMManager(
    primary_provider="openai",
    fallback_providers=["anthropic", "gemini"],
    model_preferences={
        "openai": "gpt-4.1",
        "anthropic": "claude-sonnet-4-20250514",
        "gemini": "gemini-2.5-pro",
        "deepseek": "deepseek-reasoner"
    }
)

# Use with automatic fallback
response = await llm_manager.generate("Explain quantum computing")
```

## Configuration

### Environment Variables

```bash
# Provider API Keys
OPENAI_API_KEY=sk-your_openai_key_here
ANTHROPIC_API_KEY=sk-ant-your_anthropic_key_here
GEMINI_API_KEY=your_gemini_key_here
DEEPSEEK_API_KEY=your_deepseek_key_here
OPENROUTER_API_KEY=sk-or-your_openrouter_key_here

# Default Settings
DEFAULT_LLM_PROVIDER=openai
DEFAULT_MODEL=gpt-4.1
DEFAULT_TEMPERATURE=0.3
```

### Runtime Configuration

```json
{
  "llm": {
    "provider": "openai",
    "model": "gpt-4.1",
    "temperature": 0.3,
    "max_tokens": 32768,
    "fallback_providers": ["anthropic", "deepseek", "gemini"]
  }
}
```

## Advanced Features

### Prompt Caching (Anthropic)

```python
from spoon_ai.llm.cache import PromptCache

# Enable prompt caching for repeated system prompts
llm = ChatBot(
    model_name="claude-sonnet-4-20250514",
    llm_provider="anthropic",
    enable_caching=True
)
```

### Streaming Responses

```python
# Stream responses for real-time interaction
async for chunk in llm.stream("Write a long story about AI"):
    print(chunk, end="", flush=True)
```

### Function Calling

```python
# Define functions for the model to call
functions = [
    {
        "name": "get_weather",
        "description": "Get current weather",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string"}
            }
        }
    }
]

response = await llm.generate(
    "What's the weather in New York?",
    functions=functions
)
```

## Model Selection Guide

### Task-Based Recommendations

#### Code Generation

- Primary: DeepSeek-Reasoner, GPT-4.1
- Alternative: Claude-Sonnet-4

#### Analysis & Reasoning

- Primary: DeepSeek-Reasoner, GPT-4.1, Claude-Sonnet-4
- Alternative: Gemini-2.5-Pro

#### Cost-Sensitive Tasks

- Primary: DeepSeek-Reasoner, Gemini-2.5-Pro
- Alternative: GPT-4.1

#### Long Context Tasks

- Primary: Gemini-2.5-Pro (250K tokens), Claude-Sonnet-4 (200K tokens)
- Alternative: DeepSeek-Reasoner (65K tokens)

### Performance Comparison

| Provider                  | Speed     | Cost     | Context | Quality              |
| ------------------------- | --------- | -------- | ------- | -------------------- |
| OpenAI GPT-4.1            | Fast      | Medium   | 128K    | Excellent            |
| Anthropic Claude-Sonnet-4 | Medium    | Medium   | 200K    | Excellent            |
| Google Gemini-2.5-Pro     | Very Fast | Low      | 250K    | Very Good            |
| DeepSeek-Reasoner         | Fast      | Very Low | 65K     | Superior (Reasoning) |
| OpenAI o1-preview         | Slow      | High     | 128K    | Superior (Reasoning) |

## Error Handling & Fallbacks

### Automatic Fallback

The framework provides built-in error handling with automatic fallback between providers:

```python
from spoon_ai.llm.manager import LLMManager

# Configure fallback chain - errors are handled automatically
llm_manager = LLMManager(
    primary_provider="openai",
    fallback_providers=["anthropic", "google"],
    retry_attempts=3,
    timeout=30
)

# Automatic fallback on provider failures
response = await llm_manager.generate("Hello world")
```

### Error Types & Recovery

The framework uses structured error types for clean error handling:

```python
from spoon_ai.llm.errors import RateLimitError, AuthenticationError, ModelNotFoundError

# Simple error handling with specific error types
response = await llm.generate("Hello world")

# Framework handles common errors automatically:
# - Rate limits: automatic retry with backoff
# - Network issues: automatic retry with fallback
# - Authentication: clear error messages
# - Model availability: fallback to alternative models
```

### Graceful Degradation

```python
# Framework provides graceful degradation patterns
llm_manager = LLMManager(
    primary_provider="openai",
    fallback_providers=["deepseek", "gemini"],  # Cost-effective fallbacks
    enable_graceful_degradation=True
)

# If primary fails, automatically uses fallback
# No manual error handling required
response = await llm_manager.generate("Complex reasoning task")
```

## Monitoring & Metrics

### Usage Tracking

```python
from spoon_ai.llm.monitoring import LLMMonitor

# Track usage and costs automatically
monitor = LLMMonitor()
response = await llm.generate("Hello", monitor=monitor)

# Get metrics
metrics = monitor.get_metrics()
print(f"Tokens used: {metrics.total_tokens}")
print(f"Cost: ${metrics.total_cost}")
```

### Performance Monitoring

```python
# Monitor response times and success rates
monitor.log_request(
    provider="openai",
    model="gpt-4",
    tokens=150,
    latency=1.2,
    success=True
)
```

## Best Practices

### Provider Selection

- **Test multiple providers** for your specific use case
- **Consider cost vs. quality** trade-offs
- **Use fallbacks** for production reliability

### Configuration Management

- **Store API keys securely** in environment variables
- **Use configuration files** for easy switching
- **Monitor usage and costs** regularly

### Performance Optimization

- **Cache responses** when appropriate
- **Use streaming** for long responses
- **Batch requests** when possible

### Error Handling Philosophy

The SpoonOS framework follows a "fail-fast, recover-gracefully" approach:

- **Automatic Recovery**: Common errors (rate limits, network issues) are handled automatically
- **Structured Errors**: Use specific error types instead of generic exceptions
- **Fallback Chains**: Configure multiple providers for automatic failover
- **Minimal Try-Catch**: Let the framework handle errors; only catch when you need custom logic

```python
# Preferred: Let framework handle errors
response = await llm_manager.generate("Hello world")

# Only use explicit error handling for custom business logic
if response.provider != "openai":
    logger.info(f"Fell back to {response.provider}")
```

## Next Steps

- [Agents](./agents.md) - Learn how agents use LLMs
- [MCP Protocol](./mcp-protocol.md) - Dynamic tool integration
- [Configuration Guide](../getting-started/configuration.md) - Detailed setup instructions

---

FILE: docs/core-concepts/mcp-protocol.md

# MCP Protocol

The Model Context Protocol (MCP) enables dynamic tool discovery and execution, allowing agents to access external capabilities at runtime.

## What is MCP?

MCP is a standardized protocol that allows AI agents to:

- **Discover** available tools and resources dynamically
- **Execute** tools with proper parameter validation
- **Access** external APIs and services seamlessly
- **Extend** capabilities without code changes

## How MCP Works

### Architecture Overview

```mermaid
graph TD
    A[Agent] --> B[MCP Client]
    B --> C[MCP Server]
    C --> D[Tool 1]
    C --> E[Tool 2]
    C --> F[Tool N]

    B --> G[Tool Discovery]
    B --> H[Tool Execution]
    B --> I[Resource Access]
```

### MCP Components

1. **MCP Server** - Hosts tools and resources
2. **MCP Client** - Connects agents to servers
3. **Tools** - Executable functions with defined schemas
4. **Resources** - Data sources and content

## Setting Up MCP

### Basic MCP Server

```python
import asyncio
from spoon_ai.tools.mcp_tools_collection import MCPToolsCollection

mcp_tools = MCPToolsCollection()

async def main():
    # Runs a FastMCP SSE server. Change the port as needed.
    await mcp_tools.run(port=8765)

asyncio.run(main())
```

### MCP Client Configuration

```python
import asyncio
from spoon_ai.agents.mcp_client_mixin import MCPClientMixin

class MCPEnabledClient(MCPClientMixin):
    def __init__(self, transport: str):
        super().__init__(transport)

client = MCPEnabledClient("ws://localhost:8765")

async def list_tools():
    async with client.get_session() as session:
        return await session.list_tools()

tools = asyncio.run(list_tools())
for tool in tools:
    print(f"{tool.name}: {tool.description}")
```

## Tool Discovery

### Automatic Discovery

```python
async def discover_tools():
    async with client.get_session() as session:
        return await session.list_tools()

tools = asyncio.run(discover_tools())
for tool in tools:
    print(tool.name)
```

### Tool Registration

```python
from spoon_ai.tools.base import BaseTool
from spoon_ai.tools.mcp_tools_collection import mcp_tools

class WeatherTool(BaseTool):
    name = "get_weather"
    description = "Get current weather for a location"
    parameters = {
        "type": "object",
        "properties": {
            "location": {"type": "string", "description": "City name"}
        },
        "required": ["location"]
    }

    async def execute(self, location: str) -> dict:
        # Weather API call implementation
        return {"location": location, "temperature": 22, "condition": "sunny"}

# Register tool with the running MCP server
asyncio.run(mcp_tools.add_tool(WeatherTool()))
```

## Tool Execution

### Direct Execution

```python
# Execute tool via MCP client
result = asyncio.run(client.call_mcp_tool("get_weather", location="New York"))
print(result)
```

### Agent-Driven Execution

```python
from spoon_ai.agents.spoon_react_mcp import SpoonReactMCP

# Agent that can consume MCP tools discovered via its transport
agent = SpoonReactMCP()
response = await agent.run("What's the weather like in San Francisco?")
print(response)
```

## MCP Configuration

### Server Configuration

```json
{
  "mcp": {
    "servers": [
      {
        "name": "local_tools",
        "url": "http://localhost:8000",
        "timeout": 30
      },
      {
        "name": "external_api",
        "url": "https://api.example.com/mcp",
        "auth": {
          "type": "bearer",
          "token": "your_token_here"
        }
      }
    ]
  }
}
```

### Client Configuration

```python
from spoon_ai.tools.mcp_client import MCPClient

# Configure MCP client
client = MCPClient(
    servers=[
        "http://localhost:8000",
        "https://api.example.com/mcp"
    ],
    timeout=30,
    retry_attempts=3
)
```

## Security Considerations

### Authentication

```python
# Server-side authentication
class AuthenticatedMCPServer:
    def __init__(self, api_key: str):
        self.api_key = api_key

    def authenticate(self, request_key: str) -> bool:
        return request_key == self.api_key
```

### Tool Permissions

```python
# Define tool permissions
class RestrictedTool(BaseTool):
    required_permissions = ["read_data", "write_files"]

    async def execute(self, **kwargs):
        # Check permissions before execution
        if not self.check_permissions():
            raise PermissionError("Insufficient permissions")

        return await self.perform_action(**kwargs)
```

### Input Validation

```python
# Validate tool inputs
class SecureTool(BaseTool):
    async def execute(self, user_input: str) -> str:
        # Sanitize input
        clean_input = self.sanitize_input(user_input)

        # Validate against schema
        if not self.validate_input(clean_input):
            raise ValueError("Invalid input")

        return await self.process(clean_input)
```

## Performance Optimization

### Connection Pooling

```python
# Use connection pooling for multiple MCP servers
from spoon_ai.tools.mcp_pool import MCPConnectionPool

pool = MCPConnectionPool(
    servers=["http://server1:8000", "http://server2:8000"],
    max_connections=10,
    timeout=30
)
```

### Caching

```python
# Cache tool discovery results
from spoon_ai.tools.mcp_cache import MCPCache

cache = MCPCache(ttl=300)  # 5-minute cache
tools = await cache.get_or_fetch("tools", mcp_tools.discover_tools)
```

### Async Operations

```python
# Execute multiple tools concurrently
import asyncio

async def parallel_execution():
    tasks = [
        mcp_tools.execute_tool("tool1", {"param": "value1"}),
        mcp_tools.execute_tool("tool2", {"param": "value2"}),
        mcp_tools.execute_tool("tool3", {"param": "value3"})
    ]

    results = await asyncio.gather(*tasks)
    return results
```

## Common Use Cases

### API Integration

```python
# Integrate external APIs through MCP
class APITool(BaseTool):
    name = "api_call"

    async def execute(self, endpoint: str, method: str = "GET") -> dict:
        async with aiohttp.ClientSession() as session:
            async with session.request(method, endpoint) as response:
                return await response.json()
```

### Database Access

```python
# Database operations through MCP
class DatabaseTool(BaseTool):
    name = "query_database"

    async def execute(self, query: str) -> list:
        # Execute database query
        return await self.db.execute(query)
```

### File Operations

```python
# File system operations
class FileTool(BaseTool):
    name = "read_file"

    async def execute(self, filepath: str) -> str:
        with open(filepath, 'r') as f:
            return f.read()
```

## Best Practices

### Tool Design

- **Clear naming** - Use descriptive tool names
- **Comprehensive schemas** - Define complete parameter schemas
- **Error handling** - Leverage framework's automatic error handling
- **Documentation** - Provide clear descriptions and examples

### Performance

- **Connection reuse** - Reuse MCP connections when possible
- **Caching** - Cache discovery results and frequently used data
- **Timeouts** - Set appropriate timeouts for tool execution

### Security

- **Input validation** - Always validate tool inputs
- **Authentication** - Implement proper authentication mechanisms
- **Permissions** - Use least-privilege access principles

### Error Handling Philosophy

The SpoonOS framework follows a "fail-fast, recover-gracefully" approach for MCP operations:

- **Automatic Recovery**: Connection failures, timeouts, and server errors are handled automatically
- **Graceful Degradation**: When tools are unavailable, the system provides meaningful fallbacks
- **Minimal Manual Handling**: Let the framework handle errors; only intervene for custom business logic

```python
# Preferred: Let framework handle MCP errors
result = await mcp_tools.execute_tool("weather_tool", {"location": "NYC"})

# Framework automatically handles:
# - Server connection issues
# - Tool discovery failures
# - Execution timeouts
# - Parameter validation errors
```

## Troubleshooting

### Common Issues

#### Connection Errors

The framework automatically handles connection failures with built-in retry mechanisms:

```python
# Framework handles connection failures automatically
await mcp_client.connect()  # Automatic retry with exponential backoff
```

#### Tool Discovery Failures

```python
# Framework provides graceful handling of discovery issues
tools = await mcp_tools.discover_tools()
# Automatic fallback to cached tools if server unavailable
```

#### Execution Timeouts

```python
# Framework manages timeouts automatically
result = await mcp_tools.execute_tool("slow_tool", {})
# Automatic timeout handling with configurable limits
```

## Next Steps

### ðŸ“š **MCP Implementation Examples**

#### ðŸ” [MCP Spoon Search Agent](../examples/mcp-spoon-search-agent.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

**What it demonstrates:**
- Complete MCP server integration and tool discovery
- Real-world MCP implementation with Tavily web search
- Dynamic tool loading and orchestration
- Production-ready MCP error handling and recovery

**Key features:**
- Tavily MCP server integration via `npx tavily-mcp`
- Automatic tool discovery and validation
- Seamless integration with existing SpoonOS architecture
- Advanced error handling for MCP server failures

**Learning outcomes:**
- How to initialize and manage MCP servers
- Dynamic tool discovery patterns
- MCP server error handling and recovery
- Integration of MCP tools with LLM workflows

### ðŸ› ï¸ **Development Resources**

- **[Tools System](./tools.md)** - Learn about the complete tool ecosystem
- **[Custom Tool Development](../how-to-guides/add-custom-tools.md)** - Build MCP-compatible tools
- **[MCP Tool Reference](../api-reference/tools/builtin-tools.md)** - MCP-specific tool documentation

### ðŸ“– **Additional Resources**

- **[Graph System](../core-concepts/graph-system.md)** - Advanced workflow orchestration
- **[Agent Architecture](../core-concepts/agents.md)** - Agent-MCP integration patterns
- **[API Reference](../api-reference/)** - Complete SpoonOS API documentation
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

**What it demonstrates:**
- Complete MCP server integration and tool discovery
- Real-world MCP implementation with Tavily web search
- Dynamic tool loading and orchestration
- Production-ready MCP error handling and recovery

**Key features:**
- Tavily MCP server integration via `npx tavily-mcp`
- Automatic tool discovery and validation
- Seamless integration with existing SpoonOS architecture
- Advanced error handling for MCP server failures

**Learning outcomes:**
- How to initialize and manage MCP servers
- Dynamic tool discovery patterns
- MCP server error handling and recovery
- Integration of MCP tools with LLM workflows

### ðŸ› ï¸ **Development Resources**

- **[Tools System](./tools.md)** - Learn about the complete tool ecosystem
- **[Custom Tool Development](../how-to-guides/add-custom-tools.md)** - Build MCP-compatible tools
- **[MCP Tool Reference](../api-reference/tools/builtin-tools.md)** - MCP-specific tool documentation

### ðŸ“– **Additional Resources**

- **[Graph System](../core-concepts/graph-system.md)** - Advanced workflow orchestration
- **[Agent Architecture](../core-concepts/agents.md)** - Agent-MCP integration patterns
- **[API Reference](../api-reference/)** - Complete SpoonOS API documentation

---

FILE: docs/core-concepts/Short-term memory.md

---

## Overview

Memory is a system that enables an AI agent to retain information from past interactions.
For intelligent agents, memory is vital because it allows them to recall previous conversations, learn from feedback, and adapt to user preferences over time. As agents engage in increasingly complex tasks involving many user exchanges, a reliable memory mechanism becomes essential for both efficiency and user experience.

### Short-term memory

Short-term memory allows your application to remember recent interactions within a single thread or conversation.
A thread groups multiple exchanges in a session, much like how an email client organizes messages into a single conversation.

The conversation history is the most common form of short-term memory. However, long-running conversations pose a major challenge for modern large language models (LLMs):
a complete history may exceed the modelâ€™s context window, leading to context loss or inconsistent responses.

Even when a model supports large context windows, most LLMs still degrade in performance with long contextsâ€”they tend to become distracted by irrelevant or outdated information, respond more slowly, and consume more tokens (and cost).

### Managing context effectively

Chat models receive context through messages, which typically include:

* System messages (instructions or role definitions)
* User messages (human inputs)
* Assistant messages (model responses)

As conversations progress, the message list grows continuously.
Because context windows are finite, applications benefit from strategies that trim, summarize, or forget stale informationâ€”keeping the modelâ€™s focus sharp while maintaining coherence and reducing computational cost.

---


##  Quick Start: ChatBot with Built-in Memory
 When you initialise `ChatBot` with `enable_short_term_memory=True` (the default), it creates a `ShortTermMemoryManager` internally. Before every LLM call, the chatbot feeds the running history into the manager, which handles trimming or summarising and (optionally) saving checkpoints. 
 You can still access the same manager manually via `chatbot.short_term_memory_manager` if you want to tweak the behaviour or call its methods directly, as shown in the next section.

```python
import asyncio
from spoon_ai.chat import ChatBot

chatbot = ChatBot(enable_short_term_memory=True)

messages = [
    {"role": "user", "content": "Explain blockchain in one sentence."},
    {"role": "assistant", "content": "A blockchain is an append-only, shared ledger."},
]

reply = asyncio.run(chatbot.ask(messages))
print(reply)
```

---

##  Manage short-term memory by using `ShortTermMemoryManager` 

```python
import asyncio
from spoon_ai.memory.short_term_manager import ShortTermMemoryManager, TrimStrategy
from spoon_ai.schema import Message

manager = ShortTermMemoryManager()
history = [
    Message(id="u1", role="user", content="Hello!"),
    Message(id="a1", role="assistant", content="Hi there â€” how can I help?"),
    Message(id="u2", role="user", content="What's DeFi?"),
]


#  Trim the message list by token budget
trimmed = asyncio.run(
    manager.trim_messages(
        messages=history,
        max_tokens=48,
        strategy=TrimStrategy.FROM_END,
        keep_system=True,
    )
)

#  Summarize history before model call
llm_ready, removals, summary = asyncio.run(
    manager.summarize_messages(
        messages=history,
        max_tokens_before_summary=48,
        messages_to_keep=2,
        summary_model="anthropic/claude-3.5-sonnet",
        llm_manager=chatbot.llm_manager,
        llm_provider=chatbot.llm_provider,
        existing_summary=chatbot.latest_summary or "",
    )
)
```

 `llm_ready` â€” condensed history you can pass to the LLM  
 `removals` â€” list of RemoveMessage directives:apply removals to your persisted history using spoon_ai.graph.reducers.add_messages.


```python
from spoon_ai.chat import ChatBot
from spoon_ai.graph.reducers import add_messages

chatbot = ChatBot(enable_short_term_memory=True)
history = [
    Message(id="u1", role="user", content="Hello!"),
    Message(id="a1", role="assistant", content="Hi there â€” how can I help?"),
    Message(id="u2", role="user", content="What's DeFi?"),
]
# Remove the latest assistant message
assistant_ids = [msg.id for msg in history if msg.role == "assistant"]
remove_last = chatbot.remove_message(assistant_ids[-1])

# Or clear the entire history
remove_all = chatbot.remove_all_messages()

# Apply directives to persisted history
updated_history = add_messages(history, [remove_last])
cleared_history = add_messages(history, [remove_all])
```

`add_messages()` merges the removal directives into the existing history,
deleting targeted entries (or the entire transcript).
This mirrors how the short-term memory manager emits `RemoveMessage` items
when summarization trims older turns.

---

##  Inspecting Thread State and Checkpoints

every time the graph runs, you can retrieve the latest snapshot (messages plus metadata), iterate the full checkpoint history, or read a `CheckpointTuple` for an external consumer. This makes it easy to debug memory behaviour, replay from any checkpoint, or sync state to persistent storage. The example below shows how to fetch the most recent summary, list all checkpoints, and view the tuple-style payload.

```python
config = {"configurable": {"thread_id": "memory_demo_thread"}}

snapshot = graph.get_state(config)
print("Latest checkpoint:", snapshot.metadata.get("checkpoint_id"))

for snap in graph.get_state_history(config):
    print("History id:", snap.metadata.get("checkpoint_id"))

checkpoint_tuple = graph.checkpointer.get_checkpoint_tuple(config)
print("Checkpoint tuple:", checkpoint_tuple)

for entry in graph.checkpointer.iter_checkpoint_history(config):
    print("Tuple history entry:", entry)
```

---

FILE: docs/core-concepts/tools.md

# Tools

Tools are the hands and eyes of your agents - they provide the capabilities to interact with the external world. SpoonOS supports both built-in tools and custom tools through a flexible, extensible architecture.

## What are Tools?

Tools in SpoonOS are discrete capabilities that agents can use to:

- **Access external APIs** (web search, databases, APIs)
- **Perform calculations** (math, data analysis, statistics)
- **Manipulate data** (file operations, data processing)
- **Interact with services** (blockchain, social media, messaging)
- **Execute code** (Python, shell commands, scripts)

## Tool Types

### 1. Built-in Tools

Built-in tools are core capabilities provided by the **spoon-toolkit** package. These tools are directly integrated into SpoonOS and include:

> To use `spoon_toolkits.*` modules, install the toolkits package alongside the core SDK:
>
> ```bash
> pip install spoon-toolkits
> ```

- **Crypto Data Tools**: Price data, trading history, wallet analysis, liquidity analysis
- **Data Platform Tools**: AI-powered search, academic research, social media analysis
- **ThirdWeb Tools**: Blockchain data and transaction analysis
- **Neo Blockchain Tools**: Complete Neo ecosystem tools (addresses, assets, contracts, transactions)
- **Crypto PowerData Tools**: Advanced market data and technical analysis

**ðŸ“– For detailed usage and configuration, see: [Built-in Tools Reference](../api-reference/tools/builtin-tools.md)**

### 2. MCP Tools

MCP (Model Context Protocol) tools enable dynamic tool loading from external servers. These tools provide:

- **Dynamic Discovery**: Tools loaded at runtime without restarts
- **Multiple Transports**: Support for stdio, HTTP, and SSE communication
- **Extensible Architecture**: Easy integration of third-party tools
- **Process Isolation**: Tools run in separate processes for stability

**ðŸ“– For detailed MCP protocol usage and configuration, see: [MCP Protocol Guide](./mcp-protocol.md)**

### 3. Custom Tools

Tools you create for specific use cases:

```python
from spoon_ai.tools.base import BaseTool

class CustomTool(BaseTool):
    name: str = "my_custom_tool"
    description: str = "Does something specific"

    async def execute(self, **kwargs) -> str:
        # Your custom logic here
        return "Tool result"
```

**ðŸ“– For detailed custom tool development, see: [How-To Guide: Add Custom Tools](../how-to-guides/add-custom-tools.md)**

## Next Steps

- **[Built-in Tools Reference](../api-reference/tools/builtin-tools.md)** - Complete guide to using spoon-toolkit built-in tools
- **[MCP Protocol Guide](./mcp-protocol.md)** - Detailed MCP tool configuration and usage
- **[Custom Tools Guide](../how-to-guides/add-custom-tools.md)** - Learn to create your own tools

Ready to use tools? Start with the [Built-in Tools Reference](../api-reference/tools/builtin-tools.md)! ðŸ”§
- **Access** real-time information

## Built-in Tools

### Crypto & Trading Tools

**CryptoTools** - Market data and price information
```python
from spoon_ai.tools.crypto_tools import CryptoTools

# Get current Bitcoin price
price = await CryptoTools.get_token_price("BTC")
```

**Web3Tools** - Blockchain interaction
```python
from spoon_ai.tools.web3_tools import Web3Tools

# Get wallet balance
balance = await web3_tools.GetAccountBalanceToo("0x742d35Cc6634C0532925a3b8D4C9db96590e4265")
```

### Data & Analysis Tools

**ChainbaseTools** - Comprehensive blockchain data
```python
from spoon_toolkits.chainbase import ChainbaseTools

# Get token information
token_info = await chainbase_tools.GetTokenMetadataTool("0xA0b86a33E6441E6C8D3c8C7C5b998e7d8e4C8e8e")
```

### Storage Tools

**StorageTools** - Decentralized storage
```python
from spoon_toolkits.storage import StorageTools

# Upload file to IPFS
hash = await storage_tools.upload_file("document.pdf")
```

## Tool Manager

The Tool Manager handles tool registration, discovery, and execution:

```python
from spoon_ai.tools import ToolManager
from spoon_ai.tools.crypto_tools import CryptoTools

# Create tool manager
tool_manager = ToolManager([
    CryptoTools(),
    Web3Tools()
])

# Execute tool
result = await tool_manager.execute_tool("get_token_price", {"symbol": "BTC"})
```

## MCP (Model Context Protocol) Tools

MCP enables dynamic tool discovery and execution:

```python
from spoon_ai.tools.mcp_tools_collection import MCPToolsCollection

# Initialize MCP tools
mcp_tools = MCPToolsCollection()

# Discover available tools
tools = await mcp_tools.discover_tools()

# Execute MCP tool
result = await mcp_tools.execute("weather_tool", {"location": "New York"})
```

## Creating Custom Tools

### Basic Tool Structure

```python
from spoon_ai.tools.base import BaseTool
from typing import Dict, Any

class CustomTool(BaseTool):
    name: str = "custom_tool"
    description: str = "Description of what this tool does"
    parameters: dict = {
        "type": "object",
        "properties": {
            "param1": {"type": "string", "description": "Parameter description"}
        },
        "required": ["param1"]
    }

    async def execute(self, param1: str) -> str:
        # Tool implementation
        return f"Result: {param1}"
```

### Tool Registration

```python
# Register custom tool
from spoon_ai.tools import ToolManager

tool_manager = ToolManager([CustomTool()])

# Use in agent
agent = SpoonReactAI(
    llm=ChatBot(model_name="gpt-4.1", llm_provider="openai"),
    tools=[CustomTool()]
)
```

### Advanced Tool Features

**Async Operations**
```python
import aiohttp

class APITool(BaseTool):
    async def execute(self, endpoint: str) -> dict:
        async with aiohttp.ClientSession() as session:
            async with session.get(endpoint) as response:
                return await response.json()
```

**Error Handling**

```python
class RobustTool(BaseTool):
    async def execute(self, data: str) -> str:
        # Framework handles errors automatically with graceful degradation
        return self.process_data(data)
```

## Tool Configuration

### Environment Variables
```bash
# API Keys for tools
COINGECKO_API_KEY=your_key_here
CHAINBASE_API_KEY=your_key_here
GOPLUS_API_KEY=your_key_here
```

### Runtime Configuration
```json
{
  "tools": {
    "enabled": ["crypto_tools", "web3_tools"],
    "crypto_tools": {
      "default_currency": "USD",
      "cache_duration": 300
    }
  }
}
```

## Best Practices

### Tool Design

- **Single Responsibility** - Each tool should have one clear purpose
- **Clear Parameters** - Use descriptive parameter names and types
- **Error Handling** - Leverage framework's automatic error handling
- **Documentation** - Provide clear descriptions and examples

### Performance

- **Caching** - Cache expensive API calls when appropriate
- **Async Operations** - Use async/await for I/O operations
- **Rate Limiting** - Respect API rate limits

### Security

- **Input Validation** - Validate all input parameters
- **API Key Management** - Store keys securely in environment variables
- **Permission Checks** - Verify permissions before executing sensitive operations

## Tool Categories

### Data Sources

- Market data APIs (CoinGecko, CoinMarketCap)
- Blockchain data (Chainbase, Alchemy)
- Social media APIs (Twitter, Discord)

### Execution Tools

- Blockchain transactions (Web3, Solana)
- File operations (Storage, IPFS)
- Communication (Email, Slack)

### Analysis Tools

- Security scanning (GoPlus Labs)
- Technical analysis (Trading indicators)
- Data processing (Pandas, NumPy)

## Next Steps

### ðŸ“š **Practical Examples**

#### ðŸ” [MCP Spoon Search Agent](../examples/mcp-spoon-search-agent.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

**What it demonstrates:**
- Complete MCP (Model Context Protocol) implementation
- Integration of web search capabilities with cryptocurrency analysis
- Real-time data correlation between multiple APIs
- Dynamic tool discovery and orchestration

**Key features:**
- Tavily web search integration via MCP
- Crypto PowerData tools for market analysis
- Unified analysis combining search results with technical indicators
- Production-ready error handling and API management

#### ðŸ“Š [Graph Crypto Analysis](../examples/graph-crypto-analysis.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py)

**What it demonstrates:**
- Advanced cryptocurrency analysis using multiple tool types
- Real-time technical indicator calculation (RSI, MACD, EMA)
- Multi-timeframe data processing and correlation
- LLM-driven decision making with tool integration

**Key features:**
- Real Binance API integration for live market data
- Intelligent token selection and analysis
- Comprehensive market sentiment analysis
- Investment recommendations based on technical and fundamental data

### ðŸ› ï¸ **Development Guides**

- **[MCP Protocol](./mcp-protocol.md)** - Learn about dynamic tool discovery and execution
- **[Custom Tool Development](../how-to-guides/add-custom-tools.md)** - Build your own tools from scratch
- **[Tool API Reference](../api-reference/tools/base-tool.md)** - Complete tool development documentation

### ðŸ“– **Additional Resources**

- **[Built-in Tools Reference](../api-reference/tools/builtin-tools.md)** - Complete guide to spoon-toolkit
- **[Graph System](../core-concepts/graph-system.md)** - Learn about workflow orchestration
- **[Agent Architecture](../core-concepts/agents.md)** - Understand agent-tool integration patterns

**What it demonstrates:**
- Complete MCP (Model Context Protocol) implementation
- Integration of web search capabilities with cryptocurrency analysis
- Real-time data correlation between multiple APIs
- Dynamic tool discovery and orchestration

**Key features:**
- Tavily web search integration via MCP
- Crypto PowerData tools for market analysis
- Unified analysis combining search results with technical indicators
- Production-ready error handling and API management

#### ðŸ“Š [Graph Crypto Analysis](../examples/graph-crypto-analysis.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py)

**What it demonstrates:**
- Advanced cryptocurrency analysis using multiple tool types
- Real-time technical indicator calculation (RSI, MACD, EMA)
- Multi-timeframe data processing and correlation
- LLM-driven decision making with tool integration

**Key features:**
- Real Binance API integration for live market data
- Intelligent token selection and analysis
- Comprehensive market sentiment analysis
- Investment recommendations based on technical and fundamental data

### ðŸ› ï¸ **Development Guides**

- **[MCP Protocol](./mcp-protocol.md)** - Learn about dynamic tool discovery and execution
- **[Custom Tool Development](../how-to-guides/add-custom-tools.md)** - Build your own tools from scratch
- **[Tool API Reference](../api-reference/tools/base-tool.md)** - Complete tool development documentation

### ðŸ“– **Additional Resources**

- **[Built-in Tools Reference](../api-reference/tools/builtin-tools.md)** - Complete guide to spoon-toolkit
- **[Graph System](../core-concepts/graph-system.md)** - Learn about workflow orchestration
- **[Agent Architecture](../core-concepts/agents.md)** - Understand agent-tool integration patterns

---

FILE: docs/core-concepts/x402-payments.md

# x402 Payments in SpoonOS

x402 is the payment rail SpoonOS uses to gate agent capabilities behind verifiable, instant crypto authorizations. This page explains the concepts you need before wiring the APIs or running demos.

## Mental model

| Piece | Role inside SpoonOS |
| --- | --- |
| **x402 facilitator** | Public service (`https://x402.org/facilitator` by default) that verifies and settles signed payment payloads. |
| **Paywall server** | Your FastAPI router (`spoon_ai.payments.app`) that refuses unpaid requests with a 402 payload and forwards valid calls to agents. |
| **SpoonReact agent** | Issues HTTP probes, signs payments via tools, and stores payment receipts in memory. |
| **Signer** | Either the `PRIVATE_KEY` loaded in-process or a Turnkey identity configured via `TURNKEY_*` variables. |

## Configuration surfaces

Most deployments only need a `.env` entry and (optionally) config overrides:

```bash
X402_RECEIVER_ADDRESS=0xwallet-that-receives-fees
X402_FACILITATOR_URL=https://x402.org/facilitator
X402_DEFAULT_ASSET=
X402_DEFAULT_NETWORK=
X402_DEFAULT_SCHEME=exact
X402_DEFAULT_AMOUNT_USDC=
X402_PAYWALL_APP_NAME=SpoonOS Agent Services
X402_PAYWALL_APP_LOGO=https://your-domain.example/logo.png
X402_DEMO_URL=https://www.x402.org/protected
```

Key points:

- The system always prefers the local `PRIVATE_KEY`. If that variable is empty and Turnkey credentials (`TURNKEY_*`) exist, SpoonOS transparently switches to hosted signing.
- In CLI workflows (spoon-cli or the legacy `main.py` CLI), the `x402` block in the CLI `config.json` mirrors these defaults (branding, description, timeout, etc.). Update that file when you need per-environment variance. The core SDK still reads values from environment variables.
- Setting `X402_DEFAULT_ASSET` ensures all typed-data domains reference the real USDC contract so signatures pass facilitator validation.

## Runtime lifecycle

```mermaid
flowchart TD
    A[User task -> SpoonReact agent] --> B[Step 1: http_probe tool<br/>Unauthenticated probe of paywalled URL]
    B -->|HTTP 200| J[Step 4: Agent parses body/headers<br/>No payment required]
    B -->|HTTP 402| C[Step 2: x402_paywalled_request tool<br/>Parse paywall requirements]
    C --> D[Merge paywall + config overrides]
    D --> E[Signer selection<br/>PRIVATE_KEY preferred, Turnkey fallback]
    E --> F[Typed-data build<br/>TransferWithAuthorization payload]
    F --> G[Signature via eth_account or Turnkey]
    G --> H[Encode header -> X-PAYMENT]
    H --> I[Step 3: Paid retry with X-PAYMENT header]
    I --> P1

    subgraph "Paywall server (FastAPI /x402)"
        P1[Incoming request carrying X-PAYMENT] --> P2[verify_payment -> Facilitator API]
        P2 --> P3{Valid payment?}
        P3 -- No --> P4[Return HTTP 402 + error]
        P3 -- Yes --> P5[Optional settle_payment]
        P5 --> P6[Invoke agent/tooling]
        P6 --> P7[Return HTTP 200 + X-PAYMENT-RESPONSE]
    end

    P4 -.-> C
    P7 --> J
    J --> K[Memory/log update + ReAct summary output]
```

If the paid retry fails (for example `verify_payment` rejects the header or the facilitator reports an error), the paywall server immediately returns another `402` or error payload and the agent decides whether to run `x402_paywalled_request` again with corrected parameters. A successful verification moves straight into settlement and target agent execution, so there is no additional retry cycle once the `X-PAYMENT` header is accepted.

## Operational checklist

1. Use [https://faucet.circle.com/](https://faucet.circle.com/) to mint 0.01 USDC for the public demo.
2. Keep `X402_RECEIVER_ADDRESS` aligned with the wallet that ultimately receives settlements.
3. Monitor facilitator responses. Any `invalid_exact_evm_payload_signature` errors typically mean the `asset`, `chainId`, or nonce encoding no longer matches the paywall challenge.
4. Use `X402PaymentService.decode_payment_response(header)` to archive payment receipts in logs or analytics pipelines.

---

FILE: docs/examples/graph-crypto-analysis.md

---
sidebar_position: 2
---

# Graph Crypto Analysis

This example implements a complete cryptocurrency research and analysis pipeline using the declarative graph building system, demonstrating end-to-end LLM-driven decision making for market analysis and investment recommendations.

#### ðŸ“Š **Workflow Diagram**

```mermaid
graph TD
    A[Start] --> B[Fetch Binance Market Data]
    B --> C[Select Top 10 Pairs by Volume]
    C --> D[Prepare Token List]

    D --> E[Parallel Token Analysis]
    E --> F1[Token 1: Technical + News Analysis]
    E --> F2[Token 2: Technical + News Analysis]
    E --> F3[Token 3: Technical + News Analysis]
    E --> F4[Token 4: Technical + News Analysis]
    E --> F5[Token 5: Technical + News Analysis]
    E --> F6[Token 6: Technical + News Analysis]
    E --> F7[Token 7: Technical + News Analysis]
    E --> F8[Token 8: Technical + News Analysis]
    E --> F9[Token 9: Technical + News Analysis]
    E --> F10[Token 10: Technical + News Analysis]

    F1 --> G[Aggregate All Results]
    F2 --> G
    F3 --> G
    F4 --> G
    F5 --> G
    F6 --> G
    F7 --> G
    F8 --> G
    F9 --> G
    F10 --> G

    G --> H[LLM Final Aggregation]
    H --> I[Generate Market Report]
    I --> J[END]

    style A fill:#e1f5fe
    style J fill:#c8e6c9
    style E fill:#fff3e0
    style G fill:#fce4ec

    subgraph "Technical Analysis"
        F1
        F2
        F3
        F4
        F5
        F6
        F7
        F8
        F9
        F10
    end
```

#### ðŸŽ¯ **Core Features**

**Intelligent Market Analysis:**
- LLM-driven token selection based on real-time market conditions
- Multi-timeframe analysis (1h, 4h) for comprehensive market view
- Dynamic decision flow guided by LLM analysis at each step

**Advanced Technical Analysis:**
- Real-time indicator calculation (RSI, MACD, EMA) using PowerData toolkit
- Market sentiment analysis and momentum evaluation
- Risk assessment and volatility metrics for each token

**LLM-Powered Synthesis:**
- Intelligent summarization of complex market data
- Data-driven investment recommendations with reasoning
- Short-term and macro-level market outlook generation

#### ðŸš€ **Key Capabilities**

- **Declarative Graph Building** - `GraphTemplate`, `NodeSpec`, `EdgeSpec` for modular workflows
- **High-Level API Integration** - `HighLevelGraphAPI` for automatic parameter inference
- **Complete Workflow** - End-to-end from data ingestion to final recommendations
- **Real API Integration** - Live Binance and cryptocurrency data via PowerData toolkit
- **LLM Decision Making** - Every major decision guided by LLM analysis
- **Advanced State Management** - Complex analysis state throughout the process
- **Error Recovery** - Robust error handling and fallback mechanisms

#### ðŸ“‹ **Prerequisites**

```bash
# Required environment variables
export OPENAI_API_KEY="your-openai-api-key"          # Primary LLM
export ANTHROPIC_API_KEY="your-anthropic-api-key"   # Alternative LLM
export TAVILY_API_KEY="your-tavily-api-key"       # Search engine
```

#### ðŸƒ **Quick Start**

```bash
# Navigate to examples directory
cd spoon-core/example

# Install dependencies
pip install -r requirements.txt

# Run the declarative crypto analysis
python graph_crypto_analysis.py
```

#### ðŸ” **What to Observe**

**Architecture:**
- How `GraphTemplate` and `NodeSpec` simplify workflow construction
- `HighLevelGraphAPI` automatically inferring parameters from queries
- Modular node implementations with better separation of concerns

**Data Flow:**
- Real market data fetching from Binance API and PowerData toolkit
- LLM analysis of raw data for intelligent decision making
- Step-by-step process from data collection to final recommendations

**Technical Analysis:**
- Real-time indicator calculation using PowerData toolkit
- Correlation of different data sources
- Market sentiment analysis and quantification

**LLM Decision Process:**
- Token evaluation and selection for analysis
- Synthesis combining technical and fundamental analysis
- Investment recommendations with detailed reasoning

#### ðŸ“Š **Sample Output**

```
ðŸ” MARKET ANALYSIS REPORT
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ðŸ“ˆ SELECTED TOKENS FOR ANALYSIS: BTC, ETH, SOL, ADA

ðŸ“Š TECHNICAL ANALYSIS:
â€¢ BTC/USDT: Bullish momentum, RSI: 68, MACD positive crossover
â€¢ ETH/USDT: Consolidation phase, approaching key resistance
â€¢ SOL/USDT: Strong uptrend, breaking previous highs
â€¢ ADA/USDT: Recovery phase, positive volume momentum

ðŸŽ¯ INVESTMENT RECOMMENDATIONS:
â€¢ SHORT-TERM: Consider BTC and SOL for momentum plays
â€¢ MEDIUM-TERM: Hold ETH through current consolidation
â€¢ RISK ASSESSMENT: Moderate volatility expected in next 24-48 hours

ðŸ’¡ MARKET OUTLOOK:
The current market shows strong bullish momentum with BTC leading...
```

#### ðŸ“ **Source Code**

- **Main Example**: [graph_crypto_analysis.py](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py)
- **Supporting Modules**:
  - `spoon_ai/graph/builder.py` - Declarative templates and high-level API
  - `spoon_ai/tools/crypto_tools.py` - PowerData integration helpers
  - `spoon_ai/graph/` - Core engine and monitoring utilities
  - [Tool System Docs](../core-concepts/tools.md)

#### ðŸŽ“ **Learning Outcomes**

- Using declarative graph building (`GraphTemplate`, `NodeSpec`, `EdgeSpec`)
- Leveraging `HighLevelGraphAPI` for automatic parameter inference
- Implementing modular, maintainable node functions
- Building complete end-to-end analysis systems with LLM integration
- Advanced cryptocurrency market analysis techniques
- Real-time data processing and technical indicator calculation
- LLM-driven decision making in complex workflows
- Error handling and data validation in financial applications

#### ðŸ’¡ **Best Practices**

- Declarative architecture for improved modularity
- High-level API usage for automatic parameter inference
- Data validation and comprehensive error handling
- Performance optimization and efficient data processing
- Security considerations for API keys and financial data
- Modular architecture with clean separation of concerns

---

FILE: docs/examples/intent-graph-demo.md

---
sidebar_position: 1
---


# Intent Graph Demo

This example demonstrates an intelligent StateGraph workflow with advanced query routing, parallel execution, and memory management using the modern declarative graph building system.

#### ðŸ“Š **Workflow Diagram**

```mermaid
graph TD
    A[User Query] --> B[Bootstrap Session]
    B --> C[Load Memory]
    C --> D[Plan Analysis]
    D --> E{LLM Intent Analysis}
    E -->|general_qa| F[General Q&A]
    E -->|short_term_trend| G[Extract Symbol]
    E -->|macro_trend| H[Extract Symbol]
    E -->|deep_research| I[Deep Research Search]

    G --> J[Short-term Data Collection]
    H --> K[Macro Data Collection]
    I --> L[Research Sources]

    J --> M[Short-term Summary]
    K --> N[Macro Summary]
    L --> O[Research Report]

    M --> P[Review Trade]
    N --> P
    O --> P
    F --> P

    P --> Q[Update Memory]
    Q --> R[Finalize Response]
    R --> S[END]

    style A fill:#e1f5fe
    style S fill:#c8e6c9
    style E fill:#fff3e0
    style P fill:#fce4ec
```

#### ðŸŽ¯ **Core Features**

**Intelligent Query Routing:**
- LLM-powered intent classification into: `general_qa`, `short_term_trend`, `macro_trend`, or `deep_research`
- Dynamic routing based on detected intent and conversation history
- Context-aware decision making with market context

**Parallel Data Processing:**
- Concurrent data fetching across multiple timeframes (15m, 30m, 1h, 4h, daily, weekly)
- Real-time cryptocurrency data integration
- Performance optimization through parallel execution

**Advanced Memory Management:**
- Persistent conversation context across sessions
- Automatic storage of learned patterns and market insights
- State preservation for analysis results and routing decisions

#### ðŸš€ **Key Capabilities**

- **Declarative Graph Building** - `GraphTemplate`, `NodeSpec`, `EdgeSpec` for modular workflows
- **High-Level API Integration** - `HighLevelGraphAPI` for automatic parameter inference
- **LLM Integration** - Advanced prompt engineering and response processing
- **Tool Orchestration** - Multi-source data integration (PowerData, Tavily, EVM swap)
- **Error Handling** - Robust recovery with duplicate log prevention
- **Performance Monitoring** - Built-in metrics and execution tracking

#### ðŸ“‹ **Prerequisites**

```bash
# Required environment variables
export OPENAI_API_KEY="your-openai-api-key"
export TAVILY_API_KEY="your-tavily-api-key"       # Search engine
```

#### ðŸƒ **Quick Start**

```bash
# Navigate to examples directory
cd spoon-core/example

# Install dependencies
pip install -r requirements.txt

# Run the declarative intent graph demo
python intent_graph_demo.py
```

#### ðŸ” **What to Observe**

**Architecture:**
- How `GraphTemplate` and `NodeSpec` simplify workflow construction
- `HighLevelGraphAPI` automatically inferring parameters from queries
- Modular node implementations with better separation of concerns

**Execution Flow:**
- Intelligent routing to appropriate analysis paths based on query intent
- Parallel data fetching across multiple timeframes
- Memory loading and updates throughout the process

**Performance:**
- Execution times for different routing paths
- Parallel vs sequential processing performance
- Memory usage optimization and duplicate log prevention

**Advanced Behaviors:**
- LLM-powered routing decisions based on intent analysis
- Real-time data integration from multiple sources
- Context maintenance across complex workflows

#### ðŸ“ **Source Code**

- **Main Example**: [intent_graph_demo.py](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py)
- **Supporting Modules**:
  - `spoon_ai/graph/` - Core graph system and declarative builders
  - `spoon_ai/graph/builder.py` - High-level API and parameter inference
  - [Graph System Docs](../core-concepts/graph-system.md)

#### ðŸŽ“ **Learning Outcomes**

- Using declarative graph building (`GraphTemplate`, `NodeSpec`, `EdgeSpec`)
- Leveraging `HighLevelGraphAPI` for automatic parameter inference
- Implementing modular, maintainable node functions
- Advanced LLM integration and prompt engineering
- Parallel processing for performance optimization
- Memory management in long-running processes
- Error handling and recovery strategies

#### ðŸ’¡ **Best Practices**

- Declarative architecture for improved modularity
- High-level API usage for automatic parameter inference
- Scalable design for easy extension
- Resource-efficient implementation
- Maintainable, well-documented code

---

FILE: docs/examples/mcp-spoon-search-agent.md

---
sidebar_position: 3
---

# MCP Spoon Search Agent

This example demonstrates how to build an MCP (Model Context Protocol) enabled agent that seamlessly integrates web search capabilities with cryptocurrency analysis tools, creating a powerful research and analysis assistant.

#### ðŸŽ¯ **Core Functionality**

**Intelligent Web Search Integration:**
- **Tavily MCP integration** - Advanced web search capabilities through the Model Context Protocol
- **Real-time information retrieval** - Access to current news, articles, and market data from across the web
- **Context-aware search** - Searches are guided by user intent and current market context

**Cryptocurrency Analysis Tools:**
- **Crypto PowerData integration** - Professional-grade cryptocurrency market data and analysis
- **Multi-exchange support** - Access to data from major exchanges (Binance, Coinbase, etc.)
- **Technical indicators** - Real-time calculation of RSI, MACD, EMA, and other key indicators

**Unified Analysis System:**
- **Cross-referenced insights** - Combines web search results with technical analysis
- **Macro market analysis** - Provides comprehensive market outlook by correlating multiple data sources
- **Intelligent synthesis** - LLM-powered synthesis of diverse information sources into coherent analysis

#### ðŸš€ **Key Features Demonstrated**

- **MCP Protocol Implementation** - Complete MCP server integration and tool discovery
- **Multi-tool Orchestration** - Seamless coordination between search and analysis tools
- **Real-time Data Processing** - Live data integration from multiple APIs
- **Advanced Error Handling** - Robust error recovery and fallback mechanisms
- **Modular Architecture** - Clean separation between MCP tools and analysis logic

#### ðŸ“‹ **Prerequisites**

```bash
# Required environment variables
export TAVILY_API_KEY="your-tavily-api-key"        # Web search API
export OPENAI_API_KEY="your-openai-api-key"        # LLM responses
export ANTHROPIC_API_KEY="your-anthropic-api-key"  # Alternative LLM

# System requirements
npm install -g tavily-mcp  # Install Tavily MCP server
npx --version              # Ensure npx is available
```

#### ðŸƒ **Quick Start**

```bash
# Navigate to examples directory
cd spoon-core/example

# Install dependencies
pip install -r requirements.txt

# Run the MCP search agent
python spoon_search_agent.py
```

#### ðŸ” **What to Observe**

**MCP Tool Discovery:**
- Watch how the system automatically discovers and connects to MCP servers
- Observe the dynamic tool loading process
- See how tools are validated and initialized

**Search-Analysis Integration:**
- Monitor how web search results are combined with market data
- Observe the correlation between news sentiment and technical indicators
- Track how the system synthesizes diverse information sources

**Real-time Processing:**
- See live data fetching from both web sources and crypto exchanges
- Watch the real-time analysis and recommendation generation
- Observe how the system handles API rate limits and errors

#### ðŸ“Š **Analysis Output Example**

```
ðŸ” COMPREHENSIVE MARKET ANALYSIS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ðŸ“° LATEST MARKET NEWS:
â€¢ Federal Reserve signals potential rate cut in Q4 2024
â€¢ Bitcoin ETF inflows reach record $2.1B this week
â€¢ Ethereum staking rewards hit 7.2% APY
â€¢ Major tech companies announce crypto payment integration

ðŸ“Š TECHNICAL ANALYSIS:
â€¢ BTC/USDT: Breaking above $45K resistance, volume spike detected
â€¢ ETH/USDT: Testing $2,800 support level, RSI showing oversold conditions
â€¢ Market-wide momentum: Bullish divergence across major altcoins

ðŸŽ¯ INVESTMENT INSIGHTS:
â€¢ SHORT-TERM: Bullish momentum favors BTC accumulation
â€¢ MEDIUM-TERM: ETH showing strong fundamental support
â€¢ RISK FACTORS: Monitor Federal Reserve policy decisions

ðŸ’¡ MARKET SENTIMENT:
Overall market sentiment is cautiously optimistic with strong institutional...
```

#### ðŸ“ **Source Code & Documentation**

- **GitHub Link**: [MCP Spoon Search Agent](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)
- **Related Files**:
  - `spoon-core/examples/mcp/spoon_search_agent.py` - Core MCP implementation
  - `spoon-core/spoon_ai/tools/mcp_tools.py` - MCP tool integration
  - `docs/core-concepts/mcp-protocol.md` - MCP protocol documentation

#### ðŸŽ“ **Learning Objectives**

This example teaches you:
- How to integrate MCP (Model Context Protocol) servers into your agents
- Advanced multi-tool orchestration and data synthesis techniques
- Real-time web search integration with LLM-powered analysis
- Error handling and recovery in distributed tool systems
- Building research assistants that combine multiple data sources

#### ðŸ’¡ **Best Practices Demonstrated**

- **MCP Server Management** - Proper initialization and error handling for MCP servers
- **Tool Discovery** - Dynamic tool loading and validation
- **Data Correlation** - Effective synthesis of diverse information sources
- **API Rate Limiting** - Intelligent handling of API limitations and quotas
- **Fallback Mechanisms** - Robust error recovery when tools or APIs are unavailable

---

FILE: docs/examples/x402-react-agent.md

# x402 Agent

This walkthrough mirrors `core/examples/x402_agent_demo.py`, which shows a SpoonReact agent autonomously paying the official x402 paywall before summarising the protected content.

## Prerequisites

1. Install SpoonOS core dependencies (`uv pip install -r requirements.txt`).
2. Configure `core/.env` with:
   - `OPENAI_API_KEY`
   - `PRIVATE_KEY` (0x-prefixed; must hold â‰¥0.01 USDC). If omitted, set the Turnkey variables documented in the API reference.
   - `X402_RECEIVER_ADDRESS` (usually matches the private key address).
   - Optional: `X402_FACILITATOR_URL`, `X402_DEFAULT_NETWORK`, `X402_DEMO_URL`.
3. Acquire USDC Testnet Token (0.01 is enough) via [https://faucet.circle.com/](https://faucet.circle.com/).

## Run the demo

```bash
uv run python examples/x402_agent_demo.py
```

What happens:

1. The script prints signer details and the target resource (`https://www.x402.org/protected` by default).
2. A `SpoonReactAI` instance performs a ReAct loop:
   - Calls `http_probe` (no payment) to capture the 402 challenge.
   - Calls `x402_paywalled_request` to sign and submit a 0.01 USDC payment.
   - Retrieves the protected payload (a SoundCloud embed) after settlement.
3. The console logs tool traces, the signed `X-PAYMENT` header, and the decoded settlement receipt (transaction hash, payer, network).

## Troubleshooting

| Symptom | Likely cause | Fix |
| --- | --- | --- |
| `invalid_exact_evm_payload_signature` | Asset/network mismatch or stale nonce. | Ensure you copied the paywall's `asset` and `pay_to` fields, and confirm `PRIVATE_KEY` funds exist. |
| `Configuration error: API key is required for provider ...` | Missing `OPENAI_API_KEY`. | Export a valid LLM key before running the demo. |
| `x402 configuration error: Turnkey signing identity missing` | `X402_USE_TURNKEY` enabled but no `TURNKEY_SIGN_WITH`. | Provide the required Turnkey identifiers or disable Turnkey by setting `X402_USE_TURNKEY=0`. |

## Next steps

- Automate the payment header generation inside your own agents by reusing the `x402_paywalled_request` tool documented in the API reference.
- Expose your agents via the paywall router (`python -m spoon_ai.payments.app`) so external callers must submit verified x402 payments before invoking them.

---

FILE: docs/getting-started/configuration.md

# Configuration

SpoonOS is **env-first**. The core Python SDK only reads environment variables (including values from a `.env` file). The `spoon-cli` workflow is the only place `config.json` is read; the CLI loads that file and exports the values into the environment before starting agents.

## Configuration Priority

At runtime (latest wins):

1. Built-in defaults in the SDK  
2. Environment variables (`.env` or shell)  
3. Values exported by `spoon-cli` from `config.json` (CLI only)

## Environment Variables

Create a `.env` file in your project root:

```bash
# LLM Provider API Keys (set at least one)
GEMINI_API_KEY=your_gemini_key_here        # recommended for Quick Start
OPENAI_API_KEY=your_openai_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here
DEEPSEEK_API_KEY=your_deepseek_key_here
OPENROUTER_API_KEY=your_openrouter_key_here

# Optional: Default LLM Settings
DEFAULT_LLM_PROVIDER=gemini                # or openai / anthropic / deepseek / openrouter
DEFAULT_MODEL=gemini-2.5-pro
GEMINI_MAX_TOKENS=20000                    # recommended context limit for Gemini

# Web3 Configuration (only needed for on-chain tools)
WEB3_PROVIDER_URL=https://mainnet.infura.io/v3/your_project_id
PRIVATE_KEY=your_private_key_here
```

## CLI Configuration File (optional)

If you use `spoon-cli`, manage CLI-specific settings in `config.json`. The CLI exports that file into environment variables automatically; the SDK does **not** read it directly. See `docs/cli/configuration.md` for the full schema and commands.

## API Key Setup

### OpenAI
1. Visit [OpenAI API Keys](https://platform.openai.com/api-keys)
2. Create a new API key
3. Add to your `.env` file

### Anthropic
1. Visit [Anthropic Console](https://console.anthropic.com/)
2. Generate an API key
3. Add to your `.env` file

### Google (Gemini)
1. Visit [Google AI Studio](https://aistudio.google.com/app/apikey)
2. Create an API key
3. Add to your `.env` file

## Verification

Test your configuration:

```bash
python -c "from spoon_ai.utils.config_manager import ConfigManager; print('âœ… Configuration loaded successfully')"
```

The framework automatically validates your configuration and provides helpful error messages if any issues are detected.

## Next Steps

- [Quick Start](./quick-start.md) - Build your first agent
- [Core Concepts](../core-concepts/agents.md) - Learn about agents

---

FILE: docs/getting-started/installation.md

# Installation

## Prerequisites

- Python 3.12 or higher
- Git
- Virtual environment (recommended)

## Quick Installation

### Option A: Install from PyPI (recommended)

You can use the published PyPI packages without cloning the repository:

1. Create and activate a virtual environment

```bash
# macOS/Linux
python3 -m venv spoon-env
source spoon-env/bin/activate

# Windows (PowerShell)
python -m venv spoon-env
.\spoon-env\Scripts\Activate.ps1
```

2. Install the core SDK (and optionally the toolkits package)

```bash
pip install spoon-ai-sdk        # core framework
pip install spoon-toolkits      # optional: extended blockchain & data toolkits
```

### Option B: Use a local repository checkout

If you are working inside this monorepo (for example you already opened it in your IDE), you can install directly from the local folders without needing to `git clone` again.

1. Create Virtual Environment

```bash
# macOS/Linux
python3 -m venv spoon-env
source spoon-env/bin/activate

# Windows (PowerShell)
python -m venv spoon-env
.\spoon-env\Scripts\Activate.ps1
```

> ðŸ’¡ On newer Apple Silicon Macs the `python` shim may not point to Python 3.
> Use `python3` for all commands unless you have explicitly configured `python`
> to target Python 3.12 or later.

2. Install core package in editable mode

```bash
git clone https://github.com/XSpoonAi/spoon-core.git
cd spoon-core
pip install -e .
```

3. (Optional) Install Toolkits Package from local repo

If you want to use the extended blockchain and data tools from `spoon_toolkits`, install the **spoon-toolkits** package from the `spoon-toolkits` folder:

```bash
git clone https://github.com/XSpoonAi/spoon-toolkits.git
cd spoon-toolkits
pip install -e .
```

## Framework Validation

The SpoonOS framework includes built-in validation that automatically:

- Checks API key configuration
- Validates provider connectivity
- Ensures proper dependency installation
- Provides clear error messages if issues are found

## Next Steps

- [Configuration](./configuration.md) - Set up API keys and configuration
- [Quick Start](./quick-start.md) - Build your first agent

---

FILE: docs/getting-started/quick-start.md

# Quick Start

Get up and running with SpoonOS framework in under 5 minutes.

## Prerequisites

- [Installation](./installation.md) completed
- [Configuration](./configuration.md) set up with at least one provider API key (for example `OPENAI_API_KEY`)

## Your First Agent

### 1. Create a Simple Agent

Create a new Python file `my_first_agent.py`:

```python
import asyncio
from spoon_ai.agents.toolcall import ToolCallAgent
from spoon_ai.chat import ChatBot
from spoon_ai.tools import ToolManager
from spoon_ai.tools.base import BaseTool

# Define a custom tool
class GreetingTool(BaseTool):
    name: str = "greeting"
    description: str = "Generate personalized greetings"
    parameters: dict = {
        "type": "object",
        "properties": {
            "name": {"type": "string", "description": "Person's name"}
        },
        "required": ["name"]
    }

    async def execute(self, name: str) -> str:
        return f"Hello {name}! Welcome to SpoonOS! ðŸš€"

# Create your agent
class MyFirstAgent(ToolCallAgent):
    name: str = "my_first_agent"
    description: str = "A friendly assistant with greeting capabilities"

    system_prompt: str = """
    You are a helpful AI assistant built with SpoonOS framework.
    You can greet users and help with various tasks.
    """

    available_tools: ToolManager = ToolManager([GreetingTool()])

async def main():
    # Initialize agent with LLM
    agent = MyFirstAgent(
        llm=ChatBot(
            llm_provider="openai",         # or "anthropic", "gemini", "deepseek", "openrouter"
            model_name="gpt-5.1"   # Framework default for OpenAI
        )
    )

    # Run the agent - framework handles all error cases automatically
    response = await agent.run("Please greet me, my name is Alice")
    return response

if __name__ == "__main__":
    result = asyncio.run(main())
    # Agent response will be returned directly
```

### 2. Run Your Agent

```bash
python my_first_agent.py
```

The agent will respond with a personalized greeting and offer to help with various tasks.

### 3. Add Web3 Capabilities

Enhance your agent with blockchain tools:

```python
from spoon_ai.tools.crypto_tools import get_crypto_tools

class Web3Agent(ToolCallAgent):
    name: str = "web3_agent"
    description: str = "AI agent with Web3 and crypto capabilities"

    system_prompt: str = """
    You are a Web3-native AI assistant with access to blockchain data.
    You can help with crypto prices, DeFi operations, and blockchain analysis.
    """

    available_tools: ToolManager = ToolManager([
        GreetingTool(),
        # Loads all crypto/Web3 tools from spoon-toolkits (requires `pip install -e spoon-toolkits`)
        *get_crypto_tools()
    ])

# Usage
async def web3_demo():
    agent = Web3Agent(
        llm=ChatBot(
            llm_provider="anthropic",
            model_name="claude-sonnet-4-20250514"  # Framework default
        )
    )

    # Framework automatically handles crypto data fetching and error cases
    response = await agent.run("What's the current price of Bitcoin?")
    return response
```

### 4. Framework Features Overview

The SpoonOS framework provides:

- **Multiple LLM Providers**: OpenAI (`openai`), Anthropic (`anthropic`), Google Gemini (`gemini`), DeepSeek (`deepseek`), OpenRouter (`openrouter`)
- **Built-in Tools**: Crypto, DeFi, social media, data analysis
- **Agent Types**: ReAct, ToolCall, Graph-based agents
- **MCP Integration**: Dynamic tool discovery and execution

### Framework Simplicity

SpoonOS eliminates common development complexity:

```python
# Simple agent creation - no error handling needed
agent = ToolCallAgent(
    llm=ChatBot(llm_provider="openai", model_name="gpt-4.1"),
    available_tools=ToolManager([CryptoTool(), Web3Tool()])
)


response = await agent.run("Analyze Bitcoin trends and suggest trades")
```

## Framework Development Patterns

### Agent Composition

```python
# Combine multiple agents for complex workflows
from spoon_ai.agents.graph import GraphAgent

class MultiAgentSystem(GraphAgent):
    def __init__(self):
        super().__init__()
        self.add_agent("researcher", ResearchAgent())
        self.add_agent("analyst", AnalysisAgent())
        self.add_agent("trader", TradingAgent())
```

### Custom Tool Development

```python
# Create domain-specific tools
class BlockchainAnalysisTool(BaseTool):
    name: str = "blockchain_analysis"
    description: str = "Analyze blockchain transactions and patterns"

    async def execute(self, address: str, chain: str = "ethereum") -> str:
        # Your custom blockchain analysis logic
        return f"Analysis results for {address} on {chain}"
```

### MCP Integration

```python
# Use Model Context Protocol for dynamic tools
from spoon_ai.tools.mcp_tools_collection import MCPToolsCollection

agent = ToolCallAgent(
    llm=ChatBot(llm_provider="anthropic", model_name="claude-sonnet-4-20250514"),
    available_tools=ToolManager([
        MCPToolsCollection()  # Automatically discovers MCP tools
    ])
)
```

## Next Steps

Now that you understand the framework basics:

- [Core Concepts](../core-concepts/agents.md) - Deep dive into agent architecture
- [Built-in Tools](../core-concepts/tools.md) - Explore Web3 and crypto tools
- [How-To Guides](../how-to-guides/build-first-agent.md) - Advanced agent patterns

---

FILE: docs/how-to-guides/add-custom-tools.md

# Adding Custom Tools to SpoonOS

This guide shows you how to create and integrate custom tools into the SpoonOS framework. Tools extend agent capabilities by providing specific functionality like API integrations, data processing, or blockchain interactions.

## Tool Architecture Overview

SpoonOS uses a modular tool system where:

- **BaseTool**: Abstract base class defining the tool interface
- **ToolManager**: Manages tool collections and execution
- **MCP Integration**: Exposes tools via Model Context Protocol
- **Dynamic Loading**: Tools can be added at runtime

## Creating a Basic Tool

### Step 1: Define Your Tool Class

Create a new tool by inheriting from `BaseTool`:

```python
from spoon_ai.tools.base import BaseTool, ToolResult
from typing import Any, Dict

class MyCustomTool(BaseTool):
    name: str = "my_custom_tool"
    description: str = "A custom tool that processes data"
    parameters: dict = {
        "type": "object",
        "properties": {
            "input_data": {
                "type": "string",
                "description": "The data to process"
            },
            "options": {
                "type": "object",
                "description": "Optional processing parameters",
                "properties": {
                    "format": {"type": "string", "default": "json"}
                }
            }
        },
        "required": ["input_data"]
    }

    async def execute(self, input_data: str, options: Dict[str, Any] = None) -> ToolResult:
        """Execute the tool logic - framework handles errors automatically"""
        # Your tool logic here
        processed_data = self.process_data(input_data, options or {})

        return ToolResult(
            output=processed_data,
            system=f"Successfully processed {len(input_data)} characters"
        )

    def process_data(self, data: str, options: Dict[str, Any]) -> str:
        """Your custom processing logic"""
        # Example: simple data transformation
        format_type = options.get("format", "json")
        if format_type == "uppercase":
            return data.upper()
        return f'{{"processed": "{data}"}}'
```

### Step 2: Tool Parameters Schema

The `parameters` field defines the JSON schema for tool inputs:

```python
parameters: dict = {
    "type": "object",
    "properties": {
        "required_param": {
            "type": "string",
            "description": "A required parameter"
        },
        "optional_param": {
            "type": "integer",
            "description": "An optional parameter",
            "default": 42
        },
        "enum_param": {
            "type": "string",
            "enum": ["option1", "option2", "option3"],
            "description": "Choose from predefined options"
        }
    },
    "required": ["required_param"]
}
```

## Advanced Tool Examples

### API Integration Tool

```python
import aiohttp
from spoon_ai.tools.base import BaseTool, ToolResult

class APITool(BaseTool):
    name: str = "api_fetcher"
    description: str = "Fetches data from external APIs"
    parameters: dict = {
        "type": "object",
        "properties": {
            "url": {"type": "string", "description": "API endpoint URL"},
            "method": {"type": "string", "enum": ["GET", "POST"], "default": "GET"},
            "headers": {"type": "object", "description": "HTTP headers"}
        },
        "required": ["url"]
    }

    async def execute(self, url: str, method: str = "GET", headers: dict = None) -> ToolResult:
        # Framework provides automatic error handling and retry logic
        async with aiohttp.ClientSession() as session:
            async with session.request(method, url, headers=headers) as response:
                data = await response.json()
                return ToolResult(
                    output=data,
                    system=f"API call successful: {response.status}"
                )
```

### Blockchain Tool Example

```python
from web3 import Web3
from spoon_ai.tools.base import BaseTool, ToolResult

class BlockchainTool(BaseTool):
    name: str = "get_eth_balance"
    description: str = "Gets Ethereum balance for an address"
    parameters: dict = {
        "type": "object",
        "properties": {
            "address": {
                "type": "string",
                "description": "Ethereum address to check"
            },
            "network": {
                "type": "string",
                "enum": ["mainnet", "goerli", "sepolia"],
                "default": "mainnet"
            }
        },
        "required": ["address"]
    }

    def __init__(self):
        super().__init__()
        self.w3 = Web3(Web3.HTTPProvider("https://eth-mainnet.alchemyapi.io/v2/YOUR_KEY"))

    async def execute(self, address: str, network: str = "mainnet") -> ToolResult:
        # Framework handles validation and error cases automatically
        if not self.w3.is_address(address):
            return ToolResult(error="Invalid Ethereum address")

        balance_wei = self.w3.eth.get_balance(address)
        balance_eth = self.w3.from_wei(balance_wei, 'ether')

        return ToolResult(
            output={
                "address": address,
                "balance_eth": str(balance_eth),
                "balance_wei": str(balance_wei),
                "network": network
            }
        )
```

## Integrating Tools

### Method 1: Add to Tool Manager

```python
from spoon_ai.tools.tool_manager import ToolManager
from your_module import MyCustomTool

# Create tool manager with existing tools
tool_manager = ToolManager([])

# Add your custom tool
custom_tool = MyCustomTool()
tool_manager.add_tool(custom_tool)

# Or add multiple tools at once
tool_manager.add_tools(
    MyCustomTool(),
    APITool(),
    BlockchainTool()
)
```

### Method 2: Create Tool Collection

```python
# tools/my_tools.py
from typing import List
from spoon_ai.tools.base import BaseTool
from .my_custom_tool import MyCustomTool
from .api_tool import APITool

def get_my_tools() -> List[BaseTool]:
    """Return collection of custom tools"""
    return [
        MyCustomTool(),
        APITool(),
        # Add more tools here
    ]

def create_my_tool_manager() -> ToolManager:
    """Create tool manager with custom tools"""
    from spoon_ai.tools.tool_manager import ToolManager
    return ToolManager(get_my_tools())
```

### Method 3: MCP Integration

```python
# mcp_server.py
from fastmcp import FastMCP
from spoon_ai.tools.tool_manager import ToolManager
from your_tools import get_my_tools

mcp = FastMCP("My Custom Tools")

# Add tools to MCP server
tools = get_my_tools()
tool_manager = ToolManager(tools)

for tool in tools:
    mcp.add_tool(
        tool.execute,
        name=tool.name,
        description=tool.description
    )

if __name__ == "__main__":
    import asyncio
    asyncio.run(mcp.run_async(transport="sse", port=8766))
```

## Tool Configuration

### Environment Variables

```python
import os
from spoon_ai.tools.base import BaseTool, ToolResult

class ConfigurableTool(BaseTool):
    name: str = "configurable_tool"
    description: str = "Tool that uses environment configuration"

    def __init__(self):
        super().__init__()
        self.api_key = os.getenv("MY_API_KEY")
        self.base_url = os.getenv("MY_API_URL", "https://api.example.com")

        if not self.api_key:
            raise ValueError("MY_API_KEY environment variable required")

    async def execute(self, query: str) -> ToolResult:
        # Use self.api_key and self.base_url
        pass
```

### Configuration Class

```python
from pydantic import BaseModel
from typing import Optional

class ToolConfig(BaseModel):
    api_key: str
    base_url: str = "https://api.example.com"
    timeout: int = 30
    retries: int = 3

class ConfigurableTool(BaseTool):
    def __init__(self, config: ToolConfig):
        super().__init__()
        self.config = config

    async def execute(self, **kwargs) -> ToolResult:
        # Use self.config.api_key, etc.
        pass
```

## Error Handling Best Practices

### Framework Error Handling

```python
async def execute(self, **kwargs) -> ToolResult:
    # Framework provides automatic input validation and error handling
    if not kwargs.get("required_param"):
        return ToolResult(error="Missing required parameter")

    # Execute tool logic - framework handles network errors, timeouts, etc.
    result = await self.do_work(**kwargs)

    return ToolResult(
        output=result,
        system="Operation completed successfully"
    )
```

### Framework Monitoring

```python
from spoon_ai.tools.base import BaseTool, ToolResult

class MonitoredTool(BaseTool):
    async def execute(self, **kwargs) -> ToolResult:
        # Framework provides automatic logging and monitoring
        result = await self.do_work(**kwargs)

        # Framework tracks:
        # - Execution time and performance metrics
        # - Success/failure rates
        # - Parameter usage patterns
        # - Error frequencies and types
        return ToolResult(output=result)
```

## Testing Your Tools

### Unit Testing

```python
import pytest
from your_tools import MyCustomTool

@pytest.mark.asyncio
async def test_my_custom_tool():
    tool = MyCustomTool()

    # Test successful execution
    result = await tool.execute(input_data="test data")
    assert result.output is not None
    assert result.error is None

    # Test error handling
    result = await tool.execute(input_data="")
    assert result.error is not None

@pytest.mark.asyncio
async def test_tool_parameters():
    tool = MyCustomTool()

    # Test with optional parameters
    result = await tool.execute(
        input_data="test",
        options={"format": "uppercase"}
    )
    assert "TEST" in result.output
```

### Integration Testing

```python
from spoon_ai.tools.tool_manager import ToolManager
from your_tools import MyCustomTool

@pytest.mark.asyncio
async def test_tool_manager_integration():
    tool_manager = ToolManager([MyCustomTool()])

    # Test tool execution through manager
    result = await tool_manager.execute(
        name="my_custom_tool",
        tool_input={"input_data": "test"}
    )

    assert result.output is not None
```

## Tool Discovery and Documentation

### Auto-generating Tool Docs

```python
def generate_tool_docs(tools: List[BaseTool]) -> str:
    """Generate markdown documentation for tools"""
    docs = "# Available Tools

"

    for tool in tools:
        docs += f"## {tool.name}

"
        docs += f"{tool.description}

"
        docs += "### Parameters

"

        for param, config in tool.parameters.get("properties", {}).items():
            required = param in tool.parameters.get("required", [])
            docs += f"- **{param}** ({'required' if required else 'optional'}): {config.get('description', '')}
"

        docs += "
"

    return docs
```

### Tool Registry

```python
class ToolRegistry:
    """Central registry for tool discovery"""

    def __init__(self):
        self._tools = {}

    def register(self, tool_class: type):
        """Register a tool class"""
        tool = tool_class()
        self._tools[tool.name] = tool_class
        return tool_class

    def get_tool(self, name: str) -> BaseTool:
        """Get tool instance by name"""
        if name not in self._tools:
            raise ValueError(f"Tool {name} not found")
        return self._tools[name]()

    def list_tools(self) -> List[str]:
        """List all registered tool names"""
        return list(self._tools.keys())

# Usage
registry = ToolRegistry()

@registry.register
class MyTool(BaseTool):
    # Tool implementation
    pass
```

## Best Practices

### 1. Tool Naming

- Use descriptive, action-oriented names
- Follow snake_case convention
- Avoid generic names like "tool" or "helper"

### 2. Parameter Design

- Provide clear descriptions for all parameters
- Use appropriate data types and validation
- Set sensible defaults for optional parameters

### 3. Error Messages

- Be specific about what went wrong
- Include suggestions for fixing issues
- Don't expose sensitive information in errors

### 4. Performance

- Use async/await for I/O operations
- Leverage framework's built-in timeout handling
- Cache results when appropriate

### 5. Security

- Validate all inputs thoroughly
- Use environment variables for secrets
- Rely on framework's rate limiting features

## Next Steps

### ðŸ“š **Custom Tool Examples**

#### ðŸ” [MCP Spoon Search Agent](../examples/mcp-spoon-search-agent.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

**Custom tool integration demonstrated:**
- MCP server integration with custom search tools
- Web search capabilities using Tavily MCP
- Custom error handling for external API calls
- Real-world custom tool deployment patterns

**Key learning points:**
- How to wrap external APIs as custom tools
- MCP server integration patterns
- Error handling for unreliable external services
- Tool validation and testing strategies

#### ðŸ“Š [Graph Crypto Analysis](../examples/graph-crypto-analysis.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py)

**Financial tool development:**
- Custom cryptocurrency data processing tools
- Real-time technical indicator calculations
- Multi-source data aggregation and validation
- Financial data error handling and recovery

**Key learning points:**
- Domain-specific tool development patterns
- Financial data validation techniques
- Multi-API integration strategies
- Performance optimization for data-intensive tools

#### ðŸŽ¯ [Intent Graph Demo](../examples/intent-graph-demo.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py)

**Advanced tool orchestration:**
- Custom routing and decision-making tools
- Memory management and context preservation tools
- Parallel processing coordination tools
- Performance monitoring and metrics tools

**Key learning points:**
- Complex tool interaction patterns
- State management in custom tools
- Performance optimization techniques
- Error recovery in multi-tool workflows

### ðŸ› ï¸ **Development Resources**

- **[Core Concepts: Tools](../core-concepts/tools.md)** - Complete tool system understanding
- **[MCP Protocol](../core-concepts/mcp-protocol.md)** - Advanced integration patterns
- **[Tool API Reference](../api-reference/tools/base-tool.md)** - Complete development documentation

### ðŸ“– **Additional Resources**

- **[Built-in Tools Reference](../api-reference/tools/builtin-tools.md)** - Explore existing tool implementations
- **[Graph System](../core-concepts/graph-system.md)** - Advanced workflow orchestration
- **[Agent Architecture](../core-concepts/agents.md)** - Tool-agent integration patterns

## Troubleshooting

### Common Issues

**Tool not found in manager:**

- Ensure tool is properly added to ToolManager
- Check tool name matches exactly
- Verify tool class inherits from BaseTool

**Parameter validation errors:**

- Check JSON schema syntax in parameters
- Ensure required parameters are marked correctly
- Validate parameter types match schema

**Execution failures:**

- Leverage framework's automatic error handling
- Check for missing dependencies or API keys
- Use framework's built-in debugging features
**Key learning points:**
- Domain-specific tool development patterns
- Financial data validation techniques
- Multi-API integration strategies
- Performance optimization for data-intensive tools

#### ðŸŽ¯ [Intent Graph Demo](../examples/intent-graph-demo.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py)

**Advanced tool orchestration:**
- Custom routing and decision-making tools
- Memory management and context preservation tools
- Parallel processing coordination tools
- Performance monitoring and metrics tools

**Key learning points:**
- Complex tool interaction patterns
- State management in custom tools
- Performance optimization techniques
- Error recovery in multi-tool workflows

### ðŸ› ï¸ **Development Resources**

- **[Core Concepts: Tools](../core-concepts/tools.md)** - Complete tool system understanding
- **[MCP Protocol](../core-concepts/mcp-protocol.md)** - Advanced integration patterns
- **[Tool API Reference](../api-reference/tools/base-tool.md)** - Complete development documentation

### ðŸ“– **Additional Resources**

- **[Built-in Tools Reference](../api-reference/tools/builtin-tools.md)** - Explore existing tool implementations
- **[Graph System](../core-concepts/graph-system.md)** - Advanced workflow orchestration
- **[Agent Architecture](../core-concepts/agents.md)** - Tool-agent integration patterns

## Troubleshooting

### Common Issues

**Tool not found in manager:**

- Ensure tool is properly added to ToolManager
- Check tool name matches exactly
- Verify tool class inherits from BaseTool

**Parameter validation errors:**

- Check JSON schema syntax in parameters
- Ensure required parameters are marked correctly
- Validate parameter types match schema

**Execution failures:**

- Leverage framework's automatic error handling
- Check for missing dependencies or API keys
- Use framework's built-in debugging features

---

FILE: docs/how-to-guides/build-first-agent.md

# Build Your First Agent

Learn how to create a custom AI agent from scratch using SpoonOS.

## Prerequisites

- SpoonOS installed and configured
- API keys set up for your chosen LLM provider
- Basic Python knowledge

## Step 1: Basic Agent Setup

### Create Agent File

Create a new file `my_first_agent.py` (works with Gemini, OpenAI, or any configured provider):

```python
import os
from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot
from spoon_ai.tools.crypto_tools import get_crypto_tools

# Create your first agent
def create_agent():
    # Configure LLM
    llm = ChatBot(
        # Pick up provider/model from env to support Gemini out of the box.
        # Example: set DEFAULT_LLM_PROVIDER=gemini and GEMINI_API_KEY=***
        llm_provider=os.getenv("LLM_PROVIDER") or os.getenv("DEFAULT_LLM_PROVIDER") or "gemini",
        model_name=os.getenv("LLM_MODEL") or "gemini-2.5-pro",
        temperature=0.3
    )

    # Create agent with tools
    agent = SpoonReactAI(
        llm=llm,
        tools=[*get_crypto_tools()]  # requires `pip install -e toolkit`
    )

    return agent

# Test the agent
async def main():
    agent = create_agent()

    # Framework handles all errors automatically
    response = await agent.run("Hello! What can you help me with?")
    response = await agent.run("What's the current price of Bitcoin?")

    return response

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

### Run Your Agent

```bash
python my_first_agent.py
```

Your agent will respond with helpful information and current Bitcoin price data.

## Step 2: Add Custom Functionality

### Create Custom Tool

```python
from spoon_ai.tools.base import BaseTool
from typing import Dict, Any

class GreetingTool(BaseTool):
    name: str = "greeting_tool"
    description: str = "Generate personalized greetings"
    parameters: dict = {
        "type": "object",
        "properties": {
            "name": {"type": "string", "description": "Person's name"},
            "style": {"type": "string", "description": "Greeting style (formal/casual)"}
        },
        "required": ["name"]
    }

    async def execute(self, name: str, style: str = "casual") -> str:
        if style == "formal":
            return f"Good day, {name}. It's a pleasure to meet you."
        else:
            return f"Hey {name}! Nice to meet you! ðŸ‘‹"
```

### Enhanced Agent with Custom Tool

```python
def create_enhanced_agent():
    import os
    llm = ChatBot(
        llm_provider=os.getenv("LLM_PROVIDER") or os.getenv("DEFAULT_LLM_PROVIDER") or "gemini",
        model_name=os.getenv("LLM_MODEL") or "gemini-2.5-pro",
        temperature=0.3,
        enable_short_term_memory=True,
        short_term_memory_config={
            "max_tokens": 8000,
            "strategy": "summarize",
            "messages_to_keep": 6,
        },
    )

    # Add multiple tools
    agent = SpoonReactAI(
        llm=llm,
        tools=[
            *get_crypto_tools(),
            GreetingTool()
        ]
    )

    return agent

# Run enhanced agent (same entry style as Step 1)
async def main_enhanced():
    agent = create_enhanced_agent()

    # Framework automatically handles tool selection and execution
    response = await agent.run("Give me a formal greeting for John")
    response = await agent.run("Greet Alice casually and then tell her the Bitcoin price")

    return response

if __name__ == "__main__":
    import asyncio
    asyncio.run(main_enhanced())

### Optional: Stream responses

If you want token-by-token output (works with any supported provider):

```python
async def stream_demo():
    llm = ChatBot(model_name="gpt-4.1", llm_provider="openai")
    messages = [{"role": "user", "content": "Stream a 3-step plan to learn SpoonOS"}]
    async for chunk in llm.astream(messages=messages):
        print(chunk.delta or "", end="", flush=True)
```

## Step 3: Add Memory and Context

> Tip: Short-term memory trimming/summarization is already enabled in `create_enhanced_agent` via `enable_short_term_memory=True`. Use a higher `max_tokens` or switch `strategy` to `"trim"` if you prefer dropping history instead of summarizing.

### Agent with Memory

```python
class MemoryAgent:
    def __init__(self):
        self.agent = create_enhanced_agent()
        self.conversation_history = []

    async def chat(self, message: str) -> str:
        # Add context from previous conversations
        context = self.build_context()
        full_message = f"{context}

User: {message}"

        # Get response
        response = await self.agent.run(full_message)

        # Store in memory
        self.conversation_history.append({
            "user": message,
            "agent": response,
            "timestamp": time.time()
        })

        return response

    def build_context(self) -> str:
        if not self.conversation_history:
            return "This is the start of our conversation."

        # Include last 3 exchanges for context
        recent = self.conversation_history[-3:]
        context_parts = []

        for exchange in recent:
            context_parts.append(f"User: {exchange['user']}")
            context_parts.append(f"Agent: {exchange['agent']}")

        return "Previous conversation:" + "".join(context_parts)

# Test memory functionality
async def test_memory_agent():
    agent = MemoryAgent()

    # Framework maintains conversation context automatically
    response1 = await agent.chat("My name is Sarah")
    response2 = await agent.chat("What's my name?")  # Agent remembers Sarah

    return response1, response2
```

## Step 4: Framework Error Handling

### Built-in Robustness

SpoonOS provides automatic error handling and robustness features:

```python
class SimpleAgent:
    def __init__(self):
        # Framework handles all error cases automatically
        self.agent = create_enhanced_agent()

    async def run(self, message: str) -> str:
        # Framework provides:
        # - Automatic retry with exponential backoff
        # - Provider fallback (OpenAI -> Anthropic -> Google)
        # - Tool error recovery with graceful degradation
        # - Timeout handling with configurable limits
        return await self.agent.run(message)

# Simple usage - no error handling needed
async def test_agent():
    agent = SimpleAgent()

    # Framework handles all error scenarios automatically
    response = await agent.run("Hello!")
    response = await agent.run("Perform a complex analysis")

    return response
```

## Step 5: Configuration and Deployment

### Configurable Agent

```python
import json
from pathlib import Path

class ConfigurableAgent:
    def __init__(self, config_path: str = "agent_config.json"):
        self.config = self.load_config(config_path)
        self.agent = self.create_agent_from_config()

    def load_config(self, config_path: str) -> dict:
        config_file = Path(config_path)
        if config_file.exists():
            with open(config_file, 'r') as f:
                return json.load(f)
        else:
            # Default configuration
            default_config = {
                "llm": {
                    "provider": "gemini",
                    "model": "gemini-2.5-pro",
                    "temperature": 0.3
                },
                "tools": ["crypto_tools", "greeting_tool"],
                "memory": {
                    "enabled": True,
                    "max_history": 10
                }
            }
            # Save default config
            with open(config_file, 'w') as f:
                json.dump(default_config, f, indent=2)
            return default_config

    def create_agent_from_config(self):
        # Create LLM from config
        llm_config = self.config["llm"]
        llm = ChatBot(
            model_name=llm_config["model"],
            llm_provider=llm_config["provider"],
            temperature=llm_config["temperature"]
        )

        # Create tools from config
        tools = []
        if "crypto_tools" in self.config["tools"]:
            tools.extend(get_crypto_tools())
        if "greeting_tool" in self.config["tools"]:
            tools.append(GreetingTool())

        return SpoonReactAI(llm=llm, tools=tools)

    async def run(self, message: str) -> str:
        return await self.agent.run(message)

# Example configuration file (agent_config.json)
example_config = {
    "llm": {
        "provider": "anthropic",
        "model": "claude-sonnet-4-20250514",
        "temperature": 0.1
    },
    "tools": ["crypto_tools", "greeting_tool"],
    "memory": {
        "enabled": True,
        "max_history": 5
    }
}
```

## Step 6: Testing Your Agent

### Unit Tests

```python
import pytest
from unittest.mock import AsyncMock, patch

class TestMyAgent:
    @pytest.fixture
    async def agent(self):
        return create_enhanced_agent()

    @pytest.mark.asyncio
    async def test_basic_greeting(self, agent):
        with patch.object(agent, 'run', new_callable=AsyncMock) as mock_run:
            mock_run.return_value = "Hello! How can I help you?"

            response = await agent.run("Hello")
            assert "Hello" in response
            mock_run.assert_called_once_with("Hello")

    @pytest.mark.asyncio
    async def test_crypto_tool_integration(self, agent):
        # Test that crypto tools are available
        tool_names = [tool.name for tool in agent.tools]
        assert "get_price" in tool_names or any("crypto" in name.lower() for name in tool_names)

# Run tests
# pytest test_my_agent.py -v
```

### Integration Tests

```python
async def integration_test():
    """Test complete agent workflow"""
    agent = create_enhanced_agent()

    # Framework provides built-in validation and testing
    response1 = await agent.run("Hello")
    response2 = await agent.run("What's the Bitcoin price?")
    response3 = await agent.run("Give me a casual greeting for Alice")

    # Framework automatically validates responses and tool execution
    return all([response1, response2, response3])

# Run integration tests
if __name__ == "__main__":
    result = asyncio.run(integration_test())
```

## Complete Example

Here's the complete, production-ready agent:

```python
import asyncio
import json
import logging
import time
from pathlib import Path
from typing import List, Dict, Any

from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot
from spoon_ai.tools.crypto_tools import get_crypto_tools
from spoon_ai.tools.base import BaseTool

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GreetingTool(BaseTool):
    name: str = "greeting_tool"
    description: str = "Generate personalized greetings"
    parameters: dict = {
        "type": "object",
        "properties": {
            "name": {"type": "string", "description": "Person's name"},
            "style": {"type": "string", "description": "Greeting style (formal/casual)"}
        },
        "required": ["name"]
    }

    async def execute(self, name: str, style: str = "casual") -> str:
        if style == "formal":
            return f"Good day, {name}. It's a pleasure to meet you."
        else:
            return f"Hey {name}! Nice to meet you! ðŸ‘‹"

class ProductionAgent:
    def __init__(self, config_path: str = "agent_config.json"):
        self.config = self.load_config(config_path)
        self.agent = self.create_agent()
        self.conversation_history = []
        self.logger = logging.getLogger(self.__class__.__name__)

    def load_config(self, config_path: str) -> dict:
        config_file = Path(config_path)
        if config_file.exists():
            with open(config_file, 'r') as f:
                return json.load(f)

        # Default configuration
        default_config = {
            "llm": {
                "provider": "openai",
                "model": "gpt-4.1",
                "temperature": 0.3
            },
            "tools": ["crypto_tools", "greeting_tool"],
            "memory": {"enabled": True, "max_history": 10},
            "retry_attempts": 3,
            "timeout": 30
        }

        with open(config_file, 'w') as f:
            json.dump(default_config, f, indent=2)

        return default_config

    def create_agent(self):
        llm_config = self.config["llm"]
        llm = ChatBot(
            model_name=llm_config["model"],
            llm_provider=llm_config["provider"],
            temperature=llm_config["temperature"]
        )

        tools = []
        if "crypto_tools" in self.config["tools"]:
            tools.extend(get_crypto_tools())
        if "greeting_tool" in self.config["tools"]:
            tools.append(GreetingTool())

        return SpoonReactAI(llm=llm, tools=tools)

    async def chat(self, message: str) -> str:
        # Framework handles timeouts and errors automatically
        if self.config["memory"]["enabled"]:
            context = self.build_context()
            full_message = f"{context}\n\nUser: {message}"
        else:
            full_message = message

        # Framework provides automatic timeout and error handling
        response = await self.agent.run(full_message)

        # Store in memory
        if self.config["memory"]["enabled"]:
            self.store_conversation(message, response)

        return response

    def build_context(self) -> str:
        if not self.conversation_history:
            return "This is the start of our conversation."

        max_history = self.config["memory"]["max_history"]
        recent = self.conversation_history[-max_history:]

        context_parts = ["Previous conversation:"]
        for exchange in recent:
            context_parts.append(f"User: {exchange['user']}")
            context_parts.append(f"Agent: {exchange['agent']}")

        return "
".join(context_parts)

    def store_conversation(self, user_message: str, agent_response: str):
        self.conversation_history.append({
            "user": user_message,
            "agent": agent_response,
            "timestamp": time.time()
        })

        # Limit history size
        max_history = self.config["memory"]["max_history"]
        if len(self.conversation_history) > max_history:
            self.conversation_history = self.conversation_history[-max_history:]

# Usage example
async def main():
    agent = ProductionAgent()

    # Simple chat interface - framework handles all complexity
    while True:
        user_input = input("You: ")
        if user_input.lower() in ['quit', 'exit', 'bye']:
            break

        response = await agent.chat(user_input)
        # Response is automatically formatted and error-free

if __name__ == "__main__":
    asyncio.run(main())
```

## Next Steps

Now that you've built your first agent, explore these advanced topics:

- [Add Custom Tools](./add-custom-tools.md) - Create specialized tools

---

FILE: docs/how-to-guides/vibe-coding.md

# Vibe Coding / IDE AI Guide

Give Cursor, Codex, Claude Code, and similar assistants the right context to generate accurate XSpoonAi code. Pick **one** of the four methods below; they are independent, ordered from most guided to most manual.

## Method 1: Use MCP / online retrieval (no local clone)

- Configure MCP connectors (e.g., deepwiki/context7) to fetch files from GitHub or the web.
- Target upstream repos directly: `XSpoonAi/spoon-core` and `XSpoonAi/toolkit` (source + examples) plus this Cookbook site for docs.
- Instruct the assistant to: list relevant files, fetch the exact file contents it will mimic, then cite paths/lines. No need to mirror or clone locally.

## Method 2: Supply the bundled docs file (`cookbook/llm.txt`)

- The repo ships an auto-generated bundle of every Markdown doc at `cookbook/llm.txt` (CI keeps it fresh).
- Share that single file with your LLM to give it the full Cookbook context for Vibe Coding.
- GitHub copy (fallback): [`llm.txt`](https://github.com/XSpoonAi/xspoonai.github.io/blob/main/llm.txt).

## Method 3: Point to installed package paths

- If you installed editable/local packages, expose their locations instead of the repo:
  ```bash
  python -c "import importlib.util, os; spec = importlib.util.find_spec('spoon_ai'); print('spoon_ai:', os.path.dirname(spec.origin) if spec and spec.origin else 'not installed')"
  python -c "import importlib.util, os; spec = importlib.util.find_spec('spoon_toolkits'); print('spoon_toolkits:', os.path.dirname(spec.origin) if spec and spec.origin else 'not installed')"
  ```
- Provide those paths to the assistant as read-only references so it can scan the actual shipped code and examples inside the installed packages.
- If a package isnâ€™t installed (output shows â€œnot installedâ€), just share the corresponding repo directory (e.g., `core/spoon_ai` or `toolkit/spoon_toolkits`) instead.

## Method 4: Share the workspace directly

- Allow the assistant to read this repo (especially `core/`, `toolkit/`, `cookbook/docs/`). No extra cloning required.
- Need a local checkout first? From your working directory run:
  ```bash
  git clone https://github.com/XSpoonAi/spoon-core.git
  git clone https://github.com/XSpoonAi/spoon-toolkit.git
  ```
- Keep the repo up to date (`git pull`) so the assistant always sees current code and docs.
- Ask it to open real source first, e.g. `core/spoon_ai/**` and runnable samples in `core/examples/` and `toolkit/**/examples`.
- Tell the AI to derive function signatures and configs from code files or examples, not invented abstractions.

## Hallucination-reduction tips (tell the AI)

- "Read the source or examples you will copy from; cite file path and line range before coding."
- "Confirm tool/agent/LLM signatures from code, not from guesswork; show the imports you plan to use."
- "If unsure, fetch the specific file via MCP and restate the interface before implementing."

---

FILE: docs/how-to-guides/x402-payments.md

# x402 Integration Guide

The x402 documentation is now split by audience so you can jump directly to the right level of detail.

## 1. Core concepts

Read [Core Concepts: x402 payments](../core-concepts/x402-payments.md) to understand:

- Why SpoonOS uses x402 to gate agent actions.
- Required environment variables and configuration fallbacks.
- The end-to-end payment lifecycle (probe -> sign -> retry -> settle) plus the flow diagram.

## 2. API reference

Consult [API Reference: x402](../api-reference/payments/x402.md) for:

- Python services (`X402PaymentService`, request/response models, helper methods).
- Built-in tools (`x402_create_payment`, `x402_paywalled_request`) with parameter tables.
- CLI commands (`requirements`, `sign`) and the FastAPI paywall endpoints.
- Environment and Turnkey integration notes.

## 3. Example

Follow [Example: x402 ReAct agent](../examples/x402-react-agent.md) to run the ReAct demo (`uv run python examples/x402_agent_demo.py`). It walks through:

- Preparing `.env` and funding the signer.
- Observing the agent call `http_probe`, then `x402_paywalled_request`, retrieve the protected page, and print the signed `X-PAYMENT` + settlement receipt.
- Troubleshooting common facilitator or configuration errors.

## Quick checklist

- Fund the signer via [Circle faucet](https://faucet.circle.com/).
- Keep `PRIVATE_KEY` populated; rely on Turnkey only when the local key is absent.
- Reuse `X402PaymentService.decode_payment_response()` anywhere you archive facilitator receipts.

---

FILE: docs/troubleshooting/common-issues.md

# Common Issues and Solutions

This guide covers the most frequently encountered issues when working with SpoonOS and their solutions.

## Installation Issues

### Python Version Compatibility

**Problem:** `ImportError` or `ModuleNotFoundError` when importing SpoonOS modules

**Symptoms:**
```bash
ImportError: No module named 'spoon_ai'
ModuleNotFoundError: No module named 'asyncio'
```

**Solution:**
1. Ensure Python 3.12+ is installed:
   ```bash
   python --version
   # Should show Python 3.12.0 or higher
   ```

2. Create a new virtual environment:
   ```bash
   python -m venv spoon-env
   source spoon-env/bin/activate  # Linux/macOS
   # or
   spoon-env\\Scripts\\activate     # Windows
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

### Dependency Conflicts

**Problem:** Package version conflicts during installation

**Symptoms:**
```bash
ERROR: pip's dependency resolver does not currently have a solution
Conflicting dependencies: package-a requires package-b>=2.0, but package-c requires package-b<2.0
```

**Solution:**
1. Use a fresh virtual environment
2. Install packages one by one to identify conflicts
3. Check `requirements.txt` for version pinning issues
4. Use `pip install --upgrade` for outdated packages

## Configuration Issues

### API Key Problems

**Problem:** Authentication errors with LLM providers

**Symptoms:**
```bash
AuthenticationError: Invalid API key
Unauthorized: API key not found
```

**Solution:**
1. Verify API keys are set correctly:
   ```bash
   echo $OPENAI_API_KEY
   echo $ANTHROPIC_API_KEY
   ```

2. Check `.env` file format:
   ```bash
   # Correct format
   OPENAI_API_KEY=sk-your-actual-key-here

   # Incorrect (no quotes needed)
   OPENAI_API_KEY="sk-your-actual-key-here"
   ```

3. Validate API key format:
   - OpenAI: Starts with `sk-`
   - Anthropic: Starts with `sk-ant-`
   - Google: 39-character string

4. Test API key validity:
   ```bash
   curl -H "Authorization: Bearer $OPENAI_API_KEY" \\
        https://api.openai.com/v1/models
   ```

### Configuration File Errors

**Problem:** JSON parsing errors in `config.json`

**Symptoms:**
```bash
JSONDecodeError: Expecting ',' delimiter
ConfigurationError: Invalid configuration format
```

**Solution:**
1. Validate JSON syntax using online validator or:
   ```bash
   python -m json.tool config.json
   ```

2. Common JSON errors:
   ```json
   // Wrong: trailing comma
   {
     "key": "value",
   }

   // Correct
   {
     "key": "value"
   }
   ```

3. Use CLI validation:
   ```bash
   python main.py
   > validate-config
   ```

## Agent Issues

### Agent Loading Failures

**Problem:** Agent fails to load or initialize

**Symptoms:**
```bash
AgentError: Agent 'my_agent' not found
ImportError: cannot import name 'MyAgent'
```

**Solution:**

1. Verify agent class exists:
   ```python
   from spoon_ai.agents import SpoonReactAI
   # Should not raise ImportError
   ```

2. Check for typos in agent names and class names

3. List available agents:
   ```bash
   python main.py
   > list-agents
   ```

### Tool Loading Issues (CLI `config.json`)

**Problem:** Tools not available or failing to load

**Symptoms:**
```bash
ToolError: Tool 'crypto_tool' not found
ModuleNotFoundError: No module named 'spoon_toolkits'
```

**Solution:**
1. Install spoon-toolkit package:
   ```bash
   pip install spoon-toolkits
   ```

2. Verify environment variables for tools:
   ```bash
   echo $OKX_API_KEY
   echo $COINGECKO_API_KEY
   ```

3. List available tools:
   ```bash
   python main.py
   > list-toolkit-categories
   > list-toolkit-tools crypto
   ```

## LLM Provider Issues

### Provider Connection Failures

**Problem:** Cannot connect to LLM providers

**Symptoms:**
```bash
ConnectionError: Failed to connect to OpenAI API
TimeoutError: Request timed out
```

**Solution:**
1. Check internet connectivity:
   ```bash
   ping api.openai.com
   ping api.anthropic.com
   ```

2. Verify API endpoints are accessible:
   ```bash
   curl -I https://api.openai.com/v1/models
   ```

3. Check firewall and proxy settings

4. Test with different provider:
   ```bash
   python main.py
   > llm-status
   ```

## MCP (Model Context Protocol) Issues

### MCP Server Connection Problems

**Problem:** Cannot connect to MCP servers

**Symptoms:**
```bash
MCPError: Failed to connect to MCP server
ConnectionRefusedError: [Errno 111] Connection refused
```

**Solution:**
1. Verify MCP server is running:
   ```bash
   curl http://localhost:8765/health
   ```

2. Check MCP server configuration:
   ```json
   {
     "mcp_servers": [
       {
         "name": "my_server",
         "url": "http://localhost:8765",
         "transport": "sse"
       }
     ]
   }
   ```

3. Start MCP server:
   ```bash
   python mcp_server.py
   ```

4. Check server logs for errors

### MCP Tool Discovery Issues

**Problem:** MCP tools not discovered or available

**Symptoms:**
```bash
MCPError: No tools found on server
ToolError: MCP tool 'my_tool' not available
```

**Solution:**
1. Verify tools are registered on MCP server:
   ```python
   @mcp.tool()
   def my_tool():
       return "Hello from MCP"
   ```

2. Check MCP server tool listing:
   ```bash
   curl http://localhost:8765/tools
   ```

3. Restart MCP server after adding tools

4. Verify tool permissions and authentication

## Performance Issues

### Slow Response Times

**Problem:** Agent responses are very slow

**Symptoms:**
- Long delays before responses
- Timeout errors
- High CPU/memory usage

**Solution:**
1. Check system resources:
   ```bash
   python main.py
   > system-info
   ```

2. Optimize LLM configuration:
   ```json
   {
     "llm": {
       "temperature": 0.7,
       "max_tokens": 1000,
       "timeout": 30
     }
   }
   ```

3. Enable caching:
   ```json
   {
     "cache": {
       "enabled": true,
       "ttl": 3600
     }
   }
   ```

4. Reduce tool complexity and number of tools

### Memory Issues

**Problem:** High memory usage or out-of-memory errors

**Symptoms:**
```bash
MemoryError: Unable to allocate memory
Process killed (OOM)
```

**Solution:**
1. Monitor memory usage:
   ```bash
   python main.py
   > system-info
   ```

2. Reduce conversation history:
   ```bash
   python main.py
   > new-chat
   ```

3. Optimize agent configuration:
   ```json
   {
     "config": {
       "max_steps": 5,
       "max_tokens": 500
     }
   }
   ```

4. Use lighter LLM models (e.g., GPT-3.5 instead of GPT-4)

## Blockchain Integration Issues

### RPC Connection Problems

**Problem:** Cannot connect to blockchain RPC endpoints

**Symptoms:**
```bash
ConnectionError: Failed to connect to RPC
HTTPError: 403 Forbidden
```

**Solution:**
1. Verify RPC URL is correct:
   ```bash
   curl -X POST -H "Content-Type: application/json" \\
        --data '{"jsonrpc":"2.0","method":"eth_blockNumber","params":[],"id":1}' \\
        $RPC_URL
   ```

2. Check RPC provider limits and authentication

3. Try alternative RPC endpoints:
   ```bash
   # Ethereum
   export RPC_URL="https://eth.llamarpc.com"
   export RPC_URL="https://rpc.ankr.com/eth"
   ```

4. Verify network connectivity and firewall settings

### Transaction Failures

**Problem:** Blockchain transactions fail or revert

**Symptoms:**
```bash
TransactionError: Transaction reverted
InsufficientFundsError: Not enough balance
```

**Solution:**
1. Check wallet balance:
   ```bash
   python main.py
   > token-by-symbol ETH
   ```

2. Verify gas settings:
   ```json
   {
     "blockchain": {
       "gas_limit": 21000,
       "gas_price": "20000000000"
     }
   }
   ```

3. Check transaction parameters and recipient address

4. Verify private key and wallet configuration

## Debugging Techniques

### Enable Debug Logging

```bash
# Set environment variables
export DEBUG=true
export LOG_LEVEL=debug

# Run with verbose output
python main.py
```

### Use System Diagnostics

```bash
python main.py
> system-info
> llm-status
> validate-config
```

### Check Configuration

```bash
# Validate configuration
python main.py
> validate-config

# Check migration status
python main.py
> check-config
```

### Test Individual Components

```python
# Test LLM connection
from spoon_ai.llm import LLMManager
llm = LLMManager()
response = await llm.generate("Hello, world!")

# Test tool execution
from spoon_toolkits.crypto import GetTokenPriceTool
tool = GetTokenPriceTool()
result = await tool.execute(symbol="BTC")
```

## Getting Help

### Documentation Resources
- [Installation Guide](../getting-started/installation.md)
- [Configuration Guide](../getting-started/configuration.md)
- [API Reference](../api-reference/)
- [How-To Guides](../how-to-guides/)

### ðŸ“š **Working Examples**

#### ðŸŽ¯ [Intent Graph Demo](../examples/intent-graph-demo.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py)

**Perfect for troubleshooting:**
- Graph system setup and configuration
- Memory management and state persistence issues
- Parallel execution and routing problems
- Production deployment patterns

#### ðŸ” [MCP Spoon Search Agent](../examples/mcp-spoon-search-agent.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

**Great for debugging:**
- MCP server connection and integration issues
- Tool discovery and loading problems
- API rate limiting and error handling
- Multi-tool orchestration challenges

#### ðŸ“Š [Graph Crypto Analysis](../examples/graph-crypto-analysis.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py)

**Excellent for testing:**
- Real API integration and authentication
- Data processing and validation issues
- Performance optimization problems
- Complex workflow debugging

### Community Support
- GitHub Issues: Report bugs and feature requests
- Discord: Real-time community support
- Documentation: Comprehensive guides and working examples

### Diagnostic Information
When reporting issues, include:
- Python version (`python --version`)
- SpoonOS version
- Operating system
- Error messages and stack traces
- Configuration files (sanitized)
- Steps to reproduce

### Log Collection

```bash
# Enable debug logging
export DEBUG=true
export LOG_LEVEL=debug

# Capture logs
python main.py 2>&1 | tee spoon_debug.log

# Include relevant log sections in issue reports
```

## Prevention Tips

### Regular Maintenance
- Keep dependencies updated
- Rotate API keys regularly
- Monitor system resources
- Backup configuration files
- Test in development environment first

### Best Practices
- Use version control for configurations
- Implement proper error handling
- Monitor API usage and costs
- Set up alerts for critical issues
- Document custom configurations

### Environment Management
- Use separate environments for development/production
- Pin dependency versions in requirements.txt
- Use environment variables for sensitive data
- Regularly test backup and recovery procedures

## See Also

- [Debugging Guide](./debugging.md)
- [Performance Optimization](./performance.md)
- [System Requirements](../getting-started/installation.md)"}

## Prevention Tips

### Regular Maintenance
- Keep dependencies updated
- Rotate API keys regularly
- Monitor system resources
- Backup configuration files
- Test in development environment first

### Best Practices
- Use version control for configurations
- Implement proper error handling
- Monitor API usage and costs
- Set up alerts for critical issues
- Document custom configurations

### Environment Management
- Use separate environments for development/production
- Pin dependency versions in requirements.txt
- Use environment variables for sensitive data
- Regularly test backup and recovery procedures

## See Also

- [Debugging Guide](./debugging.md)
- [Performance Optimization](./performance.md)
- [System Requirements](../getting-started/installation.md)"}

---

FILE: docs/troubleshooting/debugging.md

# Debugging Guide

Comprehensive guide for debugging SpoonOS applications, agents, and tools.

## Debug Configuration

### Environment Variables

```bash
# Enable debug mode
export DEBUG=true
export LOG_LEVEL=debug

# Enable specific debug categories
export DEBUG_AGENTS=true
export DEBUG_TOOLS=true
export DEBUG_LLM=true
export DEBUG_MCP=true

# Enable request/response logging
export LOG_REQUESTS=true
export LOG_RESPONSES=true
```

### Configuration File Debug Settings

```json
{
  "debug": {
    "enabled": true,
    "log_level": "debug",
    "categories": ["agents", "tools", "llm", "mcp"],
    "log_requests": true,
    "log_responses": true,
    "save_logs": true,
    "log_file": "spoon_debug.log"
  }
}
```

## Logging Setup

### Python Logging Configuration

```python
# debug_setup.py
import logging
import sys
from datetime import datetime

def setup_debug_logging():
    """Configure comprehensive debug logging"""
    
    # Create formatter
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s'
    )
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.DEBUG)
    console_handler.setFormatter(formatter)
    
    # File handler
    file_handler = logging.FileHandler(
        f'spoon_debug_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
    )
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    
    # Configure root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    root_logger.addHandler(console_handler)
    root_logger.addHandler(file_handler)
    
    # Configure SpoonOS loggers
    spoon_logger = logging.getLogger('spoon_ai')
    spoon_logger.setLevel(logging.DEBUG)
    
    toolkit_logger = logging.getLogger('spoon_toolkits')
    toolkit_logger.setLevel(logging.DEBUG)
    
    print("Debug logging configured")

if __name__ == "__main__":
    setup_debug_logging()
```

### Structured Logging

```python
# structured_logging.py
import structlog
import json
from datetime import datetime

def setup_structured_logging():
    """Configure structured logging with JSON output"""
    
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.JSONRenderer()
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )
    
    return structlog.get_logger()

# Usage example
logger = setup_structured_logging()
logger.info("Agent started", agent_name="debug_agent", tools_count=5)
logger.error("Tool execution failed", tool_name="crypto_tool", error="API timeout")
```

## Agent Debugging

### Agent State Inspection

```python
# agent_debugger.py
from spoon_ai.agents import SpoonReactAI
from spoon_ai.tools import ToolManager
import json

class DebuggableAgent(SpoonReactAI):
    """Agent with enhanced debugging capabilities"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.debug_info = {
            "steps": [],
            "tool_calls": [],
            "llm_requests": [],
            "errors": []
        }
    
    async def run(self, message: str, **kwargs):
        """Run with debug tracking"""
        self.debug_info["steps"].append({
            "timestamp": datetime.now().isoformat(),
            "action": "run_started",
            "message": message,
            "kwargs": kwargs
        })
        
        try:
            result = await super().run(message, **kwargs)
            
            self.debug_info["steps"].append({
                "timestamp": datetime.now().isoformat(),
                "action": "run_completed",
                "result_length": len(str(result))
            })
            
            return result
            
        except Exception as e:
            self.debug_info["errors"].append({
                "timestamp": datetime.now().isoformat(),
                "error_type": type(e).__name__,
                "error_message": str(e),
                "traceback": traceback.format_exc()
            })
            raise
    
    def get_debug_info(self) -> dict:
        """Get comprehensive debug information"""
        return {
            "agent_name": self.name,
            "system_prompt": self.system_prompt,
            "config": self.config,
            "available_tools": [tool.name for tool in self.tools],
            "debug_info": self.debug_info
        }
    
    def save_debug_info(self, filename: str = None):
        """Save debug information to file"""
        if not filename:
            filename = f"debug_{self.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(filename, 'w') as f:
            json.dump(self.get_debug_info(), f, indent=2)
        
        print(f"Debug info saved to {filename}")

# Usage
async def debug_agent_example():
    agent = DebuggableAgent(
        name="debug_agent",
        system_prompt="You are a debugging assistant."
    )
    
    try:
        response = await agent.run("Hello, debug me!")
        print(f"Response: {response}")
    except Exception as e:
        print(f"Error: {e}")
    finally:
        agent.save_debug_info()
        print(json.dumps(agent.get_debug_info(), indent=2))
```

### Step-by-Step Execution Tracing

```python
# execution_tracer.py
from spoon_ai.agents.base import BaseAgent
from functools import wraps
import inspect

def trace_execution(func):
    """Decorator to trace function execution"""
    @wraps(func)
    async def wrapper(self, *args, **kwargs):
        func_name = func.__name__
        
        # Log function entry
        logger.debug(
            "Function entry",
            function=func_name,
            args=args,
            kwargs=kwargs,
            agent=getattr(self, 'name', 'unknown')
        )
        
        try:
            # Execute function
            result = await func(self, *args, **kwargs)
            
            # Log successful completion
            logger.debug(
                "Function success",
                function=func_name,
                result_type=type(result).__name__,
                agent=getattr(self, 'name', 'unknown')
            )
            
            return result
            
        except Exception as e:
            # Log error
            logger.error(
                "Function error",
                function=func_name,
                error_type=type(e).__name__,
                error_message=str(e),
                agent=getattr(self, 'name', 'unknown')
            )
            raise
    
    return wrapper

class TracedAgent(BaseAgent):
    """Agent with method tracing"""
    
    @trace_execution
    async def run(self, message: str, **kwargs):
        return await super().run(message, **kwargs)
    
    @trace_execution
    async def chat(self, messages, **kwargs):
        return await super().chat(messages, **kwargs)
```

## Tool Debugging

### Tool Execution Monitoring

```python
# tool_debugger.py
from spoon_ai.tools.base import BaseTool
from spoon_ai.tools.errors import ToolError
import time
import traceback

class DebuggableTool(BaseTool):
    """Base tool with debugging capabilities"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.execution_history = []
        self.performance_stats = {
            "total_calls": 0,
            "successful_calls": 0,
            "failed_calls": 0,
            "average_duration": 0,
            "total_duration": 0
        }
    
    async def execute(self, **kwargs):
        """Execute with comprehensive debugging"""
        start_time = time.time()
        execution_id = f"{self.name}_{int(start_time)}"
        
        # Log execution start
        logger.debug(
            "Tool execution started",
            tool=self.name,
            execution_id=execution_id,
            parameters=kwargs
        )
        
        try:
            # Validate parameters
            validated_params = self.validate_parameters(**kwargs)
            
            # Execute tool
            result = await self._execute_impl(**validated_params)
            
            # Calculate duration
            duration = time.time() - start_time
            
            # Update stats
            self._update_success_stats(duration)
            
            # Log execution success
            logger.debug(
                "Tool execution completed",
                tool=self.name,
                execution_id=execution_id,
                duration=duration,
                result_type=type(result).__name__
            )
            
            # Store execution history
            self.execution_history.append({
                "execution_id": execution_id,
                "timestamp": start_time,
                "duration": duration,
                "parameters": validated_params,
                "success": True,
                "result_type": type(result).__name__
            })
            
            return result
            
        except Exception as e:
            duration = time.time() - start_time
            
            # Update stats
            self._update_error_stats(duration)
            
            # Log execution error
            logger.error(
                "Tool execution failed",
                tool=self.name,
                execution_id=execution_id,
                duration=duration,
                error_type=type(e).__name__,
                error_message=str(e),
                traceback=traceback.format_exc()
            )
            
            # Store execution history
            self.execution_history.append({
                "execution_id": execution_id,
                "timestamp": start_time,
                "duration": duration,
                "parameters": kwargs,
                "success": False,
                "error_type": type(e).__name__,
                "error_message": str(e)
            })
            
            raise
    
    async def _execute_impl(self, **kwargs):
        """Override this method in subclasses"""
        raise NotImplementedError
    
    def _update_success_stats(self, duration: float):
        """Update performance statistics for successful execution"""
        self.performance_stats["total_calls"] += 1
        self.performance_stats["successful_calls"] += 1
        self.performance_stats["total_duration"] += duration
        self.performance_stats["average_duration"] = (
            self.performance_stats["total_duration"] / 
            self.performance_stats["total_calls"]
        )
    
    def _update_error_stats(self, duration: float):
        """Update performance statistics for failed execution"""
        self.performance_stats["total_calls"] += 1
        self.performance_stats["failed_calls"] += 1
        self.performance_stats["total_duration"] += duration
        self.performance_stats["average_duration"] = (
            self.performance_stats["total_duration"] / 
            self.performance_stats["total_calls"]
        )
    
    def get_debug_info(self) -> dict:
        """Get comprehensive debug information"""
        return {
            "tool_name": self.name,
            "description": self.description,
            "performance_stats": self.performance_stats,
            "recent_executions": self.execution_history[-10:],  # Last 10 executions
            "total_executions": len(self.execution_history)
        }
```

### Parameter Validation Debugging

```python
# parameter_debugger.py
from spoon_ai.tools.base import BaseTool
from spoon_ai.tools.errors import ValidationError
import jsonschema

class ValidatedTool(DebuggableTool):
    """Tool with enhanced parameter validation and debugging"""
    
    # Define parameter schema
    parameter_schema = {
        "type": "object",
        "properties": {},
        "required": []
    }
    
    def validate_parameters(self, **kwargs) -> dict:
        """Validate parameters with detailed error reporting"""
        logger.debug(
            "Parameter validation started",
            tool=self.name,
            raw_parameters=kwargs,
            schema=self.parameter_schema
        )
        
        try:
            # Validate against schema
            jsonschema.validate(kwargs, self.parameter_schema)
            
            # Custom validation
            validated = self._custom_validation(**kwargs)
            
            logger.debug(
                "Parameter validation successful",
                tool=self.name,
                validated_parameters=validated
            )
            
            return validated
            
        except jsonschema.ValidationError as e:
            logger.error(
                "Schema validation failed",
                tool=self.name,
                validation_error=str(e),
                error_path=list(e.path),
                invalid_value=e.instance
            )
            raise ValidationError(f"Parameter validation failed: {e.message}")
        
        except Exception as e:
            logger.error(
                "Custom validation failed",
                tool=self.name,
                validation_error=str(e)
            )
            raise ValidationError(f"Parameter validation failed: {str(e)}")
    
    def _custom_validation(self, **kwargs) -> dict:
        """Override for custom validation logic"""
        return kwargs
```

## LLM Debugging

### Request/Response Logging

```python
# llm_debugger.py
from spoon_ai.llm.base import BaseLLMProvider
import json
import time

class DebuggableLLMProvider(BaseLLMProvider):
    """LLM provider with request/response logging"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.request_history = []
    
    async def generate(self, messages, **kwargs):
        """Generate with request/response logging"""
        request_id = f"req_{int(time.time() * 1000)}"
        start_time = time.time()
        
        # Log request
        logger.debug(
            "LLM request started",
            provider=self.provider_name,
            request_id=request_id,
            model=kwargs.get('model', self.default_model),
            message_count=len(messages),
            parameters=kwargs
        )
        
        # Log messages (truncated for privacy)
        for i, msg in enumerate(messages):
            content_preview = msg.get('content', '')[:100] + '...' if len(msg.get('content', '')) > 100 else msg.get('content', '')
            logger.debug(
                "LLM message",
                request_id=request_id,
                message_index=i,
                role=msg.get('role'),
                content_preview=content_preview
            )
        
        try:
            # Make request
            response = await super().generate(messages, **kwargs)
            
            duration = time.time() - start_time
            
            # Log response
            logger.debug(
                "LLM request completed",
                provider=self.provider_name,
                request_id=request_id,
                duration=duration,
                response_length=len(str(response)),
                tokens_used=response.get('usage', {}).get('total_tokens', 0)
            )
            
            # Store request history
            self.request_history.append({
                "request_id": request_id,
                "timestamp": start_time,
                "duration": duration,
                "model": kwargs.get('model', self.default_model),
                "message_count": len(messages),
                "success": True,
                "tokens_used": response.get('usage', {}).get('total_tokens', 0)
            })
            
            return response
            
        except Exception as e:
            duration = time.time() - start_time
            
            logger.error(
                "LLM request failed",
                provider=self.provider_name,
                request_id=request_id,
                duration=duration,
                error_type=type(e).__name__,
                error_message=str(e)
            )
            
            # Store request history
            self.request_history.append({
                "request_id": request_id,
                "timestamp": start_time,
                "duration": duration,
                "model": kwargs.get('model', self.default_model),
                "message_count": len(messages),
                "success": False,
                "error_type": type(e).__name__,
                "error_message": str(e)
            })
            
            raise
```

### Token Usage Monitoring

```python
# token_monitor.py
from collections import defaultdict
import time

class TokenUsageMonitor:
    """Monitor and analyze token usage patterns"""
    
    def __init__(self):
        self.usage_stats = defaultdict(lambda: {
            "total_requests": 0,
            "total_tokens": 0,
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_cost": 0.0,
            "average_tokens_per_request": 0,
            "requests_by_hour": defaultdict(int)
        })
    
    def record_usage(self, provider: str, model: str, usage: dict, cost: float = 0.0):
        """Record token usage for analysis"""
        key = f"{provider}:{model}"
        stats = self.usage_stats[key]
        
        # Update counters
        stats["total_requests"] += 1
        stats["total_tokens"] += usage.get("total_tokens", 0)
        stats["prompt_tokens"] += usage.get("prompt_tokens", 0)
        stats["completion_tokens"] += usage.get("completion_tokens", 0)
        stats["total_cost"] += cost
        
        # Update averages
        stats["average_tokens_per_request"] = (
            stats["total_tokens"] / stats["total_requests"]
        )
        
        # Track hourly usage
        hour = int(time.time() // 3600)
        stats["requests_by_hour"][hour] += 1
        
        logger.info(
            "Token usage recorded",
            provider=provider,
            model=model,
            tokens=usage.get("total_tokens", 0),
            cost=cost
        )
    
    def get_usage_report(self) -> dict:
        """Generate comprehensive usage report"""
        report = {
            "total_providers": len(self.usage_stats),
            "providers": {}
        }
        
        for key, stats in self.usage_stats.items():
            provider, model = key.split(":", 1)
            
            if provider not in report["providers"]:
                report["providers"][provider] = {
                    "models": {},
                    "total_requests": 0,
                    "total_tokens": 0,
                    "total_cost": 0.0
                }
            
            # Add model stats
            report["providers"][provider]["models"][model] = stats
            
            # Aggregate provider stats
            report["providers"][provider]["total_requests"] += stats["total_requests"]
            report["providers"][provider]["total_tokens"] += stats["total_tokens"]
            report["providers"][provider]["total_cost"] += stats["total_cost"]
        
        return report
    
    def print_usage_summary(self):
        """Print formatted usage summary"""
        report = self.get_usage_report()
        
        print("\
=== Token Usage Summary ===")
        print(f"Total Providers: {report['total_providers']}")
        
        for provider, provider_stats in report["providers"].items():
            print(f"\
{provider.upper()}:")
            print(f"  Total Requests: {provider_stats['total_requests']:,}")
            print(f"  Total Tokens: {provider_stats['total_tokens']:,}")
            print(f"  Total Cost: ${provider_stats['total_cost']:.4f}")
            
            for model, model_stats in provider_stats["models"].items():
                print(f"  {model}:")
                print(f"    Requests: {model_stats['total_requests']:,}")
                print(f"    Tokens: {model_stats['total_tokens']:,}")
                print(f"    Avg Tokens/Request: {model_stats['average_tokens_per_request']:.1f}")
                print(f"    Cost: ${model_stats['total_cost']:.4f}")
```

## MCP Debugging

### MCP Server Connection Debugging

```python
# mcp_debugger.py
from spoon_ai.tools.mcp_client import MCPClient
import asyncio
import aiohttp

class DebuggableMCPClient(MCPClient):
    """MCP client with enhanced debugging"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.connection_history = []
        self.tool_discovery_history = []
    
    async def connect(self):
        """Connect with connection debugging"""
        start_time = time.time()
        
        logger.debug(
            "MCP connection attempt",
            server_url=self.server_url,
            transport=self.transport
        )
        
        try:
            await super().connect()
            
            duration = time.time() - start_time
            
            logger.info(
                "MCP connection successful",
                server_url=self.server_url,
                duration=duration
            )
            
            self.connection_history.append({
                "timestamp": start_time,
                "duration": duration,
                "success": True
            })
            
        except Exception as e:
            duration = time.time() - start_time
            
            logger.error(
                "MCP connection failed",
                server_url=self.server_url,
                duration=duration,
                error_type=type(e).__name__,
                error_message=str(e)
            )
            
            self.connection_history.append({
                "timestamp": start_time,
                "duration": duration,
                "success": False,
                "error_type": type(e).__name__,
                "error_message": str(e)
            })
            
            raise
    
    async def discover_tools(self):
        """Discover tools with debugging"""
        start_time = time.time()
        
        logger.debug("MCP tool discovery started", server_url=self.server_url)
        
        try:
            tools = await super().discover_tools()
            
            duration = time.time() - start_time
            
            logger.info(
                "MCP tool discovery completed",
                server_url=self.server_url,
                tools_found=len(tools),
                duration=duration,
                tool_names=[tool.name for tool in tools]
            )
            
            self.tool_discovery_history.append({
                "timestamp": start_time,
                "duration": duration,
                "success": True,
                "tools_found": len(tools),
                "tool_names": [tool.name for tool in tools]
            })
            
            return tools
            
        except Exception as e:
            duration = time.time() - start_time
            
            logger.error(
                "MCP tool discovery failed",
                server_url=self.server_url,
                duration=duration,
                error_type=type(e).__name__,
                error_message=str(e)
            )
            
            self.tool_discovery_history.append({
                "timestamp": start_time,
                "duration": duration,
                "success": False,
                "error_type": type(e).__name__,
                "error_message": str(e)
            })
            
            raise
```

## Performance Debugging

### Performance Profiler

```python
# performance_profiler.py
import cProfile
import pstats
import io
from functools import wraps
import time
import psutil
import os

class PerformanceProfiler:
    """Profile performance of SpoonOS components"""
    
    def __init__(self):
        self.profiles = {}
        self.memory_snapshots = []
    
    def profile_function(self, func_name: str = None):
        """Decorator to profile function performance"""
        def decorator(func):
            name = func_name or f"{func.__module__}.{func.__name__}"
            
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # Memory snapshot before
                process = psutil.Process(os.getpid())
                memory_before = process.memory_info().rss
                
                # CPU profiling
                profiler = cProfile.Profile()
                profiler.enable()
                
                start_time = time.time()
                
                try:
                    result = await func(*args, **kwargs)
                    
                    # Stop profiling
                    end_time = time.time()
                    profiler.disable()
                    
                    # Memory snapshot after
                    memory_after = process.memory_info().rss
                    
                    # Store profile data
                    s = io.StringIO()
                    ps = pstats.Stats(profiler, stream=s)
                    ps.sort_stats('cumulative')
                    ps.print_stats()
                    
                    self.profiles[name] = {
                        "timestamp": start_time,
                        "duration": end_time - start_time,
                        "memory_before": memory_before,
                        "memory_after": memory_after,
                        "memory_delta": memory_after - memory_before,
                        "profile_stats": s.getvalue()
                    }
                    
                    logger.debug(
                        "Function profiled",
                        function=name,
                        duration=end_time - start_time,
                        memory_delta=memory_after - memory_before
                    )
                    
                    return result
                    
                except Exception as e:
                    profiler.disable()
                    raise
            
            return wrapper
        return decorator
    
    def take_memory_snapshot(self, label: str = None):
        """Take a memory usage snapshot"""
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        
        snapshot = {
            "timestamp": time.time(),
            "label": label or f"snapshot_{len(self.memory_snapshots)}",
            "rss": memory_info.rss,
            "vms": memory_info.vms,
            "percent": process.memory_percent(),
            "cpu_percent": process.cpu_percent()
        }
        
        self.memory_snapshots.append(snapshot)
        
        logger.debug(
            "Memory snapshot taken",
            label=snapshot["label"],
            rss_mb=snapshot["rss"] / 1024 / 1024,
            cpu_percent=snapshot["cpu_percent"]
        )
        
        return snapshot
    
    def generate_performance_report(self) -> str:
        """Generate comprehensive performance report"""
        report = ["\
=== Performance Report ==="]
        
        # Function profiles
        if self.profiles:
            report.append("\
Function Profiles:")
            for func_name, profile in self.profiles.items():
                report.append(f"\
{func_name}:")
                report.append(f"  Duration: {profile['duration']:.4f}s")
                report.append(f"  Memory Delta: {profile['memory_delta'] / 1024 / 1024:.2f} MB")
        
        # Memory snapshots
        if self.memory_snapshots:
            report.append("\
Memory Snapshots:")
            for snapshot in self.memory_snapshots:
                report.append(
                    f"  {snapshot['label']}: {snapshot['rss'] / 1024 / 1024:.2f} MB "
                    f"({snapshot['percent']:.1f}%) CPU: {snapshot['cpu_percent']:.1f}%"
                )
        
        return "\
".join(report)
```

## Debug CLI Commands

### Enhanced CLI with Debug Commands

```python
# debug_cli.py
from spoon_ai.cli.base import BaseCLI
import json

class DebugCLI(BaseCLI):
    """CLI with enhanced debugging commands"""
    
    def __init__(self):
        super().__init__()
        self.debug_mode = False
        self.profiler = PerformanceProfiler()
    
    def do_debug_on(self, args):
        """Enable debug mode"""
        self.debug_mode = True
        setup_debug_logging()
        print("Debug mode enabled")
    
    def do_debug_off(self, args):
        """Disable debug mode"""
        self.debug_mode = False
        print("Debug mode disabled")
    
    def do_debug_agent(self, args):
        """Show agent debug information"""
        if hasattr(self.current_agent, 'get_debug_info'):
            debug_info = self.current_agent.get_debug_info()
            print(json.dumps(debug_info, indent=2))
        else:
            print("Current agent does not support debugging")
    
    def do_debug_tools(self, args):
        """Show tool debug information"""
        if hasattr(self.current_agent, 'tools'):
            for tool in self.current_agent.tools:
                if hasattr(tool, 'get_debug_info'):
                    debug_info = tool.get_debug_info()
                    print(f"\
{tool.name}:")
                    print(json.dumps(debug_info, indent=2))
        else:
            print("No tools available for debugging")
    
    def do_debug_memory(self, args):
        """Take memory snapshot"""
        snapshot = self.profiler.take_memory_snapshot(args or "manual")
        print(f"Memory snapshot: {snapshot['rss'] / 1024 / 1024:.2f} MB")
    
    def do_debug_performance(self, args):
        """Show performance report"""
        report = self.profiler.generate_performance_report()
        print(report)
    
    def do_debug_save(self, args):
        """Save debug information to file"""
        filename = args or f"debug_session_{int(time.time())}.json"
        
        debug_data = {
            "timestamp": time.time(),
            "agent_info": self.current_agent.get_debug_info() if hasattr(self.current_agent, 'get_debug_info') else None,
            "tool_info": [tool.get_debug_info() for tool in getattr(self.current_agent, 'tools', []) if hasattr(tool, 'get_debug_info')],
            "memory_snapshots": self.profiler.memory_snapshots,
            "performance_profiles": self.profiler.profiles
        }
        
        with open(filename, 'w') as f:
            json.dump(debug_data, f, indent=2)
        
        print(f"Debug information saved to {filename}")
```

## Best Practices

### Debug-Friendly Code
- Add comprehensive logging at key points
- Use structured logging with context
- Implement debug modes in components
- Provide introspection methods
- Store execution history for analysis

### Performance Monitoring
- Profile critical code paths
- Monitor memory usage patterns
- Track API call performance
- Measure end-to-end latency
- Set up alerts for performance degradation

### Error Investigation
- Capture full stack traces
- Log relevant context information
- Implement error categorization
- Store error patterns for analysis
- Provide clear error messages

### Production Debugging
- Use log levels appropriately
- Implement feature flags for debug features
- Provide remote debugging capabilities
- Monitor system health metrics
- Set up automated error reporting

## See Also

- [Common Issues](./common-issues.md)
- [Performance Optimization](./performance.md)
- [Logging Configuration](../getting-started/configuration.md)
- [System Monitoring](../api-reference/cli/commands.md)"}

---

FILE: docs/troubleshooting/performance.md

# Performance Optimization Guide

Comprehensive guide for optimizing SpoonOS performance across agents, tools, and infrastructure.

## Performance Monitoring

### System Metrics

```bash
# Check system performance
python main.py
> system-info

# Monitor resource usage
top -p $(pgrep -f "python main.py")
htop
```

### Built-in Performance Monitoring

```python
# performance_monitor.py
import psutil
import time
from dataclasses import dataclass
from typing import Dict, List

@dataclass
class PerformanceMetrics:
    timestamp: float
    cpu_percent: float
    memory_rss: int
    memory_percent: float
    disk_io_read: int
    disk_io_write: int
    network_sent: int
    network_recv: int

class PerformanceMonitor:
    def __init__(self):
        self.metrics_history: List[PerformanceMetrics] = []
        self.process = psutil.Process()
        self.initial_io = self.process.io_counters()
        self.initial_net = psutil.net_io_counters()

    def collect_metrics(self) -> PerformanceMetrics:
        """Collect current performance metrics"""
        current_io = self.process.io_counters()
        current_net = psutil.net_io_counters()

        metrics = PerformanceMetrics(
            timestamp=time.time(),
            cpu_percent=self.process.cpu_percent(),
            memory_rss=self.process.memory_info().rss,
            memory_percent=self.process.memory_percent(),
            disk_io_read=current_io.read_bytes - self.initial_io.read_bytes,
            disk_io_write=current_io.write_bytes - self.initial_io.write_bytes,
            network_sent=current_net.bytes_sent - self.initial_net.bytes_sent,
            network_recv=current_net.bytes_recv - self.initial_net.bytes_recv
        )

        self.metrics_history.append(metrics)
        return metrics

    def get_performance_summary(self, window_minutes: int = 5) -> Dict:
        """Get performance summary for the last N minutes"""
        cutoff_time = time.time() - (window_minutes * 60)
        recent_metrics = [m for m in self.metrics_history if m.timestamp > cutoff_time]

        if not recent_metrics:
            return {"error": "No metrics available"}

        return {
            "window_minutes": window_minutes,
            "sample_count": len(recent_metrics),
            "cpu": {
                "avg": sum(m.cpu_percent for m in recent_metrics) / len(recent_metrics),
                "max": max(m.cpu_percent for m in recent_metrics),
                "min": min(m.cpu_percent for m in recent_metrics)
            },
            "memory": {
                "current_mb": recent_metrics[-1].memory_rss / 1024 / 1024,
                "peak_mb": max(m.memory_rss for m in recent_metrics) / 1024 / 1024,
                "avg_percent": sum(m.memory_percent for m in recent_metrics) / len(recent_metrics)
            },
            "io": {
                "total_read_mb": recent_metrics[-1].disk_io_read / 1024 / 1024,
                "total_write_mb": recent_metrics[-1].disk_io_write / 1024 / 1024
            }
        }
```

## Agent Performance Optimization

### Efficient Agent Configuration

```json
{
  "agents": {
    "optimized_agent": {
      "class": "SpoonReactAI",
      "config": {
        "max_steps": 5,
        "temperature": 0.7,
        "max_tokens": 1000,
        "timeout": 30,
        "stream": true,
        "cache_enabled": true
      }
    }
  }
}
```

### Memory-Efficient Agent Implementation

```python
# optimized_agent.py
from spoon_ai.agents import SpoonReactAI
from typing import List, Dict
import gc

class OptimizedAgent(SpoonReactAI):
    """Memory-optimized agent implementation"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.max_history_length = kwargs.get('max_history_length', 50)
        self.cleanup_interval = kwargs.get('cleanup_interval', 10)
        self.request_count = 0

    async def run(self, message: str, **kwargs):
        """Run with memory management"""
        try:
            result = await super().run(message, **kwargs)

            # Periodic cleanup
            self.request_count += 1
            if self.request_count % self.cleanup_interval == 0:
                self._cleanup_memory()

            return result

        except Exception as e:
            # Force cleanup on error
            self._cleanup_memory()
            raise

    def _cleanup_memory(self):
        """Clean up memory usage"""
        # Limit conversation history
        if hasattr(self, 'conversation_history'):
            if len(self.conversation_history) > self.max_history_length:
                # Keep only recent messages
                self.conversation_history = self.conversation_history[-self.max_history_length:]

        # Force garbage collection
        gc.collect()

        logger.debug("Memory cleanup performed", agent=self.name)
```

### Conversation History Management

```python
# history_manager.py
from collections import deque
from typing import Dict, List, Optional
import json
import hashlib

class ConversationHistoryManager:
    """Efficient conversation history management"""

    def __init__(self, max_length: int = 100, compression_enabled: bool = True):
        self.max_length = max_length
        self.compression_enabled = compression_enabled
        self.messages = deque(maxlen=max_length)
        self.compressed_history = {}

    def add_message(self, message: Dict):
        """Add message with automatic compression"""
        # Add to recent messages
        self.messages.append(message)

        # Compress old messages if enabled
        if self.compression_enabled and len(self.messages) == self.max_length:
            self._compress_old_messages()

    def get_recent_messages(self, count: int = 10) -> List[Dict]:
        """Get recent messages efficiently"""
        return list(self.messages)[-count:]

    def get_context_summary(self) -> str:
        """Get compressed context summary"""
        if not self.compressed_history:
            return ""

        # Generate summary from compressed history
        summaries = []
        for period, summary in self.compressed_history.items():
            summaries.append(f"Period {period}: {summary}")

        return "\
".join(summaries)

    def _compress_old_messages(self):
        """Compress older messages into summaries"""
        # Take first half of messages for compression
        to_compress = list(self.messages)[:self.max_length // 2]

        # Generate summary (simplified)
        summary = self._generate_summary(to_compress)

        # Store compressed summary
        period_key = len(self.compressed_history)
        self.compressed_history[period_key] = summary

        # Remove compressed messages from deque
        for _ in range(len(to_compress)):
            self.messages.popleft()

    def _generate_summary(self, messages: List[Dict]) -> str:
        """Generate summary of message batch"""
        # Simple summarization (could be enhanced with LLM)
        user_messages = [m['content'] for m in messages if m.get('role') == 'user']
        assistant_messages = [m['content'] for m in messages if m.get('role') == 'assistant']

        return f"User asked {len(user_messages)} questions, assistant provided {len(assistant_messages)} responses"
```

## Tool Performance Optimization

### Async Tool Implementation

```python
# async_tool.py
import asyncio
import aiohttp
from spoon_ai.tools.base import BaseTool
from typing import List, Dict, Any

class AsyncHTTPTool(BaseTool):
    """High-performance async HTTP tool"""

    name = "async_http_tool"
    description = "Optimized HTTP requests with connection pooling"

    def __init__(self):
        self.session = None
        self.connector = None

    async def __aenter__(self):
        # Create optimized connector
        self.connector = aiohttp.TCPConnector(
            limit=100,  # Total connection pool size
            limit_per_host=30,  # Per-host connection limit
            ttl_dns_cache=300,  # DNS cache TTL
            use_dns_cache=True,
            keepalive_timeout=30,
            enable_cleanup_closed=True
        )

        # Create session with optimized settings
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        self.session = aiohttp.ClientSession(
            connector=self.connector,
            timeout=timeout,
            headers={'User-Agent': 'SpoonOS/1.0'}
        )

        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
        if self.connector:
            await self.connector.close()

    async def execute(self, urls: List[str], method: str = "GET", **kwargs) -> List[Dict]:
        """Execute multiple HTTP requests concurrently"""
        if not self.session:
            async with self:
                return await self._make_requests(urls, method, **kwargs)
        else:
            return await self._make_requests(urls, method, **kwargs)

    async def _make_requests(self, urls: List[str], method: str, **kwargs) -> List[Dict]:
        """Make concurrent HTTP requests"""
        semaphore = asyncio.Semaphore(10)  # Limit concurrent requests

        async def make_request(url: str) -> Dict:
            async with semaphore:
                try:
                    async with self.session.request(method, url, **kwargs) as response:
                        return {
                            "url": url,
                            "status": response.status,
                            "data": await response.text(),
                            "headers": dict(response.headers)
                        }
                except Exception as e:
                    return {
                        "url": url,
                        "error": str(e),
                        "status": 0
                    }

        # Execute all requests concurrently
        tasks = [make_request(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        return [r for r in results if not isinstance(r, Exception)]
```

### Caching Implementation

```python
# caching_tool.py
import asyncio
import hashlib
import json
import time
from typing import Any, Dict, Optional
from spoon_ai.tools.base import BaseTool

class CachedTool(BaseTool):
    """Tool with intelligent caching"""

    def __init__(self, cache_ttl: int = 3600, max_cache_size: int = 1000):
        self.cache_ttl = cache_ttl
        self.max_cache_size = max_cache_size
        self.cache: Dict[str, Dict] = {}
        self.cache_stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0
        }

    async def execute(self, **kwargs) -> Any:
        """Execute with caching"""
        # Generate cache key
        cache_key = self._generate_cache_key(**kwargs)

        # Check cache
        cached_result = self._get_from_cache(cache_key)
        if cached_result is not None:
            self.cache_stats["hits"] += 1
            logger.debug("Cache hit", tool=self.name, cache_key=cache_key[:8])
            return cached_result

        # Cache miss - execute tool
        self.cache_stats["misses"] += 1
        logger.debug("Cache miss", tool=self.name, cache_key=cache_key[:8])

        result = await self._execute_impl(**kwargs)

        # Store in cache
        self._set_cache(cache_key, result)

        return result

    def _generate_cache_key(self, **kwargs) -> str:
        """Generate deterministic cache key"""
        # Sort kwargs for consistent key generation
        key_data = json.dumps(kwargs, sort_keys=True, default=str)
        return hashlib.sha256(key_data.encode()).hexdigest()

    def _get_from_cache(self, key: str) -> Optional[Any]:
        """Get value from cache if not expired"""
        if key not in self.cache:
            return None

        entry = self.cache[key]
        if time.time() - entry["timestamp"] > self.cache_ttl:
            # Expired - remove from cache
            del self.cache[key]
            return None

        return entry["value"]

    def _set_cache(self, key: str, value: Any):
        """Set value in cache with eviction"""
        # Evict oldest entries if cache is full
        if len(self.cache) >= self.max_cache_size:
            self._evict_oldest()

        self.cache[key] = {
            "value": value,
            "timestamp": time.time()
        }

    def _evict_oldest(self):
        """Evict oldest cache entries"""
        # Sort by timestamp and remove oldest 10%
        sorted_items = sorted(
            self.cache.items(),
            key=lambda x: x[1]["timestamp"]
        )

        evict_count = max(1, len(sorted_items) // 10)
        for i in range(evict_count):
            key = sorted_items[i][0]
            del self.cache[key]
            self.cache_stats["evictions"] += 1

    def get_cache_stats(self) -> Dict:
        """Get cache performance statistics"""
        total_requests = self.cache_stats["hits"] + self.cache_stats["misses"]
        hit_rate = self.cache_stats["hits"] / total_requests if total_requests > 0 else 0

        return {
            "cache_size": len(self.cache),
            "max_cache_size": self.max_cache_size,
            "hit_rate": hit_rate,
            "total_requests": total_requests,
            **self.cache_stats
        }

    async def _execute_impl(self, **kwargs) -> Any:
        """Override this method in subclasses"""
        raise NotImplementedError
```

## LLM Performance Optimization

### Request Batching

```python
# batch_llm_provider.py
import asyncio
from typing import List, Dict, Any
from spoon_ai.llm.base import BaseLLMProvider

class BatchedLLMProvider(BaseLLMProvider):
    """LLM provider with request batching"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.batch_size = kwargs.get('batch_size', 5)
        self.batch_timeout = kwargs.get('batch_timeout', 1.0)
        self.pending_requests = []
        self.batch_lock = asyncio.Lock()

    async def generate(self, messages: List[Dict], **kwargs) -> Dict:
        """Generate with batching optimization"""
        # For single requests, use batching
        if len(messages) == 1:
            return await self._batched_generate(messages[0], **kwargs)
        else:
            # For multi-message requests, process directly
            return await super().generate(messages, **kwargs)

    async def _batched_generate(self, message: Dict, **kwargs) -> Dict:
        """Generate with request batching"""
        # Create request future
        request_future = asyncio.Future()
        request_data = {
            "message": message,
            "kwargs": kwargs,
            "future": request_future
        }

        async with self.batch_lock:
            self.pending_requests.append(request_data)

            # If batch is full or this is the first request, process batch
            if len(self.pending_requests) >= self.batch_size:
                await self._process_batch()
            elif len(self.pending_requests) == 1:
                # Start batch timer for first request
                asyncio.create_task(self._batch_timer())

        # Wait for result
        return await request_future

    async def _batch_timer(self):
        """Timer to process batch after timeout"""
        await asyncio.sleep(self.batch_timeout)

        async with self.batch_lock:
            if self.pending_requests:
                await self._process_batch()

    async def _process_batch(self):
        """Process pending requests as a batch"""
        if not self.pending_requests:
            return

        batch = self.pending_requests.copy()
        self.pending_requests.clear()

        try:
            # Process all requests concurrently
            tasks = [
                self._process_single_request(req["message"], req["kwargs"])
                for req in batch
            ]

            results = await asyncio.gather(*tasks, return_exceptions=True)

            # Set results for each future
            for req, result in zip(batch, results):
                if isinstance(result, Exception):
                    req["future"].set_exception(result)
                else:
                    req["future"].set_result(result)

        except Exception as e:
            # Set exception for all futures
            for req in batch:
                if not req["future"].done():
                    req["future"].set_exception(e)

    async def _process_single_request(self, message: Dict, kwargs: Dict) -> Dict:
        """Process a single request"""
        return await super().generate([message], **kwargs)
```

### Response Streaming

```python
# streaming_provider.py
import asyncio
from typing import AsyncGenerator, Dict, Any
from spoon_ai.llm.base import BaseLLMProvider

class StreamingLLMProvider(BaseLLMProvider):
    """LLM provider with streaming support"""

    async def generate_stream(self, messages: List[Dict], **kwargs) -> AsyncGenerator[str, None]:
        """Generate streaming response"""
        # Implementation depends on provider API
        async for chunk in self._stream_implementation(messages, **kwargs):
            yield chunk

    async def _stream_implementation(self, messages: List[Dict], **kwargs) -> AsyncGenerator[str, None]:
        """Provider-specific streaming implementation"""
        # Example implementation
        response = await self._make_streaming_request(messages, **kwargs)

        async for line in response:
            if line.startswith('data: '):
                chunk_data = line[6:]  # Remove 'data: ' prefix
                if chunk_data.strip() == '[DONE]':
                    break

                try:
                    chunk_json = json.loads(chunk_data)
                    if 'choices' in chunk_json and chunk_json['choices']:
                        delta = chunk_json['choices'][0].get('delta', {})
                        if 'content' in delta:
                            yield delta['content']
                except json.JSONDecodeError:
                    continue
```

## Database and Storage Optimization

### Connection Pooling

```python
# db_pool.py
import asyncpg
import asyncio
from typing import Optional, Dict, Any

class DatabasePool:
    """Optimized database connection pool"""

    def __init__(self, connection_string: str, **pool_kwargs):
        self.connection_string = connection_string
        self.pool_kwargs = {
            'min_size': 5,
            'max_size': 20,
            'command_timeout': 60,
            'server_settings': {
                'jit': 'off',  # Disable JIT for faster startup
                'application_name': 'spoon_ai'
            },
            **pool_kwargs
        }
        self.pool: Optional[asyncpg.Pool] = None

    async def initialize(self):
        """Initialize connection pool"""
        if self.pool is None:
            self.pool = await asyncpg.create_pool(
                self.connection_string,
                **self.pool_kwargs
            )

    async def execute_query(self, query: str, *args) -> List[Dict[str, Any]]:
        """Execute query with connection pooling"""
        if self.pool is None:
            await self.initialize()

        async with self.pool.acquire() as connection:
            rows = await connection.fetch(query, *args)
            return [dict(row) for row in rows]

    async def execute_transaction(self, queries: List[tuple]) -> List[Any]:
        """Execute multiple queries in a transaction"""
        if self.pool is None:
            await self.initialize()

        async with self.pool.acquire() as connection:
            async with connection.transaction():
                results = []
                for query, args in queries:
                    result = await connection.fetch(query, *args)
                    results.append([dict(row) for row in result])
                return results

    async def close(self):
        """Close connection pool"""
        if self.pool:
            await self.pool.close()
            self.pool = None
```

### Efficient Data Serialization

```python
# serialization.py
import orjson  # Faster JSON library
import pickle
import gzip
from typing import Any, Union

class OptimizedSerializer:
    """High-performance data serialization"""

    @staticmethod
    def serialize_json(data: Any) -> bytes:
        """Fast JSON serialization"""
        return orjson.dumps(data)

    @staticmethod
    def deserialize_json(data: bytes) -> Any:
        """Fast JSON deserialization"""
        return orjson.loads(data)

    @staticmethod
    def serialize_compressed(data: Any) -> bytes:
        """Compressed pickle serialization"""
        pickled = pickle.dumps(data, protocol=pickle.HIGHEST_PROTOCOL)
        return gzip.compress(pickled)

    @staticmethod
    def deserialize_compressed(data: bytes) -> Any:
        """Compressed pickle deserialization"""
        decompressed = gzip.decompress(data)
        return pickle.loads(decompressed)

    @staticmethod
    def choose_serialization(data: Any) -> tuple[bytes, str]:
        """Choose optimal serialization method"""
        # Try JSON first (faster, more compatible)
        try:
            json_data = OptimizedSerializer.serialize_json(data)
            compressed_data = OptimizedSerializer.serialize_compressed(data)

            # Use JSON if it's not much larger
            if len(json_data) <= len(compressed_data) * 1.2:
                return json_data, 'json'
            else:
                return compressed_data, 'compressed'

        except (TypeError, ValueError):
            # Fall back to compressed pickle
            return OptimizedSerializer.serialize_compressed(data), 'compressed'
```

## Infrastructure Optimization

### Process Management

```python
# process_manager.py
import asyncio
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor
from typing import Any, Callable, List

class OptimizedProcessManager:
    """Optimized process management for CPU-intensive tasks"""

    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or mp.cpu_count()
        self.executor = None

    async def __aenter__(self):
        self.executor = ProcessPoolExecutor(max_workers=self.max_workers)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.executor:
            self.executor.shutdown(wait=True)

    async def run_cpu_intensive(self, func: Callable, *args, **kwargs) -> Any:
        """Run CPU-intensive function in separate process"""
        if self.executor is None:
            async with self:
                return await self._execute(func, *args, **kwargs)
        else:
            return await self._execute(func, *args, **kwargs)

    async def _execute(self, func: Callable, *args, **kwargs) -> Any:
        """Execute function in process pool"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor,
            lambda: func(*args, **kwargs)
        )

    async def map_parallel(self, func: Callable, items: List[Any]) -> List[Any]:
        """Map function over items in parallel"""
        if self.executor is None:
            async with self:
                return await self._map_execute(func, items)
        else:
            return await self._map_execute(func, items)

    async def _map_execute(self, func: Callable, items: List[Any]) -> List[Any]:
        """Execute map in process pool"""
        loop = asyncio.get_event_loop()
        tasks = [
            loop.run_in_executor(self.executor, func, item)
            for item in items
        ]
        return await asyncio.gather(*tasks)
```

### Memory Management

```python
# memory_manager.py
import gc
import psutil
import os
from typing import Dict, Any

class MemoryManager:
    """Advanced memory management utilities"""

    def __init__(self, warning_threshold: float = 0.8, critical_threshold: float = 0.9):
        self.warning_threshold = warning_threshold
        self.critical_threshold = critical_threshold
        self.process = psutil.Process(os.getpid())

    def get_memory_info(self) -> Dict[str, Any]:
        """Get detailed memory information"""
        memory_info = self.process.memory_info()
        memory_percent = self.process.memory_percent()

        return {
            "rss_mb": memory_info.rss / 1024 / 1024,
            "vms_mb": memory_info.vms / 1024 / 1024,
            "percent": memory_percent,
            "available_mb": psutil.virtual_memory().available / 1024 / 1024,
            "warning_level": self._get_warning_level(memory_percent)
        }

    def _get_warning_level(self, memory_percent: float) -> str:
        """Get memory warning level"""
        if memory_percent >= self.critical_threshold * 100:
            return "critical"
        elif memory_percent >= self.warning_threshold * 100:
            return "warning"
        else:
            return "normal"

    def cleanup_memory(self, force: bool = False) -> Dict[str, Any]:
        """Perform memory cleanup"""
        before_info = self.get_memory_info()

        # Force garbage collection
        collected = gc.collect()

        # Additional cleanup for critical memory usage
        if force or before_info["warning_level"] == "critical":
            # Clear caches
            self._clear_internal_caches()

            # Force another GC cycle
            collected += gc.collect()

        after_info = self.get_memory_info()

        return {
            "before_mb": before_info["rss_mb"],
            "after_mb": after_info["rss_mb"],
            "freed_mb": before_info["rss_mb"] - after_info["rss_mb"],
            "objects_collected": collected,
            "warning_level": after_info["warning_level"]
        }

    def _clear_internal_caches(self):
        """Clear internal caches"""
        # Clear function caches
        import functools
        for obj in gc.get_objects():
            if isinstance(obj, functools._lru_cache_wrapper):
                obj.cache_clear()

        # Clear regex cache
        import re
        re.purge()

    def monitor_memory(self) -> bool:
        """Monitor memory and return True if action needed"""
        info = self.get_memory_info()

        if info["warning_level"] == "critical":
            logger.warning(
                "Critical memory usage detected",
                memory_mb=info["rss_mb"],
                percent=info["percent"]
            )
            return True
        elif info["warning_level"] == "warning":
            logger.info(
                "High memory usage detected",
                memory_mb=info["rss_mb"],
                percent=info["percent"]
            )

        return False
```

## Configuration Optimization

### Production Configuration

```json
{
  "performance": {
    "agents": {
      "max_concurrent": 5,
      "memory_limit_mb": 512,
      "cleanup_interval": 10,
      "max_history_length": 50
    },
    "llm": {
      "connection_pool_size": 10,
      "request_timeout": 30,
      "retry_attempts": 3,
      "batch_size": 5,
      "cache_enabled": true,
      "cache_ttl": 3600
    },
    "tools": {
      "concurrent_limit": 10,
      "cache_enabled": true,
      "cache_size": 1000,
      "timeout": 30
    },
    "database": {
      "pool_min_size": 5,
      "pool_max_size": 20,
      "command_timeout": 60,
      "connection_timeout": 10
    }
  }
}
```

### Environment Variables

```bash
# Performance tuning
export PYTHONOPTIMIZE=1
export PYTHONDONTWRITEBYTECODE=1
export PYTHONUNBUFFERED=1

# Memory management
export MALLOC_TRIM_THRESHOLD_=100000
export MALLOC_MMAP_THRESHOLD_=131072

# Async settings
export PYTHONASYNCIODEBUG=0
export UV_THREADPOOL_SIZE=16

# Logging optimization
export LOG_LEVEL=INFO
export LOG_FORMAT=json
```

## Monitoring and Alerting

### Performance Alerts

```python
# alerts.py
import asyncio
from typing import Dict, Callable, Any

class PerformanceAlerting:
    """Performance monitoring and alerting"""

    def __init__(self):
        self.thresholds = {
            "memory_percent": 80,
            "cpu_percent": 80,
            "response_time": 5.0,
            "error_rate": 0.1
        }
        self.alert_handlers = []

    def add_alert_handler(self, handler: Callable[[str, Dict[str, Any]], None]):
        """Add alert handler function"""
        self.alert_handlers.append(handler)

    async def check_performance(self, metrics: Dict[str, Any]):
        """Check performance metrics against thresholds"""
        alerts = []

        # Check memory usage
        if metrics.get("memory_percent", 0) > self.thresholds["memory_percent"]:
            alerts.append({
                "type": "memory_high",
                "value": metrics["memory_percent"],
                "threshold": self.thresholds["memory_percent"],
                "message": f"Memory usage {metrics['memory_percent']:.1f}% exceeds threshold"
            })

        # Check CPU usage
        if metrics.get("cpu_percent", 0) > self.thresholds["cpu_percent"]:
            alerts.append({
                "type": "cpu_high",
                "value": metrics["cpu_percent"],
                "threshold": self.thresholds["cpu_percent"],
                "message": f"CPU usage {metrics['cpu_percent']:.1f}% exceeds threshold"
            })

        # Check response time
        if metrics.get("avg_response_time", 0) > self.thresholds["response_time"]:
            alerts.append({
                "type": "response_time_high",
                "value": metrics["avg_response_time"],
                "threshold": self.thresholds["response_time"],
                "message": f"Response time {metrics['avg_response_time']:.2f}s exceeds threshold"
            })

        # Send alerts
        for alert in alerts:
            await self._send_alert(alert)

    async def _send_alert(self, alert: Dict[str, Any]):
        """Send alert to all handlers"""
        for handler in self.alert_handlers:
            try:
                await handler(alert["type"], alert)
            except Exception as e:
                logger.error(f"Alert handler failed: {e}")
```

## Best Practices

### Code Optimization
- Use async/await for I/O operations
- Implement connection pooling
- Cache expensive computations
- Batch similar operations
- Use efficient data structures

### Memory Management
- Limit conversation history
- Implement periodic cleanup
- Monitor memory usage
- Use generators for large datasets
- Clear caches regularly

### Network Optimization
- Use connection pooling
- Implement request batching
- Enable compression
- Set appropriate timeouts
- Handle rate limiting

### Database Optimization
- Use connection pooling
- Implement query caching
- Optimize query patterns
- Use transactions efficiently
- Monitor query performance

## See Also

- [Debugging Guide](./debugging.md)
- [Common Issues](./common-issues.md)
- [System Requirements](../getting-started/installation.md)"}

---

FILE: README.md

# SpoonOS Developer Documentation

Welcome to the **SpoonOS Developer Documentation** - the comprehensive guide for building, deploying, and scaling Web3 AI agents with the SpoonOS Agentic Operating System.

This documentation site provides everything you need to get started with SpoonOS, the **Agentic OS for a Sentient Economy**.

## ðŸš€ What is SpoonOS?

SpoonOS is an **Agentic Operating System that enables AI agents to perceive, reason, plan, and execute**. It provides a robust framework for creating, deploying, and managing Web3 AI agents. SpoonOS fosters interoperability, data scalability, and privacyâ€”empowering AI agents to engage in collaborative learning while ensuring secure and efficient data processing.

### Key Features

- ðŸ§  **Intelligent Agent Framework** - Advanced reasoning and action capabilities
- ðŸŒ **Web3-Native Architecture** - Built-in blockchain and DeFi integration
- ðŸ”§ **Modular Tool System** - Extensible architecture with MCP protocol support
- ðŸ¤– **Multi-Model AI Support** - Compatible with OpenAI, Anthropic, DeepSeek, and more
- ðŸ“Š **Graph-Based Workflows** - Complex workflow orchestration and management
- ðŸ’» **Interactive CLI** - Powerful development and deployment interface
- ðŸ”’ **Privacy-First Design** - Secure data processing and collaborative learning
- âš¡ **High Performance** - Optimized for scalability and efficiency

## ðŸ“š Documentation Structure

- **Getting Started** - Installation, configuration, and quick start
- **Core Concepts** - Agents, tools, and system architecture
- **Guides** - Step-by-step tutorials and best practices
- **API Reference** - Complete API documentation
- **Examples** - Real-world use cases and sample code

## ðŸ› ï¸ Development

### Installation

```bash
npm install
```

### Local Development

```bash
npm start
```

This command starts a local development server and opens up a browser window. Most changes are reflected live without having to restart the server.

### Build

```bash
npm run build
```

This command generates static content into the `build` directory and can be served using any static contents hosting service.

### Deployment

Using SSH:

```bash
USE_SSH=true npm run deploy
```

Not using SSH:

```bash
GIT_USER=<Your GitHub username> npm run deploy
```

If you are using GitHub pages for hosting, this command is a convenient way to build the website and push to the `gh-pages` branch.

## ðŸ¤ Contributing

We welcome contributions to improve the documentation! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ðŸ“„ License

This documentation is part of the SpoonOS project and follows the same license terms.

## ðŸ”— Links

- **Main Repository**: [XSpoonAi/spoon-core](https://github.com/XSpoonAi/spoon-core)
- **Official Website**: [SpoonOS Landing](https://spoonai.io)
- **Documentation**: [Developer Documentation](https://xspoonai.github.io/spoon-doc/)
- **Community**: [GitHub Discussions](https://github.com/XSpoonAi/spoon-core/discussions)

---

**Build, deploy and scale Web3 AI agents in an evolving paradigm.**

Built with â¤ï¸ by the SpoonOS Team

---

FILE: src/pages/markdown-page.md

---
title: Markdown page example
---

# Markdown page example

You don't need React to write simple standalone pages.

---

FILE: Toolkit/crypto/data-tools.md

`spoon_toolkits.crypto.crypto_data_tools` is SpoonAIâ€™s Bitquery-centric toolbox for price discovery, liquidity intelligence, lending-rate aggregation, and wallet forensics. Every class in this package plugs directly into the Spoon tool runtime, so agents can reuse the same `BaseTool` lifecycle (validation, structured outputs, telemetry) without additional glue code.


## Environment & Dependencies

```bash
export BITQUERY_API_KEY=your_rest_key              # legacy modules
export BITQUERY_CLIENT_ID=your_oauth_client_id     # required by Bitquery OAuth
export BITQUERY_CLIENT_SECRET=your_oauth_secret
export RPC_URL=https://eth.llamarpc.com            
```

- `Get*`/`Alert*` tools depend on `web3` (for Uniswap) and respect `RPC_URL`. Provide a reliable mainnet endpoint to avoid public-rate limits.
- `PredictPrice` requires `pandas`, `scikit-learn`, and `numpy`. Install toolkit extras via `pip install -r requirements.txt` at the project root or install modules individually.
- `LendingRateMonitorTool` performs concurrent HTTP requests using `aiohttp`; event loops must be active (use `nest_asyncio` in notebooks if necessary).

## Tool Catalog 

### Spot & Historical Pricing
- `GetTokenPriceTool` resolves ERC-20 pairs to the canonical Uniswap v3 pool, fetches slot0, and converts ticks into human prices.
- `Get24hStatsTool` and `GetKlineDataTool` expose the same provider stack for downstream analytics (volatility, TWAP, etc.).

### Alerting & Monitoring
- `PriceThresholdAlertTool` compares live prices with a configurable Â±% drift versus prior day.
- `LpRangeCheckTool` reads Uniswap positions, current ticks, and warns when LPs approach range boundaries (`buffer_ticks`).
- `SuddenPriceIncreaseTool` scans Bitquery datasets for large-cap, high-volume tokens with rapid appreciationâ€”ideal for whale or listing alerts.
- `CryptoMarketMonitor` abstracts â€œset-and-forgetâ€ monitors by POSTing well-formed payloads to the monitoring daemon you run (defaults to `localhost:8888` but can be swapped).

### Wallet Intelligence
- `WalletAnalysis`, `TokenHolders`, and `TradingHistory` share the Bitquery GraphQL templates embedded in their modules so you always know the dataset (`combined` vs `archive`) and filters before execution.
- Pagination is driven by the `limit` clauses inside the template; adjust the class constants if you need bigger windows.

### DeFi Rates & Liquidity
- `LendingRateMonitorTool` merges DeFiLlama pools with Aave/Compound/Morpho APIs, derives utilization, and wraps the result in `ToolResult` for consumption by governance agents.
- `UniswapLiquidity` emits the latest Mint/Burn events so you can approximate real-time liquidity deltas without running a listener yourself.
- `PredictPrice` is intentionally heavyweight (builds a RandomForest and scaler); cache the fitted estimator in your application if you call it frequently.

## Usage Patterns

### Synchronous price lookup
```python
from spoon_toolkits.crypto.crypto_data_tools import GetTokenPriceTool

tool = GetTokenPriceTool()
result = await tool.execute(symbol="ETH-USDC")

print(result.output["price"])         # structured response
print(result.diagnostic["pool"])      # extra context for logging
```

### Long-running alert inside an agent
```python
import asyncio
from spoon_toolkits.crypto.crypto_data_tools import PriceThresholdAlertTool

alerts = PriceThresholdAlertTool()

async def monitor():
    while True:
        tool_result = await alerts.execute(symbol="ETH-USDC", threshold_percent=7)
        if tool_result.output.get("exceeded"):
            await send_notification(tool_result.output["message"])
        await asyncio.sleep(60)

asyncio.run(monitor())
```

Tips:
- Most tools return dictionaries; when a class subclasses `DexBaseTool`, the `ToolResult.output` mirrors the Bitquery JSON fragment shown in the GraphQL template.
- Async utilities (`PriceThresholdAlertTool`, `LendingRateMonitorTool`) already shield you from rate spikes with short sleeps; avoid spawning excessive parallel loops unless you also raise Bitquery limits.

## Operational Notes

- Centralize credential management: add the Bitquery OAuth keys to your `.env` and load them before instantiating tools to prevent runtime `ValueError`s from `DexBaseTool.oAuth()`.
- Because Uniswap calls hit your RPC, failures there do **not** imply Bitquery downtimeâ€”inspect `ToolResult.diagnostic` for the precise failure domain.
- The monitoring helper in `blockchain_monitor.py` assumes a scheduler reachable at `http://localhost:8888/monitoring/tasks`. Override `api_url` in the module before deploying if your infra differs.
- `PredictPrice` fetches up to ~1000 rows per query. If Bitquery throttles you, lower the template limit or use a paid API plan.

---

FILE: Toolkit/crypto/evm.md

`spoon_toolkits.crypto.evm` gathers the core on-chain primitives agents needâ€”native transfers, ERC-20 operations, swaps, quotes, bridges, and balance lookupsâ€”without shelling out to the JS plugin. Each class inherits `BaseTool`, so input validation, diagnostics, and async receipt waiting all behave consistently across Spoon agents.

## Environment & Dependencies

```bash
export EVM_PROVIDER_URL=https://mainnet.infura.io/v3/...
export EVM_PRIVATE_KEY=0xyourSignerKey
export EVM_CHAIN_ID=1            # optional; inferred via RPC when omitted
# Optional global fallback used by other crypto toolkits
export RPC_URL=https://eth.llamarpc.com
```

- Every tool accepts `rpc_url`/`private_key` overrides per call; defaults resolve to `EVM_PROVIDER_URL` and `EVM_PRIVATE_KEY`, then `RPC_URL`.
- `web3.py` is lazily imported inside each execute path, so include it (and `requests`) in your runtime environment.
- Aggregator-backed tools call public REST APIs (Bebop or LiFi). If you operate behind a proxy, ensure outbound HTTPS is allowed.

## Quick Start â€“ Native Transfer

```python
from spoon_toolkits.crypto.evm import EvmTransferTool

transfer = EvmTransferTool()
tx = await transfer.execute(
    to_address="0xrecipient...",
    amount_ether="0.05",
    data="0x",                      # optional calldata
)

print(tx.output["hash"])
```

Receipts are awaited with reasonable timeouts; if the transaction reverts, `ToolResult.error` includes the tx hash so you can inspect it with an explorer.

## Toolkit Overview

| Tool | Module | Purpose |
| --- | --- | --- |
| `EvmTransferTool` | `transfer.py` | Send native tokens with optional data payloads, auto gas estimation, and nonce management. |
| `EvmErc20TransferTool` | `erc20.py` | Transfer ERC-20 tokens (balanceOf/decimals aware) with gas estimation and optional gas price overrides. |
| `EvmBalanceTool` | `balance.py` | Read native or ERC-20 balances for any address. |
| `EvmSwapTool` | `swap.py` | Execute same-chain swaps through the Bebop aggregator, including approvals when needed. |
| `EvmSwapQuoteTool` | `quote.py` | Fetch swap quotes without execution; compares Bebop and LiFi outputs when requested. |
| `EvmBridgeTool` | `bridge.py` | Bridge assets across chains via LiFi advanced routes and step transactions. |

## Tool Notes

### `EvmTransferTool`
- Defaults to `EVM_PROVIDER_URL`/`EVM_PRIVATE_KEY` but also checks `RPC_URL` so the tool can run inside broader Spoon stacks.
- Automatically estimates gas; if estimation fails it falls back to 21k for plain transfers or 100k when `data` is present.
- Explicit overrides exist for `gas_limit`, `gas_price_gwei`, and `nonce` so advanced workflows (batched transactions, EOA abstraction) remain possible.

### `EvmErc20TransferTool`
- Resolves token decimals via the contract; if the call fails it assumes 18 decimals, so pass `amount` accordingly.
- Builds and signs a `transfer` call, then waits for completion. Emits `{hash, token, amount, decimals}` in `output`.
- Accepts `gas_price_gwei`; otherwise uses the RPCâ€™s `gas_price`.

### `EvmBalanceTool`
- Returns native balances (Ether-equivalent) when `token_address` is omitted, or ERC-20 balances plus decimals when provided.
- Useful for guardrails before making transfers or swaps.

### `EvmSwapTool`
- Calls Bebopâ€™s router for supported chains (`1`, `10`, `137`, `42161`, `8453`, `59144`). Unsupported IDs return a descriptive error.
- Handles ERC-20 approvals automatically by checking allowance against `approvalTarget` and submitting an `approve` tx when required.
- Accepts `gas_price_gwei` overrides; otherwise reuses the aggregator-suggested gas price or the RPC default.
- `slippage_bps` is currently informational because Bebop does not accept slippage; expect fills to match aggregator routing.

### `EvmSwapQuoteTool`
- Works in quote-only contexts where you want to compare outputs without signing anything.
- Resolve chain ID via `chain_id` parameter or by pointing `rpc_url` at the target network.
- `aggregator="both"` (default) fetches Bebop and LiFi; response includes `quotes` plus a precalculated `best` entry ranked by min output amount.
- Requires RPC access for ERC-20 decimals when quoting token sales; native sells use zero address.

### `EvmBridgeTool`
- Wraps LiFiâ€™s `/advanced/routes` + `/advanced/stepTransaction`. After selecting the recommended path, it executes the first step locally and waits for the on-chain receipt.
- Performs ERC-20 approvals before bridging when necessary; approvals and bridge txs both honor `gas_price_gwei`.
- `from_address` is implied from the signer, while `to_address` defaults to the same account but can target any destination address.
- Returns hashes plus source/destination chain IDs so orchestrators can track the bridge until completion on the far chain.

## Operational Tips

- Centralize signer management: store hot keys in a secure vault and inject them as environment variables just before launching the agent process.
- Throttle swap/bridge calls if you expect to fire many approvals quickly; both Bebop and LiFi enforce rate limits.
- Log the `ToolResult` payloadsâ€”especially `output["hash"]`â€”so you can reconcile actions if an agent crashes mid-run.
- When rotating RPC endpoints, prefer passing `rpc_url` explicitly to keep observability clear (environment-based fallbacks can mask misconfiguration).

---

FILE: Toolkit/crypto/neo.md

`spoon_toolkits.crypto.neo` bundles async Neo N3 helpers on top of the `neo-mamba` RPC client, covering addresses, assets, blocks, governance, and smart-contract introspection. Every tool subclasses `BaseTool` so agents get consistent parameter validation and `ToolResult` payloads.

## Environment & Network Selection

```bash
export NEO_MAINNET_RPC=https://mainmagnet.ngd.network:443   # overrides default mainnet RPC
export NEO_TESTNET_RPC=https://testmagnet.ngd.network:443   # overrides default testnet RPC
export NEO_RPC_TIMEOUT=20                                   # seconds
export NEO_RPC_ALLOW_INSECURE=false                         # set true to skip TLS verification
```

- Each tool accepts `network` (`"mainnet"` / `"testnet"`) so you can switch clusters per call. If you omit it, `NeoProvider` defaults to testnet.
- The provider reuses a single `aiohttp` session with optional insecure modeâ€”handy when you test against private RPC nodes.
- Address inputs may be Base58 (e.g., `Nf2C...`) or `0x` script hashes; `NeoProvider` normalizes them internally before hitting RPCs.

## Toolkit Map

| Module | Highlights |
| --- | --- |
| `address_tools.py` | Counts, info lookups, NEP-17 transfer history, ad-hoc tagging across multiple addresses. |
| `asset_tools.py` | Asset metadata by hash/name, portfolio snapshots per address, NEP-17/NEP-11 balance queries. |
| `block_tools.py` | Block count, block-by-height/hash, best-hash, rewards, and rolling recent block summaries. |
| `transaction_tools.py` | Transaction counts, raw tx retrieval (by hash or block), and transfer event extraction. |
| `contract_tools.py` / `sc_call_tools.py` | Contract inventories, verified contract metadata, invoke history filtered by contract/address/tx. |
| `nep_tools.py` | Specialized NEP-11/NEP-17 transfer feeds (by address, block height, contract hash) plus balance helpers. |
| `governance_tools.py` / `voting_tools.py` | Committee info, candidate/voter tallies, on-chain vote call traces. |
| `log_state_tools.py` | Application log/state fetchers for debugging contract execution. |
| `neo_provider.py`, `base.py` | Shared provider, RPC request helpers, normalization utilities, and graceful fallbacks when neo-mamba lacks a direct method. |

## Usage Patterns

### Address intelligence
```python
from spoon_toolkits.crypto.neo import GetAddressInfoTool

tool = GetAddressInfoTool()
result = await tool.execute(
    address="Nf2CXE8s1R6yoZ6e52xX5yb7Z9Uv7S3N1h",
    network="mainnet",
)
balances = result.output  # includes NEP-17 balances + script hash
```

### Asset metadata & balances
```python
from spoon_toolkits.crypto.neo import GetAssetInfoByHashTool, GetAssetsInfoByUserAddressTool

asset_info = await GetAssetInfoByHashTool().execute(
    asset_hash="0xef4073a0f2b305a38ec4050e4d3d28bc40ea63f5",  # GAS
    network="mainnet",
)

portfolio = await GetAssetsInfoByUserAddressTool().execute(
    address="Nf2CXE8s1R6yoZ6e52xX5yb7Z9Uv7S3N1h",
    network="mainnet",
    Limit=50,
)
```

### Block & transaction lookups
```python
from spoon_toolkits.crypto.neo import GetBlockByHeightTool, GetRawTransactionByHashTool

block = await GetBlockByHeightTool().execute(block_height=4500000, network="mainnet")
tx = await GetRawTransactionByHashTool().execute(transaction_hash="0x...", network="mainnet")
```

### Governance snapshots
```python
from spoon_toolkits.crypto.neo import GetCommitteeInfoTool, GetVotesByCandidateAddressTool

committee = await GetCommitteeInfoTool().execute(network="mainnet")
votes = await GetVotesByCandidateAddressTool().execute(candidate_address="NVSX...", network="mainnet")
```

## Operational Notes

- Some RPC extensions (e.g., `GetTagByAddresses`) are not fully implemented in `neo-mamba`. The toolkit surfaces shim behaviors or explanatory messages so agents fail gracefully; check `ToolResult.output` for strings such as â€œnot available with current implementationâ€.
- Long-running scans (e.g., iterating transfers) reuse `NeoProvider._make_request` with built-in retries governed by `NEO_RPC_TIMEOUT`. For heavy workflows, pool results externally rather than looping inside a single tool call.
- Always `await` tool execution; the provider relies on async context managers to open/close RPC sessions cleanly. If you compose multiple calls, instantiate the tool once and re-use it to avoid repeated class construction overhead.

---

FILE: Toolkit/crypto/powerdata.md

`spoon_toolkits.crypto.crypto_powerdata` fuses CCXT-powered CEX feeds, OKX Web3 DEX data, TA-Lib/enhanced indicators, and an MCP server that can stream results over stdio or SSE. Use it when agents need richer analytics than simple price lookups.

## Environment & Settings

```bash
export OKX_API_KEY=...
export OKX_SECRET_KEY=...
export OKX_API_PASSPHRASE=...
export OKX_PROJECT_ID=...
export OKX_BASE_URL=https://web3.okx.com/api/v5/   # optional override

# Optional overrides (defaults shown)
export RATE_LIMIT_REQUESTS_PER_SECOND=10
export MAX_RETRIES=3
export RETRY_DELAY=1.0
export TIMEOUT_SECONDS=30
```

`data_provider.Settings` ingests these variables (plus indicator defaults such as SMA/EMA periods). Missing OKX keys raise immediately before any HTTP call, so configure them centrallyâ€”either via environment or by passing `env_vars` into the MCP helpers.

## Whatâ€™s Inside the Toolkit

<table>
  <colgroup>
    <col style={{ width: "22%" }} />
    <col style={{ width: "22%" }} />
    <col style={{ width: "56%" }} />
  </colgroup>
  <thead>
    <tr>
      <th>Component</th>
      <th>File(s)</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>`CryptoPowerDataCEXTool`</td>
      <td>`tools.py`</td>
      <td>Pull OHLCV candles from 100+ CCXT exchanges and pipe them through the enhanced indicator stack.</td>
    </tr>
    <tr>
      <td>`CryptoPowerDataDEXTool`</td>
      <td>`tools.py`</td>
      <td>Hit OKX Web3 DEX APIs for on-chain pairs specified by `chain_index` + token address.</td>
    </tr>
    <tr>
      <td>`CryptoPowerDataPriceTool`</td>
      <td>`tools.py`</td>
      <td>Lightweight spot price snapshot (CEX or DEX) without fetching an entire candle set.</td>
    </tr>
    <tr>
      <td>`CryptoPowerDataIndicatorsTool`</td>
      <td>`tools.py`</td>
      <td>Enumerate every indicator name/parameter accepted by the enhanced TA registry (TA-Lib + custom extras).</td>
    </tr>
    <tr>
      <td>`Settings`, `OKXDEXClient`, `TechnicalAnalysis`</td>
      <td>`data_provider.py`</td>
      <td>Central place for rate limiting, retries, authenticated OKX calls, and TA-Lib helpers.</td>
    </tr>
    <tr>
      <td>MCP server runners (`start_crypto_powerdata_mcp_*`)</td>
      <td>`server.py`, `dual_transport_server.py`</td>
      <td>Start stdio or HTTP/SSE transports so UI agents can subscribe to continuous feeds.</td>
    </tr>
    <tr>
      <td>Analytics core</td>
      <td>`main.py`, `enhanced_indicators.py`, `talib_registry.py`</td>
      <td>Parse indicator configs, register TA functions, and expose them via FastMCP tools.</td>
    </tr>
  </tbody>
</table>

All tools inherit `CryptoPowerDataBaseTool`, which lazily initializes global settings and reuses throttled clients; you rarely need to micromanage sessions yourself.

## Indicator Configuration Cheatsheet

- Accepts either JSON strings (most MCP clients) or native dicts. Double-encoded JSON like `"\"{\\\"ema\\\": ...}\""` is auto-decoded.
- Mix-and-match multiple parameters per indicator:  
  `{"ema": [{"timeperiod": 12}, {"timeperiod": 26}], "macd": [{"fastperiod": 12, "slowperiod": 26, "signalperiod": 9}]}`
- Enhanced registry supports 150+ TA-Lib functions plus custom composites (VWAP, BB width/position, Aroon oscillators, etc.).
- Validation errors bubble back as descriptive `ToolResult.error` messages so you can surface them directly to users.

## Usage Patterns

### CEX candles + indicators

```python
from spoon_toolkits.crypto.crypto_powerdata import CryptoPowerDataCEXTool

tool = CryptoPowerDataCEXTool()
result = await tool.execute(
    exchange="binance",
    symbol="BTC/USDT",
    timeframe="1h",
    limit=200,
    indicators_config='{"ema": [{"timeperiod": 12}, {"timeperiod": 26}], "rsi": [{"timeperiod": 14}]}',
)

ohlcv_rows = result.output["data"]
metadata = result.output.get("metadata")
```

### DEX analytics on OKX Web3

```python
from spoon_toolkits.crypto.crypto_powerdata import CryptoPowerDataDEXTool

dex = CryptoPowerDataDEXTool()
result = await dex.execute(
    chain_index="1",                      # Ethereum
    token_address="0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2",  # WETH
    timeframe="1H",
    limit=150,
    indicators_config='{"macd": [{"fastperiod": 12, "slowperiod": 26, "signalperiod": 9}], "bb": [{"period": 20, "std": 2}]}',
)

candles = result.output["data"]
```

### Real-time price snapshot

```python
from spoon_toolkits.crypto.crypto_powerdata import CryptoPowerDataPriceTool

price_tool = CryptoPowerDataPriceTool()
btc = await price_tool.execute(source="cex", exchange="okx", symbol="BTC/USDT")
dex_price = await price_tool.execute(source="dex", chain_index="42161", token_address="0xFF970A61A04b1cA14834A43f5de4533eBDDB5CC8")
```

### Discover supported indicators

```python
from spoon_toolkits.crypto.crypto_powerdata import CryptoPowerDataIndicatorsTool

catalog = await CryptoPowerDataIndicatorsTool().execute()
print(catalog.output["indicators"])   # list of indicator metadata with defaults
```

## MCP Server & Streaming

- `start_crypto_powerdata_mcp_stdio(env_vars=...)` spins up a FastMCP stdio server; pass `background=True` if you need it alongside other async work.
- `start_crypto_powerdata_mcp_sse(host, port, env_vars)` exposes identical tools over HTTP + Server-Sent Events (see `dual_transport_server.py` for endpoints `/mcp` and `/health`).
- `start_crypto_powerdata_mcp_auto` picks stdio vs SSE automatically based on environment; handy for container images.
- `CryptoPowerDataMCPServer` keeps track of running threads so you can query `status()` or stop everything on shutdown.
- `mcp_bridge.py` wires FastMCP methods into the dual transport so CLI agents and browser extensions consume the same tool definitions.

When you need streaming OHLCV updates (vs. periodic `execute` calls), run the SSE server and subscribe to `/mcp` with a persistent session ID. The same OKX rate limiting (`rate_limit_requests_per_second`) and retry envelopes apply regardless of transport.

---

FILE: Toolkit/crypto/solana.md

---
id: solana
title: Solana Tools
---

# Solana Tools

`spoon_toolkits.crypto.solana` provides Python-based Solana blockchain tools built on top of `solana-py`, offering native SOL transfers, SPL token operations, and Jupiter-powered token swaps 

## Environment & Dependencies

```bash
# Required runtime settings / env vars
export SOLANA_RPC_URL=https://api.mainnet-beta.solana.com
export SOLANA_PRIVATE_KEY=your_base58_or_base64_private_key
# (aliases: WALLET_PRIVATE_KEY for private key, RPC_URL for the endpoint)

# Optional extras
export HELIUS_API_KEY=your_helius_key   # Enables enriched RPC + webhooks
export BIRDEYE_API_KEY=your_birdeye_key # Enables live price + portfolio data
```

The keypair loader accepts both base58 (phantom export) and base64 strings, and you can override any parameter per call via the tool arguments.

**Dependencies:**
- `solana-py` â€“ RPC client used by transfers, swaps, and wallet reads
- `solders` â€“ Fast keypair/pubkey primitives for signing
- `spl.token` â€“ SPL Token program bindings (ATA creation, transfers)
- `httpx` â€“ Async Jupiter and Birdeye integrations

## Tool Catalog

### Transfer Tools

#### `SolanaTransferTool`

Transfer SOL or SPL tokens to another address.

**Parameters:**
- `recipient` (str, **required**) - Destination Solana address
- `amount` (str/number, **required**) - Amount in human-readable units
- `token_address` (str, optional) - SPL token mint address; omit for SOL
- `rpc_url` (str, optional) - RPC endpoint override
- `private_key` (str, optional) - Sender private key override

**Returns:**
```python
ToolResult(output={
    "success": True,
    "signature": "5j7s...",
    "amount": "1.5",
    "recipient": "9jW8F..."
})
```

**Example:**
```python
from spoon_toolkits.crypto.solana import SolanaTransferTool

# Transfer SOL
transfer_tool = SolanaTransferTool()
result = await transfer_tool.execute(
    recipient="9jW8FPr6BSSsemWPV22UUCzSqkVdTp6HTyPqeqyuBbCa",
    amount="0.1"
)
print(f"Signature: {result.output['signature']}")

# Transfer SPL token (USDC)
result = await transfer_tool.execute(
    recipient="9jW8FPr6BSSsemWPV22UUCzSqkVdTp6HTyPqeqyuBbCa",
    amount="100",
    token_address="EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v"
)
```

**Features:**
- âœ… Automatic ATA (Associated Token Account) creation for recipients
- âœ… Decimal precision validation
- âœ… Gas estimation and blockhash management
- âœ… Transaction confirmation with timeout

### Swap Tools

#### `SolanaSwapTool`

Execute token swaps using Jupiter aggregator with intelligent token resolution.

**Parameters:**
- `input_token` (str, **required**) - Input token (symbol/mint/address)
- `output_token` (str, **required**) - Output token (symbol/mint/address)
- `amount` (str/number, **required**) - Amount to swap
- `slippage_bps` (int, optional) - Slippage tolerance in basis points (default: dynamic)
- `priority_level` (str, optional) - Transaction priority: `low`, `medium`, `high`, `veryHigh` (default)
- `rpc_url` (str, optional) - RPC endpoint override
- `private_key` (str, optional) - Wallet private key override

**Returns:**
```python
ToolResult(output={
    "success": True,
    "signature": "3xK9...",
    "input_token": "SOL",
    "input_mint": "So111...",
    "output_token": "USDC",
    "output_mint": "EPjFW...",
    "input_amount": "1.0",
    "output_amount": "150.23",
    "price_impact": 0.002,
    "slippage_bps": 50,
    "route_plan": [...],
    "fees": {"transaction_fee": 5000, "fee_sol": 0.000005}
})
```

**Example:**
```python
from spoon_toolkits.crypto.solana import SolanaSwapTool

swap_tool = SolanaSwapTool()

# Swap using symbols (automatically resolved from wallet)
result = await swap_tool.execute(
    input_token="SOL",
    output_token="USDC",
    amount="1.0",
    slippage_bps=50,  # 0.5% slippage
    priority_level="high"
)

# Swap using mint addresses
result = await swap_tool.execute(
    input_token="So11111111111111111111111111111111111111112",
    output_token="EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v",
    amount="1.0"
)
```

**Advanced Features:**

**1. Smart Token Resolution** (Unique to Python version)

Supports multiple input formats:
- Symbol: `"SOL"`, `"USDC"`, `"$BONK"`
- Mint address: `"So111..."`
- Portfolio lookup: Searches your wallet's token holdings

```python
# All these work:
await swap_tool.execute(input_token="SOL", ...)        # Symbol
await swap_tool.execute(input_token="$BONK", ...)      # From wallet
await swap_tool.execute(input_token="So111...", ...)   # Mint address
```

**2. Priority Fee Levels**

| Level | Max Lamports | Use Case |
|-------|-------------|----------|
| `low` | 1,000,000 | Non-urgent swaps |
| `medium` | 2,000,000 | Normal operations |
| `high` | 3,000,000 | Time-sensitive |
| `veryHigh` | 4,000,000 | MEV protection (default) |

**3. Dynamic Quote Context Building**

The tool validates inputs, resolves decimals, and fetches Jupiter quotes before execution:
```python
# Internally handles:
# - Token existence validation
# - Decimal precision checks
# - Amount > 0 validation
# - Slippage bounds (1-10000 bps)
# - Jupiter quote fetching
# - Output amount formatting
```

### Wallet Tools

#### `SolanaWalletInfoTool`

Query comprehensive wallet information including SOL balance and SPL token holdings.

**Parameters:**
- `address` (str, optional) - Wallet address; defaults to configured wallet
- `include_tokens` (bool, optional) - Include SPL token balances (default: `True`)
- `token_limit` (int, optional) - Max tokens to return (default: 20)
- `rpc_url` (str, optional) - RPC endpoint override

**Returns:**
```python
ToolResult(output={
    "address": "9jW8F...",
    "truncated_address": "9jW8...BbCa",
    "sol_balance": 1.523456789,
    "lamports": 1523456789,
    "token_count": 5,
    "tokens": [
        {
            "mint": "EPjFW...",
            "balance": "150.23",
            "decimals": 6,
            "raw_balance": "150230000"
        },
        # ...
    ]
})
```

**Example:**
```python
from spoon_toolkits.crypto.solana import SolanaWalletInfoTool

wallet_tool = SolanaWalletInfoTool()

# Query configured wallet
result = await wallet_tool.execute()
print(f"SOL Balance: {result.output['sol_balance']}")
print(f"Token Count: {result.output['token_count']}")

# Query specific wallet
result = await wallet_tool.execute(
    address="9jW8FPr6BSSsemWPV22UUCzSqkVdTp6HTyPqeqyuBbCa",
    include_tokens=True,
    token_limit=10
)
```

**Features:**
- âœ… **Wallet Cache Scheduler** â€“ Results are cached per `(rpc_url, address)` so repeated reads avoid RPC calls. The scheduler refresh cadence is 60s by default and can be forced manually.
- âœ… **Token metadata basics** â€“ Each entry includes the mint, UI balance, decimals, and raw balance. (Names, symbols, and USD prices are only attached when Birdeye is configured; otherwise the fields stay minimal.)
- âœ… **Portfolio cache hook** â€“ The same cache powers the swap helperâ€™s â€œsmart token resolution,â€ so swaps can reference symbols (`SOL`, `$BONK`, etc.) without extra lookups.
- âœ… **Optional price data** â€“ When `BIRDEYE_API_KEY` is present, the scheduler records token prices and wallet USD totals, which can be consumed through the service helpers.

## Service Helpers

The toolkit provides 30+ utility functions in `service.py`:

### Validation

```python
from spoon_toolkits.crypto.solana import (
    validate_solana_address,
    validate_private_key,
    is_native_sol
)

# Address validation
is_valid = validate_solana_address("9jW8FPr6BSSsemWPV22UUCzSqkVdTp6HTyPqeqyuBbCa")

# Private key validation (base58/base64)
is_valid = validate_private_key("5j7s...")

# Check if token is native SOL
is_sol = is_native_sol("So11111111111111111111111111111111111111112")
```

### Conversion

```python
from spoon_toolkits.crypto.solana import (
    lamports_to_sol,
    sol_to_lamports,
    format_token_amount,
    parse_token_amount
)

# SOL <-> Lamports
lamports = sol_to_lamports(1.5)  # â†’ 1500000000
sol = lamports_to_sol(1500000000)  # â†’ 1.5

# Token amount formatting
ui_amount = format_token_amount(150230000, decimals=6)  # â†’ 150.23
raw_amount = parse_token_amount(150.23, decimals=6)  # â†’ 150230000
```

### Address Utilities

```python
from spoon_toolkits.crypto.solana import (
    get_associated_token_address,
    truncate_address,
    detect_pubkeys_from_string
)

# Get ATA for token
ata = get_associated_token_address(
    token_mint="EPjFW...",
    owner="9jW8F..."
)

# Shorten address for display
short = truncate_address("9jW8FPr6BSSsemWPV22UUCzSqkVdTp6HTyPqeqyuBbCa")
# â†’ "9jW8...BbCa"

# Extract addresses from text
pubkeys = detect_pubkeys_from_string("Send 1 SOL to 9jW8F...")
```

### Error Parsing

```python
from spoon_toolkits.crypto.solana import parse_transaction_error

# User-friendly error messages
friendly = parse_transaction_error("Error: 0x1")
# â†’ "Insufficient funds. Ensure the account has enough SOL..."
```

## Keypair Management

```python
from spoon_toolkits.crypto.solana import (
    get_wallet_keypair,
    get_wallet_key,
    get_private_key,
    get_public_key
)

# Get full keypair (requires private key)
keypair_result = get_wallet_keypair(require_private_key=True)
if keypair_result.keypair:
    print(f"Public Key: {keypair_result.keypair.pubkey()}")

# Get public key only
keypair_result = get_wallet_keypair(require_private_key=False)
if keypair_result.public_key:
    print(f"Public Key: {keypair_result.public_key}")

# Dynamic private key support
keypair_result = get_wallet_key(
    require_private_key=True,
    private_key="5j7s..."  # Override env var
)
```

## Advanced: Wallet Cache Scheduler

**Unique to Python version** - Background service that keeps wallet data fresh.

```python
from spoon_toolkits.crypto.solana import get_wallet_cache_scheduler

scheduler = get_wallet_cache_scheduler()

# Start background refresh (runs every 60s)
await scheduler.ensure_running(
    rpc_url="https://api.mainnet-beta.solana.com",
    wallet_address="9jW8F...",
    include_tokens=True
)

# Get cached data (no RPC call)
cached = await scheduler.get_cached(
    rpc_url="https://api.mainnet-beta.solana.com",
    wallet_address="9jW8F..."
)

if cached:
    wallet_data = cached["data"]
    print(f"SOL: {wallet_data['sol_balance']}")

# Force immediate refresh
fresh_data = await scheduler.force_refresh(
    rpc_url="https://api.mainnet-beta.solana.com",
    wallet_address="9jW8F...",
    include_tokens=True
)
```

**Benefits:**
- âœ… Reduces RPC calls by 95%
- âœ… Automatic background updates
- âœ… Instant wallet info access for swap token resolution

## Constants

```python
from spoon_toolkits.crypto.solana import (
    TOKEN_ADDRESSES,
    DEFAULT_SLIPPAGE_BPS,
    JUPITER_PRIORITY_LEVELS
)

# Well-known addresses fall back to sensible defaults if you havenâ€™t overridden TOKEN_ADDRESSES.
USDC_MINT = TOKEN_ADDRESSES.get("USDC", "EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v")
BONK_MINT = TOKEN_ADDRESSES.get("BONK")  # None until you set it

# Default configuration helpers
print(DEFAULT_SLIPPAGE_BPS)       # e.g. 100 (1%)
print(JUPITER_PRIORITY_LEVELS)    # {"low": 50, "medium": 200, "high": 1000, "veryHigh": 4000000}
```

## Operational Notes

### Error Handling

Always check `ToolResult.error`:

```python
result = await swap_tool.execute(...)

if result.error:
    print(f"âŒ Swap failed: {result.error}")
    if result.diagnostic:
        print(f"Details: {result.diagnostic}")
else:
    print(f"âœ… Swap successful: {result.output['signature']}")
```

### Rate Limiting

Jupiter API limits:
- Quote endpoint: ~10 req/s
- Swap endpoint: ~5 req/s

Use the wallet cache scheduler to minimize RPC calls:

```python
# Bad: Queries RPC every time
for token in tokens:
    await wallet_tool.execute(address="...", include_tokens=True)

# Good: Use cache scheduler
scheduler = get_wallet_cache_scheduler()
await scheduler.ensure_running("...", "...", True)
for token in tokens:
    cached = await scheduler.get_cached("...", "...")
```

---

FILE: Toolkit/data-platforms/chainbase.md

`spoon_toolkits.data_platforms.chainbase` wraps the Chainbase REST API in async `BaseTool` classes and mounts them on a FastMCP server, so agents can query block, transaction, account, and token data without writing raw HTTP calls.

## Environment & Configuration

```bash
export CHAINBASE_API_KEY=your_chainbase_key             # required for every request
export CHAINBASE_HOST=0.0.0.0                           # optional when running the MCP server
export CHAINBASE_PORT=8000                              # optional (default 8000)
export CHAINBASE_PATH=/sse                              # optional SSE path
```

- Every tool loads `CHAINBASE_API_KEY` at execution time and aborts with a descriptive error if it is missing.
- HTTP calls go to `https://api.chainbase.online/v1/...`; you can override host/port/path only when launching the bundled FastMCP server.

## Package Layout

| Module | Purpose |
| --- | --- |
| `chainbase_tools.py` | Source of the `BaseTool` implementations (`GetLatestBlockNumberTool`, `GetAccountTokensTool`, `ContractCallTool`, etc.). |
| `balance.py`, `basic.py`, `token_api.py` | Individual FastMCP sub-servers exposing account, base, and token endpoints as MCP tools/resources (with docs baked in). |
| `__init__.py` | Aggregates all MCP sub-servers via `FastMCP` and re-exports the tool classes. Running the module spins up an SSE server. |
| `README.md` | Lists supported chains (`chain_id` values) and shows quick-start snippets. |

## Tooling Highlights

### Blocks & Transactions
- `GetLatestBlockNumberTool` â€“ current block height by `chain_id`.
- `GetBlockByNumberTool` â€“ detailed block payload (transactions, miner, timestamp).
- `GetTransactionByHashTool` â€“ fetch by tx hash or `(block_number, tx_index)` combo.
- `GetAccountTransactionsTool` â€“ paginated tx history per address with optional block/timestamp filters.

### Accounts & Portfolios
- `GetAccountBalanceTool` â€“ native coin balance with optional `to_block`.
- `GetAccountTokensTool` â€“ ERC-20 balances, limit/page params, optional contract filter.
- `GetAccountNFTsTool` â€“ NFT holdings, same pagination semantics.

### Contracts & Tokens
- `ContractCallTool` â€“ invoke read-only contract functions by supplying ABI JSON and params.
- `GetTokenMetadataTool` â€“ ERC-20 metadata (name, symbol, decimals, total supply).

Every tool returns the raw JSON Chainbase response: `{"code": ..., "message": ..., "data": [...]}`. You can feed the payload directly into prompts or downstream analytics without reshaping.

## Usage Examples

### Fetch the latest Ethereum block
```python
from spoon_toolkits.data_platforms.chainbase import GetLatestBlockNumberTool

height_tool = GetLatestBlockNumberTool()
block = await height_tool.execute(chain_id=1)
print(block["data"])
```

### Enumerate ERC-20 holdings for a wallet
```python
from spoon_toolkits.data_platforms.chainbase import GetAccountTokensTool

tokens = await GetAccountTokensTool().execute(
    chain_id=1,
    address="0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045",
    limit=50,
)

for token in tokens.get("data", []):
    print(token["symbol"], token["balance"])
```

### Run a read-only contract call
```python
from spoon_toolkits.data_platforms.chainbase import ContractCallTool

result = await ContractCallTool().execute(
    chain_id=1,
    contract_address="0x6B175474E89094C44Da98b954EedeAC495271d0F",  # DAI
    function_name="totalSupply",
    abi='[{"inputs":[],"name":"totalSupply","outputs":[{"type":"uint256"}],"stateMutability":"view","type":"function"}]',
    params=[],
)
print(result["data"])
```

## FastMCP Server Mode

To expose these tools over SSE (useful for MCP-compatible frontends), run:

```bash
python -m spoon_toolkits.data_platforms.chainbase
# or, in code:
from spoon_toolkits.data_platforms.chainbase import mcp_server
mcp_server.run(transport="sse", host="0.0.0.0", port=8000, path="/sse")
```

Internally, the server mounts three subdomains (`Balance`, `Basic`, `TokenAPI`) so you can enable only the scopes you need.

## Operational Notes

- Chainbase enforces per-key rate limits; handle `{"error": "...429..."}` responses by backing off or narrowing ranges (`limit`, `page`).
- Because the SDK just wraps REST, you can set `CHAINBASE_API_KEY` at runtime (e.g., from a vault) before invoking any tool to support multi-tenant agents.
- For deterministic pipelines, inspect the `code` and `message` fields returned by Chainbase before assuming `data` is presentâ€”errors come back with HTTP 200 but non-zero `code`.

---

FILE: Toolkit/data-platforms/desearch.md

`spoon_toolkits.data_platforms.desearch` wraps the official DeSearch SDK in async helpers, builtin Spoon tools, and FastMCP servers so agents can query real-time web, social, and academic sources without writing raw HTTP logic.

## Environment & Configuration

```bash
export DESEARCH_API_KEY=your_actual_key            # required
export DESEARCH_BASE_URL=https://api.desearch.ai   # optional override
export DESEARCH_TIMEOUT=30                         # optional (seconds)
```

- `env.py` loads `.env` via `python-dotenv`, so store the variables beside the process that imports the package.
- The helpers enforce `limit >= 10`, mirroring the SDK requirements.

## Package Layout

| Module | Purpose |
| --- | --- |
| `__init__.py` | Mounts `ai_search` and `web_search` FastMCP sub-servers and re-exports helper coroutines plus `mcp_server`. |
| `ai_search_official.py` | Async tools for AI/meta search, Reddit/Twitter feeds, and academic datasets. Decorated with `@mcp.tool()`. |
| `web_search_official.py` | Web search plus Twitter post/link lookups using the official SDK. |
| `builtin_tools.py` | `BaseTool` wrappers (`DesearchAISearchTool`, etc.) for spoon-core usage without MCP. |
| `cache.py` | `time_cache` decorator that memoizes tool responses for ~5 minutes. |
| `example.py`, `test_integration.py`, `README.md` | Usage demos, live API smoke tests, and a deeper quick-start. |

## Tooling Highlights

### AI and Social Search
- `search_ai_data(query, platforms, limit)` aggregates results from web, Reddit, Wikipedia, YouTube, Twitter, ArXiv, and HackerNews.
- `search_social_media(query, platform, limit)` targets Twitter or Reddit directly.
- `search_academic(query, platform, limit)` narrows to ArXiv or Wikipedia research corpora.

### Web and Twitter Search
- `search_web(query, num_results, start)` calls `basic_web_search` and returns snippet-rich results.
- `search_twitter_posts(query, limit, sort)` pulls live tweets with sort control (Top, Latest, etc.).
- `search_twitter_links(query, limit)` surfaces URLs that are trending on Twitter.

### Builtin Tools and Caching
- `DesearchAISearchTool`, `DesearchWebSearchTool`, `DesearchAcademicSearchTool`, and `DesearchTwitterSearchTool` validate API keys up front and expose JSON schemas for planners.
- `time_cache` decorates every MCP tool, preventing duplicate outbound calls during iterative planning loops.

## Usage Examples

### Mount the FastMCP server inside a Spoon agent
```python
from spoon_toolkits.data_platforms.desearch import mcp_server

agent_config = {
    "tools": [mcp_server],   # exposes ai_search.* and web_search.* namespaces
}
```

### Call async helpers directly
```python
from spoon_toolkits.data_platforms.desearch import (
    search_ai_data,
    search_web,
)

async def summarize(topic: str):
    ai = await search_ai_data(query=f"{topic} 2024", platforms="web,reddit,wikipedia", limit=12)
    web = await search_web(query=topic, num_results=5)
    return {"ai": ai, "web": web}
```

### Use builtin tools when working in spoon-core
```python
from spoon_toolkits.data_platforms.desearch.builtin_tools import DesearchAISearchTool

tool = DesearchAISearchTool()
response = await tool.execute(query="Solana MEV research", platforms="web,reddit", limit=10)
print(response["data"]["results"].keys())
```

## FastMCP Server Mode

Run the package as a server to expose the tools over SSE (compatible with MCP-aware clients):

```bash
python -m spoon_toolkits.data_platforms.desearch
# or in code:
from spoon_toolkits.data_platforms.desearch import mcp_server
mcp_server.run(transport="sse", host="0.0.0.0", port=8000, path="/sse")
```

Internally, the server mounts `ai_search` and `web_search` namespaces, so you can scope access per agent.

## Operational Notes

- Use the provided `test_integration.py` before shipping: it validates your API key and SDK wiring.
- Responses are dictionaries (e.g., `{"query": ..., "results": ..., "count": ...}`); check for `"error"` keys when handling failures.
- Increase or decrease `DESEARCH_TIMEOUT` based on network conditions; the helpers pass the value downstream when the SDK supports it.
- Remove cached entries by restarting the process if you need uncached data while debugging.
- Rate limits come from the DeSearch API; stagger large batches or broaden queries rather than hammering the same endpoint.

---

FILE: Toolkit/data-platforms/thirdweb.md

`spoon_toolkits.data_platforms.third_web` wraps the Thirdweb Insight REST API in async `BaseTool` classes so Spoon agents can fetch contract events, multichain transfers, transactions, and block data without crafting HTTP requests by hand.

## Environment & Configuration

```bash
export THIRDWEB_CLIENT_ID=your_client_id          # used by most tools
```

- Some tools (such as `GetContractEventsFromThirdwebInsight`) allow a `client_id` argument, letting you override the global key for multi-tenant agents.
- Requests use a 100-second timeout and raise descriptive errors if Insight responds with non-success status codes.

## Package Layout

| Module | Purpose |
| --- | --- |
| `third_web_tools.py` | Houses every `BaseTool` plus lightweight async test helpers. Import from `spoon_toolkits.data_platforms.third_web.third_web_tools`. |

## Tooling Highlights

### Events and Transfers
- `GetContractEventsFromThirdwebInsight` - fetch decoded events for a contract + signature (`Transfer(address,address,uint256)`, etc.) with paging metadata.
- `GetMultichainTransfersFromThirdwebInsight` - scan recent transfers for a list of chain IDs (defaults to USDT events exposed by Insight).

### Transactions and Blocks
- `GetTransactionsTool` - consolidate recent transactions across multiple chains.
- `GetContractTransactionsTool` - view activity for a single contract.
- `GetContractTransactionsBySignatureTool` - narrow contract activity down to a specific function signature.
- `GetBlocksFromThirdwebInsight` - stream the latest blocks per chain with optional sort field and order.
- `GetWalletTransactionsFromThirdwebInsight` - list wallet transactions across multiple chains, sorted by block number or timestamp.

Every tool reads `THIRDWEB_CLIENT_ID` when `client_id` is not provided explicitly and returns the raw Insight JSON payload (`{"data": [...], "meta": {...}}`) so you can feed it directly to downstream logic.

## Usage Examples

### Fetch contract events for a signature
```python
from spoon_toolkits.data_platforms.third_web.third_web_tools import GetContractEventsFromThirdwebInsight

tool = GetContractEventsFromThirdwebInsight()
result = await tool.execute(
    client_id="your-client-id",
    chain_id=1,
    contract_address="0xdAC17F958D2ee523a2206206994597C13D831ec7",
    event_signature="Transfer(address,address,uint256)",
    limit=5,
)
print(result)
```

### Aggregate transfers across chains
```python
from spoon_toolkits.data_platforms.third_web.third_web_tools import GetMultichainTransfersFromThirdwebInsight

tool = GetMultichainTransfersFromThirdwebInsight()
transfers = await tool.execute(chains=[1, 137, 8453], limit=10)
print(transfers["data"][0])
```

### Inspect wallet transactions with sorting
```python
from spoon_toolkits.data_platforms.third_web.third_web_tools import GetWalletTransactionsFromThirdwebInsight

wallet_tool = GetWalletTransactionsFromThirdwebInsight()
history = await wallet_tool.execute(
    client_id="your-client-id",
    wallet_address="0xabc...",
    chains=[1, 137],
    limit=10,
    sort_by="block_timestamp",
    sort_order="desc",
)
print(history["data"])
```

## Operational Notes

- Set `THIRDWEB_CLIENT_ID` in your runtime environment (or pass `client_id` per call) before invoking any tool; missing keys raise `ValueError` to prevent ambiguous outcomes.
- HTTP errors bubble up through `requests.raise_for_status()` so you can distinguish network failures from empty result sets.
- Test helpers at the bottom of `third_web_tools.py` (`test_get_contract_events`, etc.) offer quick sanity checks - run them with `python third_web_tools.py` once your credentials are configured.
- Insight endpoints enforce per-client rate limits; stagger large batch pulls by adjusting `limit` or splitting chain lists across multiple requests.

---

FILE: Toolkit/github/analysis-tools.md

`spoon_toolkits.github.github_analysis_tool` bundles three ready-made `BaseTool` classes that call the GitHub GraphQL API via `GitHubProvider`. They are ideal when you want actionable repository stats without writing custom GraphQL queries.

## Environment

```bash
export GITHUB_TOKEN=ghp_your_personal_access_token   # repo scope recommended
```

- Each tool pulls `GITHUB_TOKEN` unless you pass `token="..."` explicitly. Missing credentials surface as `ToolResult(error="GitHub token is required...")`.
- GitHubâ€™s GraphQL endpoint enforces a rate limit of 5,000 points/hour per token. These tools request up to 100 nodes per call; batch long-range reports accordingly.

## Tool Catalog

| Tool | Parameters | Output highlights | Use cases |
| --- | --- | --- | --- |
| `GetGitHubIssuesTool` | `owner`, `repo`, `start_date`, `end_date`, optional `token` | `total_count`, `issues_list` (state, labels, comment counts, author), `date_range` | Bug triage dashboards, changelog assembly, governance transparency |
| `GetGitHubPullRequestsTool` | Same window parameters | `pull_requests_list` with review totals, commit counts, merge timestamps | Contributor scorecards, weekly PR digests |
| `GetGitHubCommitsTool` | Same window parameters | `commits_list` including message, author, additions/deletions, `changedFiles` | Release notes, productivity metrics, feature tracking |

All parameters accept ISO `YYYY-MM-DD` strings. The window is inclusive and aligned to UTC.

## Usage Patterns

### Issues snapshot
```python
from spoon_toolkits.github.github_analysis_tool import GetGitHubIssuesTool

issues_tool = GetGitHubIssuesTool()
issues = await issues_tool.execute(
    owner="XSpoonAi",
    repo="spoon-core",
    start_date="2024-01-01",
    end_date="2024-02-01",
)

if issues.error:
    raise RuntimeError(issues.error)

for issue in issues.output["issues_list"]:
    print(issue["title"], issue["state"])
```

### Weekly contributor digest (issues â†’ PRs â†’ commits)
```python
from datetime import date, timedelta
from spoon_toolkits.github.github_analysis_tool import (
    GetGitHubIssuesTool,
    GetGitHubPullRequestsTool,
    GetGitHubCommitsTool,
)

today = date.today()
week_ago = today - timedelta(days=7)
window = dict(
    owner="XSpoonAi",
    repo="spoon-core",
    start_date=week_ago.isoformat(),
    end_date=today.isoformat(),
)

issues = await GetGitHubIssuesTool().execute(**window)
prs = await GetGitHubPullRequestsTool().execute(**window)
commits = await GetGitHubCommitsTool().execute(**window)

if any(result.error for result in (issues, prs, commits)):
    raise RuntimeError("GitHub API call failed")

summary = {
    "issues": issues.output["total_count"],
    "pull_requests": prs.output["total_count"],
    "commits": commits.output["total_count"],
}
```

## Error Handling Tips

- Inspect `ToolResult.error` before consuming `output`. Authentication failures, invalid repo names, or GraphQL validation errors are surfaced there.
- If GitHub throttles you, the GraphQL API returns `errors: [{"type": "RATE_LIMITED", ...}]`; the tool passes that text through the `error` field.
- Dates outside the repository lifetime simply return `total_count = 0`; no error is raised.

Use these tools when you need standardized analytics quickly. If you outgrow the default fields, switch to the lower-level `GitHubProvider` documented separately to craft custom queries.

---

FILE: Toolkit/github/provider.md

`spoon_toolkits.github.github_provider.GitHubProvider` gives you a thin but flexible GraphQL client built on `gql`. Use it when the stock analysis tools donâ€™t expose the fields or filtering you need.

## Environment & Auth

```bash
export GITHUB_TOKEN=ghp_your_personal_access_token   # repo scope recommended
```

- The constructor reads `GITHUB_TOKEN` automatically. Pass `GitHubProvider(token="ghp_...")` to override per request.
- The underlying transport hits `https://api.github.com/graphql`. Rate limits (5,000 points/hour) and GraphQL errors are raised as Python exceptions, so always wrap calls in `try/except`.

## Initialization

```python
from spoon_toolkits.github.github_provider import GitHubProvider

provider = GitHubProvider()
```

Provide the token only once per provider instance. Each method is `async`, so you can reuse the same instance across awaits.

## Built-in Methods

| Method | What it queries | Useful fields returned |
| --- | --- | --- |
| `fetch_issues(owner, repo, start_date, end_date)` | `issues` connection filtered by creation date | `title`, `state`, `labels`, `comments.totalCount`, `author.login`, timestamps |
| `fetch_pull_requests(...)` | `pullRequests` connection in the same date window | `mergedAt`, `reviews.totalCount`, `commits.totalCount`, labels, author |
| `fetch_commits(...)` | `defaultBranchRef.target.history` limited to 100 commits (GraphQL limit) | `message`, `committedDate`, `additions`, `deletions`, `changedFiles`, `author` |
| `fetch_repository_info(owner, repo)` | `repository` node | `stargazerCount`, `forkCount`, `watcherCount`, open issue/pr counts, topics, license |

The default history queries request 100 nodes. To page further, copy the query strings in `github_provider.py` and extend them with `after` cursors.

## Example: Custom GraphQL + Provider Methods

```python
from spoon_toolkits.github.github_provider import GitHubProvider
from gql import gql

provider = GitHubProvider()

# Use the prebuilt helper
repo = await provider.fetch_repository_info("XSpoonAi", "spoon-core")
print("Stars:", repo["stargazerCount"])

# Extend with your own query
custom_query = gql("""
  query($owner: String!, $repo: String!) {
    repository(owner: $owner, name: $repo) {
      releases(last: 5) {
        nodes {
          name
          tagName
          publishedAt
        }
      }
    }
  }
""")

result = provider.client.execute(custom_query, variable_values={"owner": "XSpoonAi", "repo": "spoon-core"})
```

## Error Handling & Best Practices

- Each method wraps `client.execute` and raises `Exception` with the underlying GraphQL message (`RATE_LIMITED`, `NOT_FOUND`, `FORBIDDEN`, etc.). Catch `Exception` at call sites to degrade gracefully.
- For long-running agents, instantiate the provider once and reuse itâ€”this avoids fetching the schema repeatedly.
- GraphQL schemas are fetched on first transport use (`fetch_schema_from_transport=True`). When developing offline, consider setting that flag to `False` in a forked provider to skip the extra call.

Reach for `GitHubProvider` when you need to compose bespoke queries, add pagination, or stitch multiple GraphQL responses together within a single agent step.

---

FILE: Toolkit/index.md

---
id: index
title: Toolkit Overview
---

# Toolkit Overview

Welcome to the Spoon Toolkit catalog. Everything here is a ready-to-use agent helper that ships with the SpoonOS runtime. The toolkit is organized by domain so you can jump straight to the integrations you need:

- **Crypto** â€“ Market data, EVM utilities, Neo RPC helpers, Chainbase-powered analytics, and more. Use these when your agent needs to trade, monitor, or analyze on-chain activity.
- **Data Platforms** â€“ Providers such as Chainbase, Thirdweb, and Desearch for structured blockchain data without writing bespoke HTTP clients.
- **Security** â€“ Token risk scores, phishing detectors, dApp approval audits, and address-risk lookups, designed to keep automated agents from interacting with malicious counterparties.
- **GitHub Intelligence** â€“ Repository scanners and provider hooks that summarize repos, issues, and activity.
- **Social Media** â€“ Discord, Telegram, Twitter, and Email helpers for outreach and notification workflows.
- **Storage** â€“ Off-chain storage bridges (AIOZ, Oort, Foureverland) so agents can archive artifacts or publish outputs.

Navigate the sidebar on the left to dive into any module. Each page documents the environment variables, key classes, and concrete usage snippets for that toolkit.

---

FILE: Toolkit/security/address-risk.md

---
title: Address & Protocol Reputation
---

# Address & Protocol Reputation

Use these helpers to vet EOAs, contracts, and launch patterns before allowing agents to interact with them.

## Environment

```bash
export GO_PLUS_LABS_APP_KEY=your_app_key
export GO_PLUS_LABS_APP_SECRET=your_app_secret
```

The GoPlusLabs HTTP clients load the variables above at import time and immediately request an access token. If either variable is unset, the token call itself fails with an HTTP 4xx error before any tool executes, so set both values (or load them from a secrets manager) prior to importing the module. All RPC calls run through shared `httpx.AsyncClient` instances, so populate the variables once per process.

## Malicious Address Checks

`malicious_address.py` exposes `check_malicious_address(contract_address, chain_name='')`.
- Normalises the address, optionally converts `chain_name` to an ID, and queries `/address_security/`.
- Returns threat labels such as `phishing_activities`, `mixer`, `blacklist_doubt`, `sanctioned`, and counts of malicious contracts deployed.
- Pass an empty `chain_name` to aggregate findings across networks; otherwise specify a chain for focused results.

**Response fields**

| Key | Description |
| --- | --- |
| `phishing_activities`, `mixer`, `blacklist_doubt` | `"1"` when the address matches GoPlusâ€™ threat feeds |
| `malicious_contract_count` | Number of high-risk contracts deployed by the address |
| `label_list` | Additional annotations, e.g., exchange labels or airdrop roles |
| `updated_time` | Timestamp (seconds) of the latest detection event |

## Rug Pull Detection

`rug_pull_detection(contract_address, chain_name)` inspects ownership privileges and withdrawal patterns:
- Flags approval abuse, privileged withdraws, self-destruct permissions, and proxy usage.
- Returns owner metadata (`owner_address`, `owner_type`) to feed governance rules.

## Chain Metadata

`supported_chains.py` offers two important utilities:
- `supported_chains()` (resource `resource://SupportedChains`) â€“ list of chain names currently covered by GoPlusLabs.
- `chain_name_to_id(name)` â€“ resolves user-friendly names to API IDs (`"Ethereum" -> "1"`).

Cache behaviour is shared across these functions, so repeated lookups stay within rate limits.

### Example

```python
from spoon_toolkits.security.gopluslabs.malicious_address import check_malicious_address

labels = await check_malicious_address("0xEvil", "Ethereum")
if labels.get("phishing_activities") == "1":
    raise ValueError("Sender flagged for phishing")
```

## Caching & Errors

- Every public coroutine is wrapped with `@time_cache(max_age_seconds=300)`, so repeated lookups within five minutes hit the in-memory cache (per process). Pass distinct parameters to force a refresh, or restart the agent for an immediate purge.
- GoPlusLabs occasionally returns HTTP 429 or structured error payloads; the shared HTTP client raises a Python exception in those cases, which you should catch to retry/back off.
- When `chain_name` is invalid, `chain_name_to_id` raises `NotImplementedError("Chain <name> not supported.")`. Use `supported_chains()` to validate user input before calling the scanners.

---

FILE: Toolkit/security/approvals.md

---
title: Approval Intelligence
---

# Approval Intelligence

`approval_security.py` inventories token allowances and evaluates spender contracts using GoPlusLabs v1/v2 APIs. All functions share the `time_cache()` decorator, so repeated calls within the TTL reuse cached responses.

## Environment

```bash
export GO_PLUS_LABS_APP_KEY=your_app_key
export GO_PLUS_LABS_APP_SECRET=your_app_secret
```

These values hydrate a bearer token for the shared HTTP clients defined in `http_client.py`. Set them before importing the module; otherwise the token request itself fails with an HTTP 4xx error and the module never finishes loading.

## Contract Safety Snapshot

```python
from spoon_toolkits.security.gopluslabs.approval_security import check_approval_security
report = await check_approval_security("0xUniswapRouter", "Ethereum")
```

The result details contract metadata, open-source status, proxy usage, and `malicious_behavior` tags for downstream filtering.

**Response fields**

| Key | Description |
| --- | --- |
| `contract_security` | Verification status, open-source flag, proxy detection |
| `malicious_behavior` | List of GoPlus threat labels (honeypot, fake_token, rug_pull, etc.) |
| `creator_address`, `deploy_timestamp` | Provenance data for the spender |
| `security_source` | Indicates whether the data came from on-chain scans or audits |

## ERC-20 Allowances

`erc20_approval_security(contract_address, chain_name)` fetches outstanding approvals granted by an EOA. Each entry contains:
- `approved_contract` and categorized behaviors (`malicious_behavior`).
- `approved_amount`, timestamps, and transaction hashes (`initial_approval_hash`).
- Token metadata (decimals, symbol) and verification flags.

Additional keys: `token_symbol`, `token_contract`, `spender_type`, `approve_time`, `dex`, and boolean strings (`is_verified`, `is_open_source`).

## ERC-721 / ERC-1155 Allowances

`erc721_approval_security` and `erc1155_approval_security` mirror the structure but capture NFT-specific approval scopes:
- Per-token approvals (`approved_token_id`).
- `approved_for_all` flags for marketplace integrations.
- Verification/open-source status of the target contracts.

## Usage Tips

- Normalize addresses before calling: the helpers internally call `normalize_ethereum_contract_address`.
- Allow an empty `chain_name` if you want GoPlusLabs to infer chains, but prefer explicit chains to avoid surprises.
- Layer results with wallet analytics (e.g., Whale tracking) to decide when to revoke allowances automatically.

## Caching & Error Handling

- All approval helpers are decorated with `@time_cache(max_age_seconds=300)`; expect up to a five-minute delay before a revoked allowance disappears from cached results.
- GoPlusLabs returns HTTP error codes for invalid chains, missing addresses, or rate limits. The shared HTTP client raises `httpx.HTTPStatusError`, so wrap calls in `try/except` if you need retries.
- Responses use strings for booleans (`"1"`, `"0"`) and numeric fields; coerce them to integers/floats before arithmetic.

---

FILE: Toolkit/security/dapp-phishing.md

---
title: dApp & Site Review
---

# dApp & Site Review

Evaluate project websites and contract bundles before agents surface them to users or execute automated flows.

## Environment

```bash
export GO_PLUS_LABS_APP_KEY=your_app_key
export GO_PLUS_LABS_APP_SECRET=your_app_secret
```

The dApp and phishing scanners share the same HTTP clients and caching layer as the other GoPlus modules. Credentials are loaded once at import, and the module immediately requests an access tokenâ€”if either value is missing, that HTTP call fails before any tool executes.

## dApp Security

`dapp_security.py` defines `get_nft_security(url)` (the name is historical; it checks full dApp stacks).
- Queries `/dapp_security/?url=...` and returns `audit_info`, contract inventories, and malicious behavior flags.
- Contract entries include creator addresses, deployment timestamps, open-source status, and any GoPlusLabs threat tags.
- `trust_list` and `is_audit` fields help quickly spot vetted ecosystems.

**Response highlights**

| Key | Description |
| --- | --- |
| `audit_info` | Map of audit firms and reports attached to the project |
| `contract_list` | Array of contracts with `contract_address`, `chain_id`, verification flags |
| `malicious_behavior` | Aggregated tags covering rug, phishing, fake_token, etc. |
| `risk_score` | Normalized 0â€“100 trust score from GoPlus |

## Phishing Sites

`phishing_site.py` provides `get_nft_security(url)` (again, legacy name) to classify URLs as phishing or safe.
- Returns a `phishing_site` flag plus `website_contract_security` entries with contract standard, NFT risk characteristics, and `address_risk` labels.
- Useful for safeguarding webhook callbacks, prompt-injected URLs, or user-submitted domains.

**Response highlights**

| Key | Description |
| --- | --- |
| `phishing_site` | `"1"` if GoPlus flagged the domain |
| `website_contract_security` | Contracts referenced by the site (standard, chain, security labels) |
| `dapp` | Basic metadata: project name, category, listed marketplaces |
| `detected_time` | Unix timestamp of last phishing detection |

### Example

```python
from spoon_toolkits.security.gopluslabs.phishing_site import get_nft_security as check_site

site_report = await check_site("go-ethdenver.com")
if site_report.get("phishing_site") == 1:
    raise ValueError("Domain flagged as phishing, abort visit")
```

Pair these scanners with the token and approval checks to build multi-step policy guards in SpoonOS agents.

## Caching & Errors

- Both helpers use `@time_cache(max_age_seconds=300)`. If a domainâ€™s status changes (e.g., cleared phishing record), cached responses may lag by up to five minutes.
- HTTP failures or invalid URLs raise exceptions from the shared HTTP client; catch them to trigger retries or fallbacks.
- GoPlus returns strings for booleans; compare against `"1"`/`"0"` instead of Python `True`/`False`.

---

FILE: Toolkit/security/internals.md

---
title: Shared Utilities
---

# Shared Utilities

The GoPlusLabs integration packages several helpers that keep the tooling efficient and consistent across modules.

## Environment Requirements

```bash
export GO_PLUS_LABS_APP_KEY=your_app_key
export GO_PLUS_LABS_APP_SECRET=your_app_secret
```

`env.py` loads these variables via `python-dotenv` on import. Set them once per process; every security helper (tokens, approvals, phishing, etc.) relies on the same values and the module immediately makes an access-token request during import.

## HTTP Clients

`http_client.py` generates a bearer token via `get_token()` and instantiates two `httpx.AsyncClient` objects:
- `go_plus_labs_client_v1` targets `https://api.gopluslabs.io/api/v1`.
- `go_plus_labs_client_v2` targets `https://api.gopluslabs.io/api/v2`.

Both attach an async `raise_on_4xx_5xx` hook so callers only deal with successful JSON payloads or Python exceptions. The clients are created at import time, so reuse them instead of spinning up new sessions.

## Caching

`time_cache(max_age_seconds=300)` wraps coroutine functions with an LRU cache that invalidates entries on a time salt. This keeps frequently accessed risk reports (e.g., the same contract address within five minutes) local while still refreshing periodically.

## Address Normalisation

`utils.normalize_ethereum_contract_address` ensures every EVM address is 0x-prefixed, 42 characters long, and hexadecimal. Raise early if a user submits malformed data.

## Environment Loading

`env.py` uses `dotenv` to pull `GO_PLUS_LABS_APP_KEY` and `GO_PLUS_LABS_APP_SECRET` from the environment at import time. Double-check the secret variable name (`GO_PLUS_LABS_APP_SECRET`) in your deployment manifests.

Leverage these utilities when extending the security toolkitâ€”keeping caching, HTTP usage, and normalisation consistent reduces the chances of subtle bugs or rate-limit regressions.

## Signature Data Decode (Preview)

`signature_data_decode.py` defines an MCP tool stub named `get_abi_decode_info(chain_name, data, contract_address='')`. The implementation currently raises `NotImplementedError` because the input contract ABI schema is still being finalized (see the GoPlus docs at [docs.gopluslabs.io/reference/getabidatainfousingpost](https://docs.gopluslabs.io/reference/getabidatainfousingpost)). If you plan to extend this area:

- Expect to provide `data` as a raw hex string (function selector + calldata) and optionally the target `contract_address`.
- Once implemented, wire the helper into the shared `time_cache` decorator and HTTP clients to match the rest of the security stack.
- Update this document with parameter and response details when the API contract stabilizes.

---

FILE: Toolkit/security/token-risk.md

---
title: Token & NFT Risk
---

# Token & NFT Risk

The token scanners wrap GoPlusLabs endpoints for both EVM contracts and Solana SPL assets. Each function is decorated with `time_cache()` to avoid hammering the API while still returning fresh data for fast-moving markets.

## Environment

```bash
export GO_PLUS_LABS_APP_KEY=your_app_key
export GO_PLUS_LABS_APP_SECRET=your_app_secret
```

Credentials are loaded as soon as `token_security.py` is imported. If either value is missing, the initial token request fails with an HTTP error, so set them in your agent runtime or `.env` file upfront.

## ERC-20 / EVM Tokens

`token_security.py` exposes `get_token_risk_and_security_data(chain_name, contract_address)`.
The helper normalises the address (`normalize_ethereum_contract_address`) and resolves the chain ID using `chain_name_to_id` before querying `/token_security/{chain_id}`.

Common response fields:
- Ownership & minting privileges (`can_take_back_ownership`, `is_mintable`, `is_proxy`).
- Trading health (`is_honeypot`, `buy_tax`, `sell_tax`, `personal_slippage_modifiable`).
- Liquidity signals (`dex` pairs, LP holder distribution, `lp_total_supply`).
- Holder intel (`holders`, whale percentages, blacklist tags).
- Upgrade risk (`is_proxy`, `owner_address`, `owner_balance`).
- Market metadata (`token_name`, `token_symbol`, `total_supply`, `price_change_24h`).

### Example

```python
from spoon_toolkits.security.gopluslabs.token_security import get_token_risk_and_security_data

report = await get_token_risk_and_security_data("Ethereum", "0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2")
risks = report[0]
if risks["is_honeypot"] == "1":
    raise ValueError("Abort swap: honeypot flag present")
```

## Solana Tokens

`get_token_security_for_solana(contract_address)` validates the 44-character address, requests `/solana/token_security`, and returns governance flags (freeze/mint authorities, metadata mutability) plus DEX pricing/TVL snapshots.

Key fields: `is_mutable`, `freeze_authority`, `mint_authority`, `liquidity`, `pair_symbol`, and `honeypot_and_anti_whale` metadata from Solana DEX integrations.

## NFT Collections

`nft_security.py` provides `get_nft_security(chain_name, contract_address, token_id='')`.
It surfaces:
- Collection metadata (supply, verification status, sales volume).
- Privileged operations (mint/burn/self-destruct).
- Oversupply detection, approval restrictions, and red flags for derivative collections.

Use the optional `token_id` when interrogating a specific collectible.

For multi-chain agents, combine these scanners with `SupportedChains` to validate user-supplied chain names before execution.

## Caching & Error Handling

- All token/NFT functions use `@time_cache(max_age_seconds=300)`; identical calls within five minutes reuse cached JSON. Include a timestamp tag or random salt in requests if you must bypass the cache.
- GoPlus returns `"N/A"` or `"Unknown"` when liquidity or holder data is missingâ€”handle those values before casting to numbers.
- Invalid chain names raise `NotImplementedError("Chain <name> not supported.")` from `chain_name_to_id`. Call `supported_chains()` first when accepting user input.
- HTTP errors (network issues, rate limits) bubble up as `httpx.HTTPStatusError`. Wrap calls in `try/except` to implement backoff or fallback providers.

---

FILE: Toolkit/social-media/index.md

---
title: Social Media Toolkit
---

# Social Media Toolkit

`spoon_toolkits.social_media` centralizes outbound and bidirectional messaging for Discord, Telegram, Twitter/X, and Email. All adapters share the same base classes, request/response models, and MCP-friendly interfaces, so you can swap channels without rewriting agent logic.

## Shared Architecture

- **Base classes** â€“ Every tool inherits from `SocialMediaToolBase`, which defines `send_message()` and `validate_config()`. Notification-only adapters extend `NotificationToolBase`; interactive bots (Discord/Telegram) extend `InteractiveToolBase` and add `start_bot()` / `stop_bot()`.
- **Models** â€“ All `execute()` methods accept a `MessageRequest` subclass (e.g., `DiscordMessageRequest`, `EmailMessageRequest`) and return a `MessageResponse` with `{success: bool, message: str, data?: dict}`. This makes the tools drop-in ready for FastMCP exposure.
- **Helper coroutines** â€“ Each adapter exports a `send_*` convenience function (`send_discord_message`, `send_email`, etc.) for one-off notifications.
- **Config validation** â€“ `validate_config()` logs a warning when required credentials are missing; call it during startup to fail fast rather than silently dropping messages.

## Environment Reference

| Channel | Required variables | Optional variables |
| --- | --- | --- |
| **Discord** | `DISCORD_BOT_TOKEN` | `DISCORD_DEFAULT_CHANNEL_ID` |
| **Telegram** | `TELEGRAM_BOT_TOKEN` | `TELEGRAM_DEFAULT_CHAT_ID` |
| **Twitter/X** | `TWITTER_CONSUMER_KEY`, `TWITTER_CONSUMER_SECRET`, `TWITTER_ACCESS_TOKEN`, `TWITTER_ACCESS_TOKEN_SECRET` | `TWITTER_BEARER_TOKEN`, `TWITTER_USER_ID` |
| **Email (SMTP)** | `EMAIL_SMTP_SERVER`, `EMAIL_SMTP_PORT`, `EMAIL_SMTP_USER`, `EMAIL_SMTP_PASSWORD`, `EMAIL_FROM` | `EMAIL_DEFAULT_RECIPIENTS` |

Set these variables before importing the corresponding tool. Missing credentials surface when `validate_config()` runs or when the client attempts its first API call.

## Discord

- **Module**: `discord_tool.py`
- **Client**: `spoon_ai.social_media.discord.DiscordClient`
- **Key methods**:
  - `send_message(message, channel_id=None)` â€“ pushes text to a specific channel or falls back to `DISCORD_DEFAULT_CHANNEL_ID`.
  - `start_bot(agent=None)` / `stop_bot()` â€“ wraps the async gateway loop for interactive bots; pass your Spoon agent at construction time so inbound events are routed through planners.
  - `execute(DiscordMessageRequest)` â€“ MCP entry point; accepts `message` and optional `channel_id`.
- **Usage**:

```python
from spoon_toolkits.social_media.discord_tool import DiscordTool

discord = DiscordTool(agent=my_agent)
if discord.validate_config():
    await discord.send_message("Graph agent finished ingesting blocks")
```

Errors (invalid token, missing intents, closed WebSocket) bubble up as exceptions; wrap calls with retries or queue them through a supervisor task to avoid duplicate sessions.

## Telegram

- **Module**: `telegram_tool.py`
- **Client**: `spoon_ai.social_media.telegram.TelegramClient`
- **Key methods**:
  - `send_message(message, chat_id=None)` â€“ proactive messaging. If the tool was instantiated without an agent, it spins a temporary client just for this send call.
  - `start_bot()` / `stop_bot()` â€“ register webhook/polling handlers so agents can react to user prompts in real time.
  - `execute(TelegramMessageRequest)` â€“ MCP-friendly send wrapper.
- **Usage**:

```python
from spoon_toolkits.social_media.telegram_tool import TelegramTool

telegram = TelegramTool()
await telegram.send_message("ðŸ”” Validator risk threshold exceeded")
```

Rate limits are per bot token; Telegram will throttle after ~30 messages/s. Catch exceptions from `send_message` to implement backoff if you broadcast alerts in bursts.

## Twitter / X

- **Module**: `twitter_tool.py`
- **Client**: `spoon_ai.social_media.twitter.TwitterClient`
- **Key methods**:
  - `post_tweet(message)` â€“ publish a new status.
  - `reply_to_tweet(tweet_id, message)` â€“ threaded replies.
  - `like_tweet(tweet_id)` â€“ reaction utility for engagement workflows.
  - `send_message(message, tags=None)` â€“ convenience layer that appends hashtags or mentions.
  - `execute(TwitterMessageRequest)` â€“ returns a `MessageResponse` for MCP.
- **Usage**:

```python
from spoon_toolkits.social_media.twitter_tool import TwitterTool

twitter = TwitterTool()
if twitter.validate_config():
    twitter.post_tweet("ðŸ“ˆ Treasury rebalance complete. New target: BTC 40%, ETH 35%.")
```

Be mindful of Twitter v2 rate limits (tweets: 300 per 3 hours; likes/replies have separate quotas). Wrap the client in a queue or add exponential backoff to avoid HTTP 429 responses.

## Email (SMTP)

- **Module**: `email_tool.py`
- **Client**: `spoon_ai.social_media.email.EmailNotifier`
- **Key methods**:
  - `send_message(message, to_emails=None, subject="Crypto Monitoring Alert", html_format=True)` â€“ send transactional emails; defaults to `EMAIL_DEFAULT_RECIPIENTS` when `to_emails` is omitted.
  - `execute(EmailMessageRequest)` â€“ MCP wrapper returning `MessageResponse`.
- **Usage**:

```python
from spoon_toolkits.social_media.email_tool import EmailTool

email_tool = EmailTool()
await email_tool.send_message(
    message="<h2>Risk Alert</h2><p>New honeypot detected.</p>",
    to_emails=["security@example.com"],
    subject="ðŸš¨ Token Risk Detected",
)
```

Most SMTP providers enforce connection limits and per-minute quotas; reuse the same tool instance to keep TLS sessions warm, and handle `smtplib` exceptions to trigger retries or fallback to another channel.

## MCP & Convenience Functions

Each adapter exposes an `execute()` method that accepts the corresponding Pydantic request model, making it trivial to register the tool with FastMCP:

```python
from spoon_toolkits.social_media.discord_tool import DiscordTool, DiscordMessageRequest

discord = DiscordTool()
response = await discord.execute(DiscordMessageRequest(message="Hello MCP"))
```

For lightweight scripts, use the helper coroutines:

```python
from spoon_toolkits.social_media.discord_tool import send_discord_message
from spoon_toolkits.social_media.email_tool import send_email

await send_discord_message("Indexing complete")
await send_email("Pipeline healthy", ["ops@example.com"])
```

These functions internally instantiate the tool, run `validate_config()`, and return `{success, message}` dictionaries.

## Operational Tips

- **Credential hygiene**: Load all tokens via environment variables or a secrets manager; never hardcode them in agent configs.
- **Retry/backoff**: Social APIs frequently throttle. Catch exceptions from `send_message`, apply exponential backoff, or offload to a job queue.
- **Bot lifecycle**: For Discord/Telegram `start_bot()`, run the bot in a dedicated task or process and call `stop_bot()` during shutdown to avoid orphaned connections.
- **Multi-channel redundancy**: Pair channels (e.g., Telegram + Email) for critical alerts so a single API outage doesnâ€™t silence notifications.

---

FILE: Toolkit/storage/aioz.md

---
title: AIOZ Storage
---

# AIOZ Storage

`spoon_toolkits.storage.aioz` adapts AIOZ's S3-compatible service for SpoonOS agents.

## Environment

```bash
export AIOZ_ENDPOINT_URL=https://gateway.aioz.io
export AWS_ACCESS_KEY=your_access_key
export AWS_SECRET_KEY=your_secret_key
```

Set optional `BUCKET_NAME` when using the helper coroutines in `aioz_tools.py`; the tools themselves only require the three variables above.

## Tools

- `AiozListBucketsTool` â€“ list all accessible buckets.
- `UploadFileToAiozTool` â€“ push a local file to a bucket.
- `DownloadFileFromAiozTool` â€“ fetch an object to a local path.
- `DeleteAiozObjectTool` â€“ remove a single object.
- `GenerateAiozPresignedUrlTool` â€“ produce temporary download URLs.

All methods return status strings (âœ… success / âŒ failure). Wrap calls in agents that interpret the prefix to decide next steps.

### Return semantics & shared helpers
- Success responses look like `âœ… Uploaded 'file' to 'bucket'`.
- Failures bubble up `botocore` messages but remain human readable: `âŒ Failed to upload ... (Error: ...)`.
- `AiozListBucketsTool` returns a newline-separated string (emojis included) such as `ðŸ“ bucket-a`; parse the string if you need structured output.
- `GenerateAiozPresignedUrlTool` accepts `expires_in` in seconds; the default is 3600. AIOZ supports up to 7 daysâ€”set higher values when sharing large datasets.

## Usage Examples

### Upload & presign
```python
from spoon_toolkits.storage.aioz.aioz_tools import UploadFileToAiozTool, GenerateAiozPresignedUrlTool

uploader = UploadFileToAiozTool()
status = await uploader.execute(bucket_name="research-artifacts", file_path="/tmp/report.pdf")
print(status)

# Object keys default to the filename (here: report.pdf). Rename after upload if needed.
presigner = GenerateAiozPresignedUrlTool()
url = await presigner.execute(bucket_name="research-artifacts", object_key="report.pdf", expires_in=900)
print(url)
```

### CLI smoke test
```
python spoon_toolkits/storage/aioz/aioz_tools.py list-buckets
```
The helper script reads the same environment variables, making it an easy credential check.

## Operation Tips
- **Endpoint style**: `S3Tool` forces path-style URLs; keep bucket names DNS-safe to avoid signature issues.
- **Access denied**: If you see `âŒ Failed ... (Error: An error occurred (AccessDenied))`, verify the access key/secret pair. AIOZ keys are distinct from AWS IAM keys.
- **Large uploads**: For >5GB objects, extend `AIOZMultipartUploadTool` (not shipped yet) using `S3Tool`'s `_create_multipart_upload` helpers; standard uploads will fail with `EntityTooLarge`.
- **Presign mismatch**: If the generated URL returns 403, ensure the bucket policy allows `s3:GetObject` for presigned requests and that your system clock is accurate.

## MCP / Agent integration

Each class inherits `BaseTool`. Register them in FastMCP like:
```python
from spoon_toolkits.storage.aioz.aioz_tools import AiozListBucketsTool

tool = AiozListBucketsTool()
result = await tool.execute()
```
To expose via FastMCP server, import the tool into your MCP registry or call the moduleâ€™s `main` script. Within Spoon agents, include `Aioz*` tools in the tool set and pass bucket parameters directly from planner prompts.

---

FILE: Toolkit/storage/foureverland.md

# 4EVERLAND Storage

`spoon_toolkits.storage.foureverland` integrates the 4EVERLAND decentralized storage service.

## Environment

```bash
export FOUREVERLAND_ENDPOINT_URL=https://endpoint.4everland.org
export FOUREVERLAND_ACCESS_KEY=your_access_key
export FOUREVERLAND_SECRET_KEY=your_secret_key
export FOUREVERLAND_BUCKET_NAME=default_bucket  # optional for helper scripts
```

## Tools

- `ListFourEverlandBuckets`
- `UploadFileToFourEverland`
- `DownloadFileFromFourEverland`
- `DeleteFourEverlandObject`
- `GenerateFourEverlandPresignedUrl`

Object listing currently requires calling the 4EVERLAND console or a custom script; there is no dedicated `ListObjectsFourEverland` helper yet.

All inherit from `FourEverlandStorageTool`, which itself extends `S3Tool`. That means you can expect identical method signatures and status string formats across the storage adapters, simplifying multi-provider automation.

## Return formats & shared behavior
- Most methods return strings starting with `âœ…`/`âŒ`. Agents can parse the first character to branch logic quickly.
- `ListFourEverlandBuckets` returns a newline-separated string with emoji prefixes (e.g., `ðŸ“ bucket-name`). Parse the string manually if structured data is required.
- `GenerateFourEverlandPresignedUrl` accepts `expires_in` (default 3600). 4EVERLAND caps presigned URLs at 24hâ€”higher values raise a validation error.
- Exceptions from boto3 bubble up only when the helper must return structured data; otherwise theyâ€™re converted to the `âŒ ... (Error: ...)` string.

## Usage examples

```python
from spoon_toolkits.storage.foureverland.foureverland_tools import (
    UploadFileToFourEverland,
    ListFourEverlandBuckets,
)

uploader = UploadFileToFourEverland()
print(await uploader.execute(bucket_name="governance-data", file_path="/tmp/summary.json"))

lister = ListFourEverlandBuckets()
print(await lister.execute())
```

CLI verification:
```
python spoon_toolkits/storage/foureverland/foureverland_tools.py list-buckets
```

## Provider-specific notes

- **Endpoint nuance**: 4EVERLAND requires HTTPS; unsigned HTTP calls fail with `SSL required`. Ensure `FOUREVERLAND_ENDPOINT_URL` includes `https://`.
- **Bucket namespace**: Bucket names are global per account. If `CreateBucket` fails with `BucketAlreadyOwnedByYou`, delete or reuse the existing one.
- **Presign errors**: 4EVERLAND enforces lowercase bucket names; uppercase characters cause signature mismatches (403).

## MCP / agent integration

Register any `FourEverland*` tool with FastMCP:
```python
from spoon_toolkits.storage.foureverland.foureverland_tools import GenerateFourEverlandPresignedUrl

tool = GenerateFourEverlandPresignedUrl()
url = await tool.execute(bucket_name="datasets", object_key="daily.csv")
```
Because these classes inherit `BaseTool`, they plug into Spoon agent tool lists directly. For service-style exposure, mount them in your MCP server alongside other storage providers.

---

FILE: Toolkit/storage/oort.md

---
title: OORT Storage
---

# OORT Storage

`spoon_toolkits.storage.oort` provides S3-style tools for the OORT network.

## Environment

```bash
export OORT_ENDPOINT_URL=https://s3.oortech.com
export OORT_ACCESS_KEY=your_access_key
export OORT_SECRET_KEY=your_secret_key
```

## Tools

- `OortCreateBucketTool` / `OortDeleteBucketTool`
- `OortListBucketsTool`
- `OortListObjectsTool`
- `OortUploadFileTool` / `OortDownloadFileTool`
- `OortDeleteObjectTool` / `OortDeleteObjectsTool`
- `OortGeneratePresignedUrlTool`

These tools inherit retry and formatting logic from `S3Tool`; responses are concise strings suited for immediate user feedback or logging. Use `object_keys` arrays for batch deletions when cleaning up agent artifacts.

## Return & error semantics

- Bucket/object operations return emoji-prefixed strings (`âœ… Created bucket ...`, `âŒ Failed to delete object ... (Error: AccessDenied)`); parse them if you need structured status codes.
- `OortListObjectsTool` returns a newline-separated string of bullet points (e.g., `â€¢ key (Size: 123)`), and `OortListBucketsTool` emits emoji-prefixed bucket names. Convert these strings yourself if your workflow requires JSON.
- Batch delete accepts `object_keys` list; failures include per-key messages from AWS in the response string.
- Presigned URLs honour `expires_in` (default 3600). OORT supports up to 12 hours per token.
- Boto3 exceptions only bubble up when returning structured data; otherwise they are embedded in the failure string.

## Usage examples

```python
from spoon_toolkits.storage.oort.oort_tools import (
    OortCreateBucketTool,
    OortUploadFileTool,
    OortDeleteObjectsTool,
)

creator = OortCreateBucketTool()
print(await creator.execute(bucket_name="agent-artifacts"))

uploader = OortUploadFileTool()
print(await uploader.execute(bucket_name="agent-artifacts", file_path="/tmp/logs.zip"))

deleter = OortDeleteObjectsTool()
print(await deleter.execute(bucket_name="agent-artifacts", object_keys=["logs.zip", "old-report.pdf"]))
```

CLI check:
```
python spoon_toolkits/storage/oort/oort_tools.py list-buckets
```

## Operational Tips

- **Batch delete**: `OortDeleteObjectsTool` leverages `_delete_objects`; pass up to 1000 keys per call. Errors list the keys that failed so you can retry selectively.
- **Bucket lifecycle**: Newly created buckets may take a few seconds to propagate; handle `BucketAlreadyExists` by retrying with backoff or generating unique names.
- **Endpoint**: Keep `OORT_ENDPOINT_URL` aligned with your accountâ€™s region; otherwise you may see `PermanentRedirect`.
- **Permissions**: Ensure your key pair has `s3:ListAllMyBuckets` and object-level permissions; lacking one results in `âŒ ... AccessDenied`.

## MCP / agent usage

```python
from spoon_toolkits.storage.oort.oort_tools import OortGeneratePresignedUrlTool

tool = OortGeneratePresignedUrlTool()
url = await tool.execute(bucket_name="agent-artifacts", object_key="logs.zip", expires_in=600)
```

Because every tool extends `BaseTool`, add them to your agentâ€™s toolset or mount them in a FastMCP server to share across workspaces.
